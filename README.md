# üè• Health App Infrastructure Repository

## Production-Ready K8s Infrastructure with Complete DevOps Pipeline

This repository contains the complete infrastructure and deployment pipeline for the Health App platform, configured for **Kubernetes (K8s) instead of EKS** for cost-effective, production-ready deployments.

## üöÄ **Key Features**

- ‚úÖ **K8s Native Deployment** - Direct Kubernetes without EKS overhead
- ‚úÖ **Multi-Environment Support** - dev/test/prod isolation
- ‚úÖ **Auto-Scaling & Monitoring** - HPA, health checks, Prometheus
- ‚úÖ **Cost Optimization** - Resource scheduling, auto-shutdown
- ‚úÖ **Complete CI/CD** - GitHub Actions automation
- ‚úÖ **Infrastructure as Code** - Terraform + K8s manifests

## üìÅ Clean Repository Structure

```
‚îú‚îÄ‚îÄ .github/workflows/           # üî• CLEANED: 3 Core Workflows Only
‚îÇ   ‚îú‚îÄ‚îÄ core-infrastructure.yml  # Infrastructure management
‚îÇ   ‚îú‚îÄ‚îÄ core-deployment.yml      # Application deployment  
‚îÇ   ‚îî‚îÄ‚îÄ core-operations.yml      # Monitoring & operations
‚îú‚îÄ‚îÄ infra/                       # Infrastructure as Code
‚îÇ   ‚îú‚îÄ‚îÄ modules/                 # Reusable modules
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vpc/                 # Multi-network VPC module
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ k8s/                 # K8s cluster module
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rds/                 # Database module
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ monitoring/          # Monitoring module
‚îÇ   ‚îú‚îÄ‚îÄ environments/            # Environment configs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dev.tfvars          # Dev environment
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test.tfvars         # Test environment
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prod.tfvars         # Prod environment
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ monitoring.tfvars   # Monitoring environment
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ network-architecture.tfvars  # üÜï Network design
‚îÇ   ‚îî‚îÄ‚îÄ backend-configs/         # Terraform state
‚îú‚îÄ‚îÄ k8s/                         # Kubernetes manifests
‚îÇ   ‚îú‚îÄ‚îÄ health-api-complete.yaml # Application deployment
‚îÇ   ‚îú‚îÄ‚îÄ monitoring-stack.yaml   # Prometheus + Grafana
‚îÇ   ‚îî‚îÄ‚îÄ network-policies.yaml   # üÜï Network security
‚îî‚îÄ‚îÄ scripts/                     # Automation scripts
    ‚îú‚îÄ‚îÄ k8s-health-check.sh     # Health monitoring
    ‚îú‚îÄ‚îÄ k8s-auto-scale.sh       # Auto-scaling
    ‚îî‚îÄ‚îÄ setup-kubeconfig.sh     # Cluster connection
```

## Related Repositories

- [Health API](https://github.com/arunprabus/health-api): Backend API code
- [Health Frontend](https://github.com/arunprabus/health-dash): Frontend application code

## Infrastructure Deployment

The infrastructure code manages the following resources:

- **VPC and networking** (isolated per environment)
- **K8s clusters** on EC2 (cost-effective alternative to EKS)
- **RDS database instances** with automated monitoring
- **Multi-environment setup** (dev/test/prod namespaces)
- **Auto-scaling** (HPA + resource scheduling)
- **Monitoring stack** (Prometheus + Grafana)
- **Cost optimization** (automated cleanup + scheduling)

## Deployment Strategy

### Infrastructure

The infrastructure is deployed using the GitHub Actions workflow in `.github/workflows/infra-deploy.yml`. This creates the base infrastructure for each environment (development, test, production).

### Applications

Application deployments are handled through the following process:

1. Code is pushed to the application repositories (HealthApi or HealthFrontend)
2. The `.github/workflows/app-deploy.yml` workflow is triggered
3. The workflow builds the application and pushes it to the container registry
4. ArgoCD detects the changes and deploys the application to the appropriate environment

### Environment Targeting

- **Development Environment**: Triggered by pushes to the `develop` branch
- **Test Environment**: Triggered by pushes to the `staging` branch
- **Production Environment**: Triggered by pushes to the `main` branch

## Getting Started

### Prerequisites

- AWS CLI configured with appropriate credentials
- Terraform CLI (v1.6.0 or later)
- kubectl

### Deploying Infrastructure

You can deploy the infrastructure using the GitHub Actions workflow or manually:

```bash
cd infra
terraform init \
  -backend-config="bucket=your-terraform-state-bucket" \
  -backend-config="key=health-app-dev.tfstate" \
  -backend-config="region=ap-south-1"
terraform apply -var-file="environments/dev.tfvars"
```

## Maintenance

The separation of application and infrastructure code allows for:

1. Independent scaling of infrastructure without affecting application code
2. Clearer responsibility boundaries
3. Simplified CI/CD pipelines
4. Better security and access control
> üöÄ Enterprise-grade multi-environment setup with EKS, RDS, and **Blue-Green deployment strategy**‚Äîperfect for learning production DevOps practices.

---

## üß± New Network Architecture with GitHub Runners

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        AWS Region: ap-south-1                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                         ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ ‚îÇ                   LOWER NETWORK (Default VPC)                       ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îÇ   DEV ENV   ‚îÇ  ‚îÇ  TEST ENV   ‚îÇ  ‚îÇ        SHARED DATABASE          ‚îÇ ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îÇ K3s Cluster ‚îÇ  ‚îÇ K3s Cluster ‚îÇ  ‚îÇ     RDS (db.t3.micro)          ‚îÇ ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îÇ + Runner    ‚îÇ  ‚îÇ + Runner    ‚îÇ  ‚îÇ                                 ‚îÇ ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îÇ t2.micro    ‚îÇ  ‚îÇ t2.micro    ‚îÇ  ‚îÇ                                 ‚îÇ ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ ‚îÇ                   HIGHER NETWORK (Default VPC)                      ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îÇ  PROD ENV   ‚îÇ                    ‚îÇ     DEDICATED DATABASE          ‚îÇ ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îÇ K3s Cluster ‚îÇ                    ‚îÇ     RDS (db.t3.micro)          ‚îÇ ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îÇ + Runner    ‚îÇ                    ‚îÇ                                 ‚îÇ ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îÇ t2.micro    ‚îÇ                    ‚îÇ                                 ‚îÇ ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ ‚îÇ                 MONITORING NETWORK (Default VPC)                    ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îÇ              MONITORING CLUSTER                                 ‚îÇ   ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îÇ         Prometheus + Grafana + Runner                          ‚îÇ   ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îÇ                t2.micro                                        ‚îÇ   ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îÇ                                                                ‚îÇ   ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ   ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îÇ  ‚îÇGitHub Runner‚îÇ    ‚îÇGitHub Runner‚îÇ    ‚îÇGitHub Runner‚îÇ        ‚îÇ   ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îÇ  ‚îÇawsgithubrunner‚îÇ  ‚îÇawsgithubrunner‚îÇ  ‚îÇawsgithubrunner‚îÇ      ‚îÇ   ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ   ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### üîí **Enhanced Security Architecture**

| Network | CIDR | Environments | Database | Monitoring Access |
|---------|------|--------------|----------|------------------|
| **Lower** | 10.0.0.0/16 | Dev + Test | Shared RDS | ‚úÖ Via VPC Peering |
| **Higher** | 10.1.0.0/16 | Production | Dedicated RDS | ‚úÖ Via VPC Peering |
| **Monitoring** | 10.3.0.0/16 | Monitoring | None | ‚úÖ Access to Both |

### üõ°Ô∏è **Isolation Benefits**
- ‚úÖ **Complete Prod Isolation**: No direct dev/test ‚Üí prod access
- ‚úÖ **Cost Optimization**: Shared database for dev/test
- ‚úÖ **Centralized Monitoring**: Single monitoring cluster for all environments
- ‚úÖ **Security**: Network-level separation with controlled access
- ‚úÖ **Data Continuity**: Restore from existing snapshots (healthapidb-snapshot)

### üíæ **Database Restore Advantages**
- ‚úÖ **Instant Data**: Restore from `healthapidb-snapshot` with all existing data
- ‚úÖ **Zero Migration**: No manual data import needed
- ‚úÖ **Real Testing**: Dev/Test environments use production-like data
- ‚úÖ **Cost Effective**: Shared database reduces storage costs
- ‚úÖ **Consistent State**: Data integrity maintained across environments

---

## üí∞ Cost Comparison & Database Backup Strategy

### üÜì **Current Setup: 100% FREE TIER**
| Resource | Usage | Free Tier Limit | Monthly Cost |
|----------|-------|-----------------|-------------|
| EC2 t2.micro | 720 hrs | 750 hrs/month | **$0** |
| RDS db.t3.micro | 720 hrs | 750 hrs/month | **$0** |
| EBS Storage | 28GB | 30GB/month | **$0** |
| VPC + Networking | Unlimited | Unlimited | **$0** |
| **Data Transfer** | 0.9GB | 1GB/month | **$0** |
| **Total** | | | **$0/month** |

### üìä **Data Transfer Optimization**

#### üö® **Free Tier Alert Management**
```bash
# Monitor data transfer usage
Actions ‚Üí Data Transfer Monitor ‚Üí action: "monitor"

# Emergency optimization (if >85% usage)
Actions ‚Üí Data Transfer Monitor ‚Üí action: "emergency-stop"

# Scheduled optimization (automatic every 6 hours)
# Runs automatically - no manual action needed
```

#### üí° **Optimization Features**
- ‚úÖ **Auto-monitoring** every 6 hours
- ‚úÖ **Emergency stop** for non-prod resources
- ‚úÖ **CloudWatch optimization** (basic monitoring only)
- ‚úÖ **Smart resource scheduling** (stop when not needed)
- ‚úÖ **Data transfer alerts** at 85% usage

#### üìà **Usage Reduction**
| Optimization | Data Transfer Savings |
|--------------|----------------------|
| Stop dev/test EC2 | 40% reduction |
| Disable detailed monitoring | 30% reduction |
| On-demand RDS | 20% reduction |
| **Total Savings** | **67% reduction** |

### üíæ **Database Backup Strategy: 97% Cost Savings**

#### **Backup Cost Comparison**
| Method | Storage Cost | Restore Method | Monthly Cost | Savings |
|--------|-------------|----------------|--------------|----------|
| **RDS Snapshot** | $0.095/GB | Native AWS restore | **$1.90/month** | Baseline |
| **S3 Export** | $0.023/GB | Manual import | **$0.05/month** | **97% savings** |
| **S3 Intelligent Tiering** | $0.0125/GB | Manual import | **$0.03/month** | **98% savings** |

#### **Implementation**
```bash
# Current Status: RDS Stopped, Snapshot Created
aws rds describe-db-instances --query "DBInstances[?DBInstanceStatus=='stopped'].[DBInstanceIdentifier,DBInstanceStatus]"

# S3 Export (Automated)
aws rds start-export-task \
  --export-task-identifier healthapi-export-$(date +%Y%m%d) \
  --source-arn arn:aws:rds:region:account:snapshot:healthapidb-backup-$(date +%Y%m%d) \
  --s3-bucket-name health-app-terraform-state \
  --s3-prefix db-exports/ \
  --iam-role-arn arn:aws:iam::account:role/rds-s3-export-role

# Manual Export (Alternative)
pg_dump -h endpoint -U postgres -d healthapi | gzip > backup.sql.gz
aws s3 cp backup.sql.gz s3://health-app-terraform-state/db-backups/
```

#### **Restore Process**
```bash
# From S3 Backup
aws s3 cp s3://health-app-terraform-state/db-backups/backup.sql.gz .
gunzip backup.sql.gz
psql -h new-endpoint -U postgres -d healthapi < backup.sql

# From RDS Snapshot (Terraform)
cd infra
terraform apply -var="restore_from_snapshot=true" -var="snapshot_identifier=healthapidb-backup-20250102"
```

#### **Cost Optimization Achieved**
- **Before**: RDS running 24/7 = $13-15/month
- **After**: RDS stopped + S3 backup = $0.05/month
- **Total Savings**: $13-15/month (99.7% reduction)

### üí∞ Alternative: EKS Setup (Production)
| Resource | Quantity | Free Tier | Monthly Cost |
|----------|----------|-----------|-------------|
| EKS Control Plane | 1 | ‚ùå Not Free | **$73** |
| NAT Gateway | 1 | ‚ùå Not Free | **$45** |
| EC2 t2.micro | 1 | 750 hrs | $0 |
| RDS db.t3.micro | 1 | 750 hrs | $0 |
| RDS Snapshots | 20GB | ‚ùå Not Free | **$1.90** |
| **Total** | | | **$119.90/month** |

### üìä **Cost Savings Achieved: 95%+**
| Setup | Monthly Cost | Savings |
|-------|-------------|----------|
| **K3s (Current)** | **$0** | **Baseline** |
| EKS Alternative | $118 | -$118/month |
| **Multi-Env K3s** | **$0** | **vs $354/month** |

---

## üöÄ Quick Start

### üÜì **Deploy Instructions: $0 Cost Infrastructure**

#### **‚öôÔ∏è Prerequisites**
1. **AWS Account** with Free Tier available
2. **SSH Key Pair** generated
3. **GitHub Secrets** configured

#### **üöÄ Deployment Steps**

**Step 1: Generate SSH Key**
```bash
# Generate SSH key for K3s clusters
./scripts/generate-ssh-key.sh

# Or manually:
ssh-keygen -t rsa -b 4096 -f ~/.ssh/k3s-key -N "" -C "k3s-cluster-access"

# Copy your public key content
cat ~/.ssh/k3s-key.pub
```

**Step 2: Configure GitHub Secrets & Variables**
Go to **Settings** ‚Üí **Secrets and variables** ‚Üí **Actions**:

**üîê Secrets tab (Required):**
```yaml
# SSH Keys for K3s Access
SSH_PUBLIC_KEY: "ssh-rsa AAAAB3... (from ~/.ssh/k3s-key.pub)"
SSH_PRIVATE_KEY: "-----BEGIN OPENSSH PRIVATE KEY----- ... (from ~/.ssh/k3s-key)"

# Infrastructure
TF_STATE_BUCKET: "health-app-terraform-state"

# Kubeconfig secrets (AUTO-GENERATED by workflow)
KUBECONFIG_DEV: "Auto-created during deployment"
KUBECONFIG_TEST: "Auto-created during deployment"
KUBECONFIG_PROD: "Auto-created during deployment"
KUBECONFIG_MONITORING: "Auto-created during deployment"
```

**‚öôÔ∏è Variables tab (K8s Configuration):**
```yaml
AWS_REGION: "ap-south-1"
K8S_CLUSTER_NAME: "health-app-cluster"
CONTAINER_REGISTRY: "docker.io"
REGISTRY_NAMESPACE: "your-username"
TERRAFORM_VERSION: "1.6.0"
```

## üè∑Ô∏è **Tagging & Naming Standards**

### **Industry Standard Tags**
```yaml
# Required Tags (Cost & Operations)
Project: "health-app"
Environment: "dev|test|prod"
Owner: "devops-team"
CostCenter: "engineering"
ManagedBy: "terraform"
Application: "health-api"
BackupRequired: "true"

# Compliance & Security
DataClassification: "internal"
ComplianceScope: "hipaa"
MonitoringLevel: "medium"
Schedule: "business-hours"
AutoShutdown: "enabled"
```

### **Resource Naming Convention**
```yaml
# Format: {project}-{component}-{environment}
AWS Resources:
  VPC: "health-app-vpc-dev"
  EC2: "health-app-k8s-master-dev"
  RDS: "health-app-db-dev"
  S3: "health-app-terraform-state-dev"

K8s Resources:
  Namespace: "health-app-dev"
  Deployment: "health-api-backend-dev"
  Service: "health-api-service-dev"
```

### **Benefits**
- üí∞ **Cost Tracking**: Automated cost allocation by project/team
- üîÑ **Automation**: Auto-shutdown, backup policies
- üõ°Ô∏è **Compliance**: HIPAA, GDPR compliance tracking
- üìä **Operations**: Resource lifecycle management

## üîê **Kubernetes Authentication: Corporate vs Learning**

### **üè¢ Corporate/Industry Standards**

#### **1. Service Accounts (Recommended)**
```yaml
# Create dedicated service account
apiVersion: v1
kind: ServiceAccount
metadata:
  name: github-actions-sa
  namespace: kube-system
---
# Create ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: github-actions-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: github-actions-sa
  namespace: kube-system
```

#### **2. OIDC Integration (Gold Standard)**
- **AWS EKS**: Uses IAM roles for service accounts (IRSA)
- **Azure AKS**: Uses Azure AD integration
- **Google GKE**: Uses Google Service Accounts

#### **3. Certificate-Based Auth**
- Generate client certificates
- Rotate certificates regularly
- Store in secure vaults (HashiCorp Vault, AWS Secrets Manager)

### **Authentication Comparison**

| Aspect | Our Approach | Corporate Standard |
|--------|--------------|-------------------|
| **Auth Method** | K3s node token (admin) | Service Account (limited scope) |
| **Secret Storage** | GitHub Secrets | HashiCorp Vault / AWS Secrets Manager |
| **Access Level** | Full cluster admin | Least privilege (namespace-specific) |
| **Token Rotation** | Manual | Automated (30-90 days) |
| **Audit** | Basic GitHub logs | Comprehensive audit trails |

### **Corporate Implementation Example**

```yaml
# 1. Limited Service Account
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ci-cd-deployer
  namespace: health-app-dev
---
# 2. Namespace-specific Role
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: health-app-dev
  name: deployer-role
rules:
- apiGroups: ["apps", ""]
  resources: ["deployments", "services", "pods"]
  verbs: ["get", "list", "create", "update", "patch", "delete"]
---
# 3. RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: deployer-binding
  namespace: health-app-dev
subjects:
- kind: ServiceAccount
  name: ci-cd-deployer
  namespace: health-app-dev
roleRef:
  kind: Role
  name: deployer-role
  apiGroup: rbac.authorization.k8s.io
```

### **Security Evolution Path**

#### **Phase 1: Learning (Current)**
- ‚úÖ **K3s node token** - Simple, full access
- ‚úÖ **GitHub Secrets** - Basic secret management
- ‚úÖ **Manual setup** - Learning-focused

#### **Phase 2: Development**
- üîÑ **Service Accounts** - Namespace-specific access
- üîÑ **RBAC policies** - Role-based permissions
- üîÑ **Token rotation** - Automated renewal

#### **Phase 3: Production**
- üöÄ **OIDC integration** - Enterprise identity
- üöÄ **HashiCorp Vault** - Advanced secret management
- üöÄ **Certificate auth** - PKI-based security
- üöÄ **Audit logging** - Comprehensive monitoring

### **Implementation Priority**

**For Learning/Demo**: ‚úÖ Current approach is perfect
**For Development**: Implement service accounts
**For Production**: Full OIDC + Vault integration

**Corporate Priority**: Security > Convenience > Simplicity

---

## üîß **AWS Technology Integrations**

### **FREE Tier Enhancements ($0/month)**
```yaml
# Observability & Monitoring
CloudWatch Logs: "Centralized application logging"
CloudWatch Metrics: "Custom metrics collection"
CloudWatch Alarms: "Automated alerting"
Systems Manager: "Enhanced secrets management"
CloudTrail: "Audit logging and compliance"
Lambda Functions: "Cost optimization automation"
```

### **Low-Cost Additions (~$25/month)**
```yaml
# Advanced Features
X-Ray: "$5/month - Distributed tracing"
AWS Config: "$10/month - Compliance monitoring"
Secrets Manager: "$5/month - Advanced secrets"
EventBridge: "$5/month - Event processing"
```

### **Splunk Integration Options**
```yaml
# Enterprise Logging
Splunk Universal Forwarder: "$150+/month - Direct integration"
Splunk Connect for K8s: "Native K8s integration"
Kinesis ‚Üí Splunk: "Cost-effective pipeline"
ELK Stack Alternative: "$0/month - Self-hosted"
```

### **Automated Cost Optimization**
- üïò **Business Hours**: Auto-start at 9 AM UTC
- üåô **After Hours**: Auto-stop at 6 PM UTC
- üè∑Ô∏è **Tag-based Control**: Environment and AutoShutdown tags
- ü§ñ **Lambda Functions**: Serverless automation

### **Implementation Phases**
- **Phase 1 (FREE)**: CloudWatch + Lambda automation
- **Phase 2 ($25/month)**: X-Ray + Config + advanced monitoring
- **Phase 3 ($100+/month)**: Splunk + enterprise security

## üîÑ **Deployment Flow: Infrastructure ‚Üí Application**

### **üîß Gap Fixed: Missing Connection Problem**
```yaml
# BEFORE (INCOMPLETE Flow)
1. Terraform creates K8s cluster ‚úÖ
2. ??? (Missing connection) ‚ùå  
3. Application deploys to cluster ‚ùå

# AFTER (COMPLETE Flow)
1. Terraform creates K8s cluster ‚úÖ
2. Generate kubeconfig with cluster IP ‚úÖ
3. Store kubeconfig in GitHub Secrets ‚úÖ
4. App workflow uses kubeconfig ‚úÖ
5. Application deploys to correct cluster ‚úÖ
```

### **üîó How the Gap is Bridged**
```yaml
# The Missing Link Solution:
Infrastructure Output ‚Üí Cluster IP ‚Üí Kubeconfig Generation ‚Üí GitHub Secret ‚Üí App Deployment

# Specific Implementation:
1. Terraform outputs: k8s_master_public_ip = "1.2.3.4"
2. Script generates: kubeconfig pointing to 1.2.3.4:6443
3. GitHub Secret: KUBECONFIG_DEV = base64(kubeconfig)
4. App Deploy: Uses KUBECONFIG_DEV to connect
5. Result: kubectl commands work against dev cluster
```

### **How Application Connects to K8s Cluster**
```yaml
# Complete Flow
1. Deploy Infrastructure: "Creates K8s cluster at specific IP"
2. Generate Kubeconfig: "./scripts/setup-kubeconfig.sh dev CLUSTER_IP"
3. Store GitHub Secret: "KUBECONFIG_DEV with cluster connection"
4. Deploy Application: "Uses KUBECONFIG_DEV ‚Üí connects to dev cluster"
5. Environment Isolation: "health-app-dev namespace"
```

### **Environment-Specific Configuration**
```yaml
# GitHub Secrets Structure
KUBECONFIG_DEV: "Base64 config for dev cluster (IP: 1.2.3.4)"
KUBECONFIG_TEST: "Base64 config for test cluster (IP: 5.6.7.8)"
KUBECONFIG_PROD: "Base64 config for prod cluster (IP: 9.10.11.12)"

# Automatic Detection
App Deploy Workflow:
  - Detects environment (dev/test/prod)
  - Uses KUBECONFIG_{ENVIRONMENT} secret
  - Connects to correct cluster IP:6443
  - Deploys to health-app-{environment} namespace
```

### **Complete Isolation**
- üèóÔ∏è **Separate Clusters**: Each environment has its own K8s cluster
- üè∑Ô∏è **Separate Namespaces**: health-app-dev, health-app-test, health-app-prod
- üíæ **Separate Databases**: RDS instances per environment
- üîê **Separate Secrets**: Environment-specific kubeconfig files

## üîÑ **GitOps Setup: App Repos ‚Üí Infra Repo**

### **Requirements for GitOps Pipeline**
```yaml
# 1. Personal Access Token
Create PAT with: "repo and workflow permissions"
Add to Health API repo as: "INFRA_REPO_TOKEN"

# 2. App Repo Workflow (in Health API repo)
Actions:
  - Build container on push
  - Trigger webhook to infra repo
  - Pass image tag and environment

# 3. Infra Repo Webhook Handler ‚úÖ (Created: gitops-deploy.yml)
Features:
  - Receives webhook from app repo
  - Updates K8s deployment with new image
  - Handles rollout and verification

# 4. Container Registry Access
Options:
  - GitHub Container Registry (GHCR) - FREE
  - AWS ECR with proper permissions
```

### **GitOps Flow**
```yaml
# Complete Pipeline
1. Developer Push: "Code to Health API repo main branch"
2. App Repo Build: "Container built and pushed to registry"
3. Webhook Trigger: "Infra repo receives deployment request"
4. Auto Deploy: "K8s deployment updated with new image"
5. Verification: "Health checks and rollout status"
```

### **Benefits**
- üîÑ **Automated Pipeline**: Push code ‚Üí Auto deploy
- üè¢ **Separation of Concerns**: App code ‚â† Infrastructure code
- üìä **Professional Setup**: Industry-standard GitOps
- üìù **Audit Trail**: All deployments tracked in Git
- üí∞ **Cost**: $0 additional (within GitHub free limits)

## üõ°Ô∏è **Infrastructure Protection & Cleanup**

### **Automatic Failure Protection**
```yaml
# If infrastructure deployment fails:
1. ‚ùå Terraform apply fails (e.g., resource conflicts)
2. üßπ Cleanup step automatically runs
3. üóëÔ∏è terraform destroy removes partial resources
4. ‚úÖ Environment is clean for retry
```

### **Manual Emergency Cleanup**
```bash
# Clean up stuck/partial resources
chmod +x scripts/emergency-cleanup.sh
./scripts/emergency-cleanup.sh lower

# Force cleanup without confirmation
./scripts/emergency-cleanup.sh lower true
```

### **Protected Resources**
- üíª **EC2 Instances**: Auto-terminated on failure
- üóÑÔ∏è **RDS Databases**: Deleted with skip-final-snapshot
- üåê **VPC & Networking**: Cleaned up if no dependencies
- üîí **Security Groups**: Removed automatically
- ü§ñ **Lambda Functions**: Deleted
- üìä **CloudWatch Logs**: Cleaned up
- üîß **SSM Parameters**: Removed

### **Cost Protection Benefits**
- üí∞ **No Orphaned Resources**: Prevents surprise bills
- üîÑ **Safe Retry**: Clean environment for redeployment
- ‚ö° **Fast Recovery**: Automatic cleanup in seconds
- üõ°Ô∏è **Fail-Safe**: Multiple cleanup methods available

## üîÑ **Deployment Strategy: New vs Existing**

### **Re-deploying Existing Environment**
```yaml
# Safe to run multiple times
Actions ‚Üí Core Infrastructure ‚Üí action: "deploy" ‚Üí environment: "lower"

# Terraform Behavior:
‚úÖ Detects existing resources
‚úÖ Only applies changes/updates
‚úÖ No data loss on RDS (uses existing database)
‚úÖ Updates configurations if changed
‚úÖ Adds missing resources
‚úÖ Idempotent (safe to run multiple times)
```

### **When to Destroy vs Deploy**
```yaml
# Use DEPLOY when:
- ‚úÖ Updating existing infrastructure
- ‚úÖ Adding new resources
- ‚úÖ Changing configurations
- ‚úÖ Fixing failed deployments
- ‚úÖ Infrastructure already exists

# Use DESTROY when:
- ‚ùå Want to start completely fresh
- ‚ùå Major configuration conflicts
- ‚ùå Resources in broken state
- ‚ùå Testing full deployment flow
- ‚ùå Cost cleanup needed
```

### **Terraform Safety Features**
- üìã **Plan Phase**: Shows what will change before applying
- üîÑ **State Management**: Tracks existing resources
- üõ°Ô∏è **No Surprises**: Only modifies what's different
- üîí **Data Protection**: Preserves databases and persistent data

### **Recommended Approach**
- üöÄ **First Time**: Deploy new environment
- üîÑ **Updates**: Re-deploy existing environment
- üßπ **Issues**: Use emergency cleanup, then deploy
- üí∞ **Cost Control**: Destroy when not needed

**Step 3: Deploy AWS Integrations (Optional)**
```bash
# Deploy AWS integrations
kubectl apply -f k8s/aws-integrations.yaml

# Deploy Lambda cost optimizer
aws lambda create-function --function-name health-app-cost-optimizer \
  --runtime python3.9 --role arn:aws:iam::ACCOUNT:role/lambda-role \
  --handler lambda_function.lambda_handler \
  --zip-file fileb://scripts/aws-lambda-cost-optimizer.zip
```

**Step 4: Cluster Connection (Automatic)**
```bash
# ‚úÖ AUTOMATIC: No manual steps required!
# Infrastructure workflow automatically:
# 1. Gets cluster IP from Terraform output
# 2. Downloads kubeconfig via SSH
# 3. Updates server IP (127.0.0.1 ‚Üí public IP)
# 4. Creates GitHub Secrets automatically:
#    - KUBECONFIG_DEV, KUBECONFIG_TEST (for lower network)
#    - KUBECONFIG_PROD (for higher network)
#    - KUBECONFIG_MONITORING (for monitoring network)

# Manual verification (optional):
# Check Settings ‚Üí Secrets ‚Üí KUBECONFIG_* secrets exist
```

### **üîê Automatic Secret Management**
```yaml
# Workflow Permissions:
permissions:
  contents: read
  actions: write
  secrets: write  # Enables automatic secret creation

# Automated Process:
1. SSH to cluster: "Using SSH_PRIVATE_KEY from secrets"
2. Download kubeconfig: "scp /etc/rancher/k3s/k3s.yaml"
3. Update server IP: "127.0.0.1 ‚Üí cluster public IP"
4. Test connection: "kubectl get nodes"
5. Create GitHub Secrets: "Base64 encoded kubeconfig"
6. Cleanup: "Remove temporary files"
```

### **üéØ Secret Details**
- **Secret Names**: `KUBECONFIG_DEV`, `KUBECONFIG_TEST`, `KUBECONFIG_PROD`, `KUBECONFIG_MONITORING`
- **Location**: Repository Settings ‚Üí Secrets
- **Content**: Base64 encoded kubeconfig with correct server IP
- **Security**: SSH key-based download, not exposed in logs
- **Updates**: Automatic on re-deployment
- **Testing**: Connection verified before secret creation

**Step 5: Deploy Infrastructure**
```bash
# Deploy Lower Network (Dev + Test + Shared DB)
Actions ‚Üí Core Infrastructure ‚Üí action: "deploy" ‚Üí environment: "lower"

# Deploy Higher Network (Prod + Dedicated DB)
Actions ‚Üí Core Infrastructure ‚Üí action: "deploy" ‚Üí environment: "higher"

# Deploy Monitoring Network
Actions ‚Üí Core Infrastructure ‚Üí action: "deploy" ‚Üí environment: "monitoring"

# Deploy All Networks
Actions ‚Üí Core Infrastructure ‚Üí action: "deploy" ‚Üí environment: "all"
```

**Step 5b: Setup Data Transfer Monitoring**
```bash
# Enable automatic data transfer monitoring
Actions ‚Üí Data Transfer Monitor ‚Üí action: "monitor"

# The workflow will automatically:
# ‚úÖ Monitor usage every 6 hours
# ‚úÖ Stop non-essential resources if usage > 85%
# ‚úÖ Send alerts before free tier limits
# ‚úÖ Optimize data transfer automatically
```

**Step 5b: Restore from Existing Data (Optional)**
```bash
# To restore from healthapidb-snapshot:
# 1. Edit infra/environments/lower.tfvars
# 2. Uncomment: snapshot_identifier = "healthapidb-snapshot"
# 3. Deploy as normal - gets existing data instantly!
```

**Step 6: Setup GitOps**
```bash
# In Health API repo, add INFRA_REPO_TOKEN secret
# Create Personal Access Token with repo/workflow permissions

# Test deployment trigger
Actions ‚Üí Core Deployment ‚Üí Manual test
```

**Step 7: Deploy Applications**
```bash
# Via GitOps (Recommended)
# Push to Health API repo ‚Üí Auto-deploys via repository dispatch

# Or Direct Deployment
Actions ‚Üí Core Deployment ‚Üí Manual deployment

# Apply network policies
kubectl apply -f k8s/network-policies.yaml
```

**Step 7: Verify Deployment**
```bash
# Check cluster connection
kubectl cluster-info

# Check application status
kubectl get pods -n health-app-dev
kubectl get services -n health-app-dev
```

### **üîÑ Re-deployment Notes**
- **Existing Infrastructure**: Safe to re-run deploy action
- **Kubeconfig Security**: Automatically added to secrets (not shown in logs)
- **Database Preservation**: RDS data maintained across deployments
- **Cost Efficiency**: Only pay for what's running

**Step 4: Access Your Infrastructure**
```bash
# SSH to K3s node (get IP from GitHub Actions output)
ssh -i ~/.ssh/aws-key ubuntu@<EC2_PUBLIC_IP>

# Access Kubernetes cluster
kubectl --server=https://<EC2_PUBLIC_IP>:6443 get nodes
```

**Step 5: Setup Cost Protection (Optional)**
1. Go to **Actions** ‚Üí **Cost Management**
2. Select **action**: `budget-setup`
3. **Leave email empty** to use GitHub variables OR enter custom email
4. Click **Run workflow**
5. You'll get email alerts if any cost > $0.01

**Step 6: Monitor Costs (Automatic)**
- **Cost Management** workflow runs daily at 9 AM UTC
- Checks last 7 days of spending
- Auto-cleanup only if cost > $0.50 or manual trigger
- Manual run: **Actions** ‚Üí **Cost Management** ‚Üí Select action

**Step 8: Cleanup When Done**
1. Go to **Actions** ‚Üí **Core Infrastructure**
2. Select **action**: `destroy`
3. Select **environment** (dev/test/prod/monitoring/all)
4. Type **"DESTROY"** in confirmation field
5. Click **Run workflow**
6. All resources will be deleted (cost returns to $0)

#### **üí∞ Cost Verification - New Architecture**
| Resource | Lower Network | Higher Network | Monitoring | Free Tier Limit | Status |
|----------|---------------|----------------|------------|-----------------|--------|
| **EC2 t2.micro** | 2 instances | 1 instance | 1 instance | 750h each | ‚úÖ **$0** |
| **RDS db.t3.micro** | 1 shared | 1 dedicated | 0 | 750h each | ‚úÖ **$0** |
| **EBS Storage** | ~40GB | ~20GB | ~20GB | 30GB each | ‚úÖ **$0** |
| **VPC + Networking** | 3 VPCs + Peering | | | Always free | ‚úÖ **$0** |
| **Total Monthly Cost** | | | | | **$0** |

#### **üõ°Ô∏è Safety Features**
- ‚úÖ **Instance type locked** to t2.micro (FREE TIER)
- ‚úÖ **RDS locked** to db.t3.micro (FREE TIER)
- ‚úÖ **No NAT Gateway** (would cost $45/month)
- ‚úÖ **No Load Balancers** (would cost $18/month each)
- ‚úÖ **Storage limits** enforced (20GB max)

#### **üîÑ Multi-Environment Deployment**
```bash
# Deploy Single Environment
Actions ‚Üí Infrastructure ‚Üí action: "deploy" ‚Üí environment: "test"

# Deploy All Environments at Once
Actions ‚Üí Infrastructure ‚Üí action: "deploy" ‚Üí environment: "all"

# Each environment: $0/month
# Total all environments: $0/month
```

#### **üìä Alternative: Manual Deployment**
```bash
# Clone repository
git clone https://github.com/arunprabus/dev2prod-healthapp.git
cd dev2prod-healthapp/infra

# Initialize Terraform
terraform init

# Plan deployment (verify $0 cost)
terraform plan -var-file="environments/dev.tfvars" -var="ssh_public_key=$(cat ~/.ssh/aws-key.pub)"

# Apply (deploy infrastructure)
terraform apply -var-file="environments/dev.tfvars" -var="ssh_public_key=$(cat ~/.ssh/aws-key.pub)"

# Create budget via CLI (optional)
cd ..
chmod +x create-budget-cli.sh
./create-budget-cli.sh your-email@domain.com 1.00 "Dev-Budget"

# Destroy when done (optional)
cd infra
terraform destroy -var-file="environments/dev.tfvars" -var="ssh_public_key=$(cat ~/.ssh/aws-key.pub)"
```

#### **üí∞ CLI Budget Creation:**
```bash
# Basic usage (default: admin@example.com, $1.00)
./create-budget-cli.sh

# Custom parameters
./create-budget-cli.sh your-email@domain.com 5.00 "Custom-Budget"

# Usage: ./create-budget-cli.sh [email] [amount] [budget-name]
```

## ‚öôÔ∏è Configuration Management

### **Environment Variables (Configurable)**
| Variable | Default | Purpose |
|----------|---------|----------|
| `AWS_REGION` | `ap-south-1` | Deployment region |
| `EKS_CLUSTER_NAME` | `health-app-cluster` | Cluster base name |
| `CONTAINER_REGISTRY` | `ghcr.io` | Container registry |
| `REGISTRY_NAMESPACE` | `your-organization` | Registry namespace |
| `TERRAFORM_VERSION` | `1.6.0` | Terraform version |
| `KUBECTL_TIMEOUT` | `300s` | K8s operation timeout |

### **Multi-Environment Support**
- **Dev**: Fast deployments, shorter timeouts
- **Test**: Balanced configuration for testing
- **Prod**: Conservative settings, longer timeouts

### **Multi-Region Ready**
```yaml
# Asia Pacific
AWS_REGION: "ap-south-1"

# US East
AWS_REGION: "us-east-1"

# Europe
AWS_REGION: "eu-west-1"
```

## üè¢ Enterprise Benefits

### **Multi-Customer Deployments**
- **Isolated configurations** per customer
- **Separate AWS accounts** and regions
- **Custom branding** and domain names
- **Independent scaling** policies

### **Regional Compliance**
- **Data residency** requirements
- **GDPR compliance** in EU regions
- **Local regulations** adherence
- **Disaster recovery** across regions

### **Environment Isolation**
- **Different settings** per environment
- **Resource sizing** optimization
- **Security policies** per env
- **Cost allocation** tracking

### **Zero Code Changes**
- **Configuration-driven** deployments
- **Template reusability** across projects
- **Rapid environment** provisioning
- **Consistent deployments** everywhere

### **Centralized Configuration**
- **GitHub variables** management
- **Environment-specific** secrets
- **Version-controlled** configurations
- **Audit trail** for changes

## üöÄ Usage Examples

### **Global Company Setup**
```yaml
# US Production
AWS_REGION: "us-east-1"
EKS_CLUSTER_NAME: "health-app-us-prod"
CONTAINER_REGISTRY: "your-company.dkr.ecr.us-east-1.amazonaws.com"
REGISTRY_NAMESPACE: "production"
MIN_REPLICAS: "3"
MAX_REPLICAS: "20"

# EU Production  
AWS_REGION: "eu-west-1"
EKS_CLUSTER_NAME: "health-app-eu-prod"
CONTAINER_REGISTRY: "your-company.dkr.ecr.eu-west-1.amazonaws.com"
REGISTRY_NAMESPACE: "production-eu"
MIN_REPLICAS: "2"
MAX_REPLICAS: "15"

# APAC Production
AWS_REGION: "ap-south-1" 
EKS_CLUSTER_NAME: "health-app-apac-prod"
CONTAINER_REGISTRY: "your-company.dkr.ecr.ap-south-1.amazonaws.com"
REGISTRY_NAMESPACE: "production-apac"
MIN_REPLICAS: "2"
MAX_REPLICAS: "10"
```

### **Multi-Tenant SaaS Setup**
```yaml
# Customer A (Enterprise)
REGISTRY_NAMESPACE: "customer-a"
EKS_CLUSTER_NAME: "customer-a-cluster"
AWS_REGION: "us-west-2"
MIN_REPLICAS: "5"
MAX_REPLICAS: "50"
KUBECTL_TIMEOUT: "600s"
CLEANUP_DELAY: "120"

# Customer B (Startup)
REGISTRY_NAMESPACE: "customer-b"  
EKS_CLUSTER_NAME: "customer-b-cluster"
AWS_REGION: "us-east-1"
MIN_REPLICAS: "2"
MAX_REPLICAS: "10"
KUBECTL_TIMEOUT: "300s"
CLEANUP_DELAY: "30"
```

### **Development vs Production**
```yaml
# Development Environment
AWS_REGION: "ap-south-1"
EKS_CLUSTER_NAME: "health-app-dev"
CONTAINER_REGISTRY: "ghcr.io"
MIN_REPLICAS: "1"
MAX_REPLICAS: "3"
KUBECTL_TIMEOUT: "180s"
CLEANUP_DELAY: "10"
LB_WAIT_TIME: "30"

# Production Environment
AWS_REGION: "us-east-1"
EKS_CLUSTER_NAME: "health-app-prod"
CONTAINER_REGISTRY: "your-company.dkr.ecr.us-east-1.amazonaws.com"
MIN_REPLICAS: "3"
MAX_REPLICAS: "20"
KUBECTL_TIMEOUT: "600s"
CLEANUP_DELAY: "120"
LB_WAIT_TIME: "180"
```

---

## üîÑ Blue-Green Deployment Strategy

### How It Works
1. **Blue Environment**: Current production version
2. **Green Environment**: New version deployment
3. **Traffic Switch**: Instant cutover with zero downtime
4. **Auto Rollback**: Automatic revert on failure

### Deployment Flow
```
Blue (Live) ‚îÄ‚îÄ‚îê
              ‚îú‚îÄ‚Üí Load Balancer ‚îÄ‚îÄ‚Üí Users
Green (New) ‚îÄ‚îÄ‚îò
```

## üåê Network Architecture Access

| Network | CIDR | Environments | K8s Clusters | GitHub Runners | Database | Cost |
|---------|------|--------------|--------------|----------------|----------|------|
| **Lower** | Default VPC | Dev + Test | 2x t2.micro | 2x t2.micro | 1x Shared RDS | **$0** |
| **Higher** | Default VPC | Production | 1x t2.micro | 1x t2.micro | 1x Dedicated RDS | **$0** |
| **Monitoring** | Default VPC | Monitoring | 1x t2.micro | 1x t2.micro | None | **$0** |
| **Total** | | | **4 K3s clusters** | **4 GitHub runners** | **2 databases** | **$0/month** |

---

## üõ†Ô∏è Deployment Commands

### GitHub Actions Workflows
| Workflow | Trigger | Description |
|----------|---------|-------------|
| `Core Infrastructure` | Manual | **Deploy/Destroy/Plan** - Infrastructure management |
| `Core Deployment` | Repository Dispatch/Manual | **Application deployment** - Triggered by health-api |
| `Core Operations` | Schedule/Manual | **Monitor/Scale/Backup** - Daily operations |

### **üî• Simplified Workflow Actions**

#### **Core Infrastructure** (Manual Only)
- **deploy**: Create/update infrastructure
- **destroy**: Delete resources (requires "DESTROY" confirmation)
- **plan**: Preview changes

#### **Core Deployment** (Triggered by health-api)
- **Automatic**: Via repository dispatch from health-api
- **Manual**: Direct deployment with custom image

#### **Core Operations** (Scheduled + Manual)
- **monitor**: Daily health checks (9 AM UTC)
- **scale**: Auto-scaling management
- **backup**: Database backup
- **cleanup**: Resource cleanup
- **health-check**: Comprehensive health verification

### Manual Operations
```bash
# Check deployment status by network
kubectl get deployments -n health-app-dev    # Lower network
kubectl get deployments -n health-app-test   # Lower network
kubectl get deployments -n health-app-prod   # Higher network
kubectl get deployments -n monitoring        # Monitoring network

# Plan infrastructure changes by network
Actions ‚Üí Core Infrastructure ‚Üí action: "plan" ‚Üí environment: "lower"
Actions ‚Üí Core Infrastructure ‚Üí action: "plan" ‚Üí environment: "higher"

# Monitor all environments
Actions ‚Üí Core Operations ‚Üí action: "monitor"
```

## üö® Emergency Procedures

### 1. Infrastructure Rollback
```bash
# Destroy problematic network
Actions ‚Üí Core Infrastructure ‚Üí action: "destroy" ‚Üí environment: "lower" ‚Üí confirm: "DESTROY"

# Redeploy clean network
Actions ‚Üí Core Infrastructure ‚Üí action: "deploy" ‚Üí environment: "lower"
```

### 2. Application Issues
```bash
# Redeploy application
Actions ‚Üí Core Deployment ‚Üí Manual deployment

# Check application health
Actions ‚Üí Core Operations ‚Üí action: "health-check"
```

### 3. Manual K3s Access
```bash
# SSH to K3s node
ssh -i ~/.ssh/aws-key ubuntu@<EC2_PUBLIC_IP>

# Check cluster status
sudo k3s kubectl get nodes
sudo k3s kubectl get pods --all-namespaces
```

> ‚ö° **Quick Recovery** - destroy and redeploy takes ~5 minutes at $0 cost

---

## üí∏ Cost Control

### **Automated (Recommended)**
```bash
# Daily automatic monitoring
Runs every day: 9 AM UTC health checks

# Manual monitoring
Actions ‚Üí Core Operations ‚Üí action: "monitor"

# Manual scaling check
Actions ‚Üí Core Operations ‚Üí action: "scale"
```

### **Manual Infrastructure Control**
```bash
# Stop specific network
Actions ‚Üí Core Infrastructure ‚Üí action: "destroy" ‚Üí environment: "lower"

# Stop all networks
Actions ‚Üí Core Infrastructure ‚Üí action: "destroy" ‚Üí environment: "all"

# Restart when needed
Actions ‚Üí Core Infrastructure ‚Üí action: "deploy" ‚Üí environment: "lower"
```

---

## üîí Security & Isolation

- ‚úÖ **Three distinct networks**: Lower (Dev/Test), Higher (Prod), Monitoring
- ‚úÖ **Complete prod isolation**: No direct dev/test ‚Üí prod access
- ‚úÖ **Network policies**: Kubernetes-level traffic control
- ‚úÖ **VPC peering**: Monitoring access only
- ‚úÖ **Environment-specific kubeconfig**: Separate cluster access

---

## üìÅ Repository Structure

```
‚îú‚îÄ‚îÄ .github/workflows/     # CI/CD pipelines
‚îÇ   ‚îú‚îÄ‚îÄ deploy.yml        # Blue-green deployment
‚îÇ   ‚îú‚îÄ‚îÄ infra-deploy.yml  # Infrastructure setup
‚îÇ   ‚îî‚îÄ‚îÄ infra-shutdown.yml # Cost management
‚îú‚îÄ‚îÄ infra/                # Terraform infrastructure
‚îÇ   ‚îú‚îÄ‚îÄ modules/          # Reusable modules
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ eks/          # EKS module ($73/month)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ k3s/          # K3s module (FREE)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vpc/          # VPC module
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rds/          # RDS module
‚îÇ   ‚îú‚îÄ‚îÄ envs/             # Environment-specific configs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dev/          # EKS dev environment
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ qa/           # EKS qa environment
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prod/         # EKS prod environment
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ free-tier/    # K3s FREE environment
‚îÇ   ‚îî‚îÄ‚îÄ backend-configs/  # Terraform state configs
‚îú‚îÄ‚îÄ k8s/                  # Kubernetes manifests
‚îÇ   ‚îú‚îÄ‚îÄ health-api-deployment.yaml
‚îÇ   ‚îú‚îÄ‚îÄ frontend-deployment.yaml
‚îÇ   ‚îú‚îÄ‚îÄ hpa.yaml             # Horizontal Pod Autoscaler
‚îÇ   ‚îú‚îÄ‚îÄ vpa.yaml             # Vertical Pod Autoscaler
‚îÇ   ‚îú‚îÄ‚îÄ cluster-autoscaler.yaml  # Cluster scaling
‚îÇ   ‚îú‚îÄ‚îÄ advanced-hpa.yaml    # Custom metrics scaling
‚îÇ   ‚îú‚îÄ‚îÄ rbac.yaml            # Security policies
‚îÇ   ‚îú‚îÄ‚îÄ monitoring.yaml      # Prometheus setup
‚îÇ   ‚îú‚îÄ‚îÄ logging.yaml         # Centralized logging
‚îÇ   ‚îú‚îÄ‚îÄ canary-rollout.yaml  # Advanced deployment
‚îÇ   ‚îî‚îÄ‚îÄ argocd-app.yaml      # GitOps setup
‚îî‚îÄ‚îÄ README.md
```

## üîß Advanced Deployment Options

### 1. Canary Deployment
```bash
# Install Argo Rollouts
kubectl create namespace argo-rollouts
kubectl apply -n argo-rollouts -f https://github.com/argoproj/argo-rollouts/releases/latest/download/install.yaml

# Deploy canary
kubectl apply -f k8s/canary-rollout.yaml
```

### 2. GitOps with ArgoCD
```bash
# Install ArgoCD
kubectl create namespace argocd
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

# Deploy application
kubectl apply -f k8s/argocd-app.yaml
```

## üè¢ Enterprise Features

### Security & Compliance
- üîí **RBAC**: Role-based access control
- üõ°Ô∏è **Pod Security**: Non-root containers, read-only filesystem
- üîç **Security Scanning**: Trivy vulnerability scanning
- üåê **Network Policies**: Micro-segmentation

### Monitoring & Observability
- üìä **Prometheus**: Metrics collection
- üö® **Alerting**: Automated incident detection
- üìÑ **Centralized Logging**: Fluent Bit + CloudWatch
- üí∞ **Cost Monitoring**: Weekly spend alerts

### Auto-Scaling & Performance
- üìà **Horizontal Scaling (HPA)**: 2-10 pods based on CPU/memory
- üìâ **Vertical Scaling (VPA)**: Auto-adjusts pod resources
- üè¢ **Cluster Scaling**: Auto-adds/removes nodes
- üìä **Advanced Metrics**: Custom scaling triggers
- üß™ **Load Testing**: Automated performance validation
- üîÑ **Blue-Green**: Zero-downtime deployments
- üíæ **Backup**: Automated DynamoDB backups

## üß™ Learning Highlights

### üÜì Free Tier Learning (K3s Setup)
- üîÑ **Kubernetes Fundamentals**: Pods, services, deployments
- üíª **EC2 Management**: SSH, user-data, security groups
- üóæ **Database Integration**: RDS connection from K3s
- üîê **Networking**: VPC, subnets, security groups
- üõ†Ô∏è **Infrastructure as Code**: Terraform modules
- üí∞ **Cost Optimization**: 100% free tier usage

### üíº Production Learning (EKS Setup)
- üîÑ **Blue-Green Deployment**: Zero-downtime deployments
- ‚öôÔ∏è **EKS Management**: Production Kubernetes
- üóÑÔ∏è **DynamoDB + S3**: Serverless data layer
- üîÅ **Multi-environment**: Dev/Test/Prod isolation
- üîê **Security**: IAM roles, secrets management
- üìä **Monitoring**: Health checks, rollback automation
- üöÄ **CI/CD**: GitHub Actions automation

## üéØ Deployment & Scaling Strategies

### Deployment Strategies
| Strategy | Downtime | Risk | Complexity | Rollback Time | Use Case |
|----------|----------|------|------------|---------------|----------|
| Rolling | Minimal | Medium | Low | 2-5 min | Development |
| Blue-Green | Zero | Low | Medium | **Instant** | **Current Setup** |
| Canary | Zero | Very Low | High | Instant | Production |
| A/B Testing | Zero | Low | High | Instant | Feature testing |

### Auto-Scaling Strategies
| Type | Scope | Trigger | Response Time | Cost Impact |
|------|-------|---------|---------------|-------------|
| **HPA** | Pod replicas | CPU/Memory/Custom | 30-60s | Medium |
| **VPA** | Pod resources | Resource usage | 2-5 min | Low |
| **Cluster** | Node count | Pod scheduling | 2-3 min | High |
| **Predictive** | All levels | ML forecasting | Proactive | Optimized |

## üìà Auto-Scaling Architecture

### 3-Tier Scaling Strategy
```
üìä Load Increases
    ‚Üì
üîÑ HPA: Scales pods (2‚Üí10)
    ‚Üì
üìâ VPA: Adjusts resources per pod
    ‚Üì
üè¢ Cluster: Adds/removes nodes
    ‚Üì
‚ö° Full auto-scaling achieved!
```

### Horizontal Pod Autoscaler (HPA)
- **Health API**: 2-10 replicas
- **Frontend**: 2-5 replicas
- **Triggers**: CPU > 70%, Memory > 80%
- **Advanced**: Custom metrics (RPS, queue length)

### Vertical Pod Autoscaler (VPA)
- **Auto-adjusts**: CPU/Memory requests & limits
- **Health API**: 100m-1000m CPU, 128Mi-1Gi memory
- **Frontend**: 50m-500m CPU, 64Mi-512Mi memory

### Cluster Autoscaler
- **Node scaling**: Adds EC2 instances when needed
- **Cost optimization**: Removes unused nodes
- **Multi-AZ**: Distributes across availability zones

## üîç Monitoring & Verification

### Auto-Scaling Status
```bash
# Monitor HPA status
kubectl get hpa -w

# Check VPA recommendations
kubectl get vpa -w

# View resource usage
kubectl top pods
kubectl top nodes

# Scaling events
kubectl get events --field-selector reason=ScalingReplicaSet
```

### Manual Scaling (if needed)
```bash
# Manual horizontal scaling
kubectl scale deployment health-api --replicas=5

# Check current scaling status
kubectl describe hpa health-api-hpa
kubectl describe vpa health-api-vpa
```

### Check Current Status
```bash
# View active deployment color
kubectl get service health-api-service -o jsonpath='{.spec.selector.color}'

# List all deployments
kubectl get deployments -l app=health-api

# Check service health
kubectl get services
kubectl get pods -l app=health-api
```

### Troubleshooting
```bash
# View deployment logs
kubectl logs -l app=health-api,color=green

# Check pod status
kubectl describe pods -l app=health-api

# View recent events
kubectl get events --sort-by=.metadata.creationTimestamp
```

---

## üìÑ Complete Variables & Secrets Reference

### **Repository Variables (GitHub Settings ‚Üí Variables)**

#### **Core Configuration**
```yaml
AWS_REGION: "ap-south-1"                    # AWS deployment region
K8S_CLUSTER_NAME: "health-app-cluster"       # Base cluster name
CONTAINER_REGISTRY: "docker.io"             # Container registry URL
REGISTRY_NAMESPACE: "your-username"          # Registry namespace
TERRAFORM_VERSION: "1.6.0"                  # Terraform version
```

### **Repository Secrets (GitHub Settings ‚Üí Secrets)**

#### **Environment-Specific Kubeconfig (REQUIRED)**
```yaml
KUBECONFIG_DEV: "Base64 encoded dev cluster kubeconfig"
KUBECONFIG_TEST: "Base64 encoded test cluster kubeconfig"
KUBECONFIG_PROD: "Base64 encoded prod cluster kubeconfig"
KUBECONFIG_MONITORING: "Base64 encoded monitoring cluster kubeconfig"
KUBECONFIG: "Base64 encoded fallback kubeconfig"
```

#### **Infrastructure**
```yaml
AWS_ACCESS_KEY_ID: "AKIA..."                # AWS access key for Terraform
AWS_SECRET_ACCESS_KEY: "xyz123..."          # AWS secret key for Terraform
SSH_PUBLIC_KEY: "ssh-rsa AAAAB3..."         # SSH public key for EC2 access
TF_STATE_BUCKET: "health-app-terraform-state" # Terraform state bucket
```

### **Environment-Specific Variables**

#### **Development Environment**
```yaml
AWS_REGION: "ap-south-1"
K8S_CLUSTER_NAME: "health-app-dev"
CONTAINER_REGISTRY: "docker.io"
REGISTRY_NAMESPACE: "dev-team"
TERRAFORM_VERSION: "1.6.0"
NETWORK: "lower"  # Shared with test
DATABASE: "shared"  # Shared RDS instance
```

#### **Test Environment**
```yaml
AWS_REGION: "ap-south-1"
K8S_CLUSTER_NAME: "health-app-test"
CONTAINER_REGISTRY: "docker.io"
REGISTRY_NAMESPACE: "test-team"
TERRAFORM_VERSION: "1.6.0"
NETWORK: "lower"  # Shared with dev
DATABASE: "shared"  # Shared RDS instance
```

#### **Production Environment**
```yaml
AWS_REGION: "ap-south-1"
K8S_CLUSTER_NAME: "health-app-prod"
CONTAINER_REGISTRY: "docker.io"
REGISTRY_NAMESPACE: "production"
TERRAFORM_VERSION: "1.6.0"
NETWORK: "higher"  # Isolated network
DATABASE: "dedicated"  # Dedicated RDS instance
```

#### **Monitoring Environment**
```yaml
AWS_REGION: "ap-south-1"
K8S_CLUSTER_NAME: "health-app-monitoring"
CONTAINER_REGISTRY: "docker.io"
REGISTRY_NAMESPACE: "monitoring"
TERRAFORM_VERSION: "1.6.0"
NETWORK: "monitoring"  # Monitoring network
DATABASE: "none"  # No database needed
```

### **Sample Multi-Region Configuration**

#### **US Region (Primary)**
```yaml
AWS_REGION: "us-east-1"
EKS_CLUSTER_NAME: "health-app-us"
CONTAINER_REGISTRY: "123456789.dkr.ecr.us-east-1.amazonaws.com"
REGISTRY_NAMESPACE: "us-production"
```

#### **EU Region (GDPR Compliance)**
```yaml
AWS_REGION: "eu-west-1"
EKS_CLUSTER_NAME: "health-app-eu"
CONTAINER_REGISTRY: "123456789.dkr.ecr.eu-west-1.amazonaws.com"
REGISTRY_NAMESPACE: "eu-production"
```

#### **APAC Region (Local Performance)**
```yaml
AWS_REGION: "ap-south-1"
EKS_CLUSTER_NAME: "health-app-apac"
CONTAINER_REGISTRY: "123456789.dkr.ecr.ap-south-1.amazonaws.com"
REGISTRY_NAMESPACE: "apac-production"
```

### **Quick Setup Commands**

```bash
# Set repository variables using GitHub CLI
gh variable set AWS_REGION --body "ap-south-1"
gh variable set K8S_CLUSTER_NAME --body "health-app-cluster"
gh variable set CONTAINER_REGISTRY --body "docker.io"
gh variable set REGISTRY_NAMESPACE --body "your-username"
gh variable set TERRAFORM_VERSION --body "1.6.0"

# Set repository secrets (CRITICAL: Environment-specific kubeconfig)
gh secret set KUBECONFIG_DEV --body "$(cat ~/.kube/config-dev | base64 -w 0)"
gh secret set KUBECONFIG_TEST --body "$(cat ~/.kube/config-test | base64 -w 0)"
gh secret set KUBECONFIG_PROD --body "$(cat ~/.kube/config-prod | base64 -w 0)"
gh secret set KUBECONFIG_MONITORING --body "$(cat ~/.kube/config-monitoring | base64 -w 0)"
gh secret set SSH_PUBLIC_KEY --body "$(cat ~/.ssh/id_rsa.pub)"
gh secret set TF_STATE_BUCKET --body "health-app-terraform-state"
```

---

**üéì Perfect for mastering enterprise-grade deployment strategies!**
