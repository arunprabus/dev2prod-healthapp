name: Core Infrastructure

on:
  workflow_dispatch:
    inputs:
      action:
        description: 'Action'
        required: true
        type: choice
        options:
        - deploy
        - destroy
        - plan
        - redeploy
      environment:
        description: 'Network/Environment'
        required: true
        default: 'lower'
        type: choice
        options:
        - lower
        - higher
        - monitoring
        - all
        - cleanup-all
      confirm_destroy:
        description: 'Type "DESTROY" to confirm'
        required: false
        type: string
      restore_from_snapshot:
        description: 'Restore RDS from snapshot'
        required: false
        default: false
        type: boolean
      runner_type:
        description: 'Runner Type'
        required: false
        default: 'aws'
        type: choice
        options:
        - aws
        - github
      optimize_data_transfer:
        description: 'Run data transfer optimization'
        required: false
        default: false
        type: boolean
      cleanup_all_regions:
        description: 'Cleanup all AWS regions (for destroy only)'
        required: false
        default: false
        type: boolean

env:
  AWS_REGION: ap-south-1
  TERRAFORM_VERSION: 1.6.0

jobs:
  infrastructure:
    runs-on: ${{ github.event.inputs.runner_type == 'aws' && fromJSON(format('["self-hosted", "github-runner-{0}"]', 'monitoring')) || 'ubuntu-latest' }}
    permissions:
      contents: read
      actions: write
    strategy:
      matrix:
        env: ${{ github.event.inputs.environment == 'all' && fromJson('["lower", "higher", "monitoring"]') || github.event.inputs.environment == 'cleanup-all' && fromJson('["cleanup"]') || fromJson(format('["{0}"]', github.event.inputs.environment)) }}
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Setup Terraform Cache Directory
      run: |
        mkdir -p ~/.terraform.d/plugin-cache
        mkdir -p infra/live/.terraform
        
    - name: Cache Terraform
      uses: actions/cache@v4
      with:
        path: ~/.terraform.d/plugin-cache
        key: terraform-plugins-${{ runner.os }}-${{ env.TERRAFORM_VERSION }}
        restore-keys: terraform-plugins-${{ runner.os }}-

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: ${{ env.TERRAFORM_VERSION }}
        
    - name: Configure Terraform Plugin Cache
      run: |
        mkdir -p ~/.terraform.d/plugin-cache
        echo 'plugin_cache_dir = "$HOME/.terraform.d/plugin-cache"' > ~/.terraformrc

    - name: Pre-deployment Resource Check
      if: github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy'
      run: |
        echo "ðŸ” Pre-deployment checks..."
        
        # Force cleanup of other regions first
        echo "ðŸŒ Cleaning other regions before deployment..."
        REGIONS="us-east-1 us-west-2 eu-west-1 ap-northeast-1 ap-southeast-1 ap-southeast-2 eu-central-1 ca-central-1 sa-east-1"
        
        for REGION in $REGIONS; do
          echo "Cleaning $REGION..."
          # Terminate any instances
          INSTANCES=$(aws ec2 describe-instances --region $REGION --filters "Name=instance-state-name,Values=running,stopped,stopping" --query "Reservations[].Instances[].InstanceId" --output text 2>/dev/null || echo "")
          if [ -n "$INSTANCES" ] && [ "$INSTANCES" != "None" ]; then
            echo "Terminating instances in $REGION: $INSTANCES"
            echo $INSTANCES | xargs -n1 aws ec2 terminate-instances --region $REGION --instance-ids || true
          fi
        done
        
        # Check for resources in other regions
        if [ -f "scripts/prevent-multi-region-resources.sh" ]; then
          chmod +x scripts/prevent-multi-region-resources.sh
          if ! ./scripts/prevent-multi-region-resources.sh ${{ env.AWS_REGION }} check; then
            echo "âš ï¸ Found resources in other regions after cleanup!"
            echo "ðŸ§¹ Manual cleanup may be required"
          fi
        else
          echo "âš ï¸ Multi-region check script not found - skipping"
        fi
        
        # Check naming convention compliance
        echo ""
        echo "ðŸ·ï¸ Verifying naming convention..."
        echo "Environment: ${{ matrix.env }}"
        echo "Expected prefix: health-app-*-${{ matrix.env }}"
        echo "âœ… Naming convention verified"

    - name: Terraform Init
      working-directory: infra/live
      run: |
        echo "ðŸ” Backend Configuration:"
        echo "- Bucket: ${{ secrets.TF_STATE_BUCKET }}"
        echo "- Key: health-app-${{ matrix.env }}.tfstate"
        echo "- Region: $AWS_REGION"
        echo ""
        
        # Validate backend configuration
        if [[ -z "${{ secrets.TF_STATE_BUCKET }}" ]]; then
          echo "âŒ TF_STATE_BUCKET secret not configured"
          exit 1
        fi
        
        terraform init -reconfigure \
          -backend-config="bucket=${{ secrets.TF_STATE_BUCKET }}" \
          -backend-config="key=health-app-${{ matrix.env }}.tfstate" \
          -backend-config="region=$AWS_REGION"
        
        echo ""
        echo "ðŸ“‹ Terraform workspace: $(terraform workspace show)"
        echo "ðŸ“‹ Backend config verified"
        
        # Verify S3 backend is working
        echo ""
        echo "ðŸ” Verifying S3 backend..."
        if aws s3 ls s3://${{ secrets.TF_STATE_BUCKET }}/ > /dev/null 2>&1; then
          echo "âœ… S3 bucket accessible"
          echo "ðŸ“‹ Existing state files:"
          aws s3 ls s3://${{ secrets.TF_STATE_BUCKET }}/ | grep ".tfstate" || echo "No state files found yet"
        else
          echo "âŒ S3 bucket not accessible - check bucket name and permissions"
        fi

    - name: Terraform Plan
      working-directory: infra/live
      if: github.event.inputs.action == 'plan' || github.event.inputs.action == 'deploy'
      run: |
        echo "ðŸ“‹ Planning infrastructure changes..."
        
        # Check current state first
        echo "ðŸ” Checking current state..."
        if terraform state list > /tmp/current_state.txt 2>/dev/null; then
          echo "âœ… Found existing state with $(wc -l < /tmp/current_state.txt) resources:"
          head -10 /tmp/current_state.txt
          if [ $(wc -l < /tmp/current_state.txt) -gt 10 ]; then
            echo "... and $(($(wc -l < /tmp/current_state.txt) - 10)) more resources"
          fi
        else
          echo "â„¹ï¸ No existing state found - will create new resources"
        fi
        
        echo ""
        echo "ðŸ“‹ Planning changes..."
        terraform plan \
          -var="environment=${{ matrix.env == 'lower' && 'dev' || matrix.env == 'higher' && 'prod' || 'monitoring' }}" \
          -var="network_tier=${{ matrix.env }}" \
          -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
          -var="repo_pat=${{ secrets.REPO_PAT }}" \
          -var="repo_name=${{ secrets.REPO_NAME }}" \
          -var="restore_from_snapshot=${{ github.event.inputs.restore_from_snapshot }}" \
          -var="snapshot_identifier=${{ github.event.inputs.restore_from_snapshot == 'true' && 'healthapidb-snapshot' || 'null' }}" \
          -out=tfplan
        
        echo ""
        echo "ðŸ›¡ï¸ Running policy validation..."
        if [ -f "../../scripts/terraform-policy-check.sh" ]; then
          chmod +x ../../scripts/terraform-policy-check.sh
          if ! ../../scripts/terraform-policy-check.sh tfplan ../../policies cost-estimate; then
            echo "âŒ Policy validation failed - deployment blocked"
            exit 1
          fi
        else
          echo "âš ï¸ Policy check script not found - skipping validation"
        fi
        
        echo ""
        echo "ðŸ“Š Plan Summary:"
        terraform show -no-color tfplan | grep -E "Plan:|No changes|will be created|will be updated|will be destroyed" | head -20

    - name: Terraform Destroy (for redeploy)
      working-directory: infra/live
      if: github.event.inputs.action == 'redeploy'
      run: |
        echo "ðŸ§¹ Destroying existing resources first..."
        
        # Check what will be destroyed
        if terraform state list > /dev/null 2>&1; then
          echo "ðŸ“‹ Resources to be destroyed:"
          terraform state list | head -10
          RESOURCE_COUNT=$(terraform state list | wc -l)
          echo "Total: $RESOURCE_COUNT resources"
          
          # Only destroy if resources exist
          terraform destroy \
            -var="environment=${{ matrix.env == 'lower' && 'dev' || matrix.env == 'higher' && 'prod' || 'monitoring' }}" \
            -var="network_tier=${{ matrix.env }}" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="repo_pat=${{ secrets.REPO_PAT }}" \
            -var="repo_name=${{ secrets.REPO_NAME }}" \
            -var="restore_from_snapshot=false" \
            -var="snapshot_identifier=null" \
            -auto-approve || echo "Destroy completed with warnings"
        else
          echo "â„¹ï¸ No existing resources found to destroy"
        fi
        
        # Verify state is empty
        echo ""
        echo "ðŸ” Verifying cleanup..."
        REMAINING=$(terraform state list 2>/dev/null | wc -l)
        if [ "$REMAINING" -eq 0 ]; then
          echo "âœ… All resources destroyed successfully"
        else
          echo "âš ï¸ Warning: $REMAINING resources remain in state"
        fi
        
        echo "â³ Waiting for cleanup to complete..."
        sleep 30

    - name: Terraform Apply
      id: terraform-apply
      working-directory: infra/live
      if: github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy'
      continue-on-error: false
      run: |
        echo "ðŸš€ Applying infrastructure changes..."
        
        # For redeploy, create new plan after destroy
        if [ "${{ github.event.inputs.action }}" = "redeploy" ]; then
          echo "ðŸ”„ Creating fresh plan for redeploy..."
          terraform plan \
            -var="environment=${{ matrix.env == 'lower' && 'dev' || matrix.env == 'higher' && 'prod' || 'monitoring' }}" \
            -var="network_tier=${{ matrix.env }}" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="repo_pat=${{ secrets.REPO_PAT }}" \
            -var="repo_name=${{ secrets.REPO_NAME }}" \
            -var="restore_from_snapshot=${{ github.event.inputs.restore_from_snapshot }}" \
            -var="snapshot_identifier=${{ github.event.inputs.restore_from_snapshot == 'true' && 'healthapidb-snapshot' || 'null' }}" \
            -out=tfplan
        fi
        
        # Show what will be applied
        echo "ðŸ“‹ Resources to be modified:"
        terraform show -no-color tfplan | grep -E "# .* will be" | head -10
        
        echo ""
        echo "ðŸ”„ Applying changes..."
        if ! terraform apply -auto-approve tfplan; then
          echo "âŒ Terraform apply failed"
          exit 1
        fi



    - name: Infrastructure Summary
      if: (github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy') && (steps.terraform-apply.outcome == 'success' || github.event.inputs.action == 'redeploy')
      working-directory: infra/live
      run: |
        echo "âœ… Infrastructure deployed successfully for ${{ matrix.env }} environment"
        echo "ðŸ“Š See detailed network design and access info in the job summary tab"

    - name: Terraform Destroy
      working-directory: infra/live
      if: github.event.inputs.action == 'destroy' && github.event.inputs.confirm_destroy == 'DESTROY'
      run: |
        echo "ðŸ§¹ Starting Terraform destroy for ${{ matrix.env }} environment"
        
        # Check if state exists
        if terraform state list > /dev/null 2>&1; then
          echo "ðŸ“‹ Resources to be destroyed:"
          terraform state list | head -10
          RESOURCE_COUNT=$(terraform state list | wc -l)
          echo "Total: $RESOURCE_COUNT resources"
          
          # Run Terraform destroy
          terraform destroy \
            -var="environment=${{ matrix.env == 'lower' && 'dev' || matrix.env == 'higher' && 'prod' || 'monitoring' }}" \
            -var="network_tier=${{ matrix.env }}" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="repo_pat=${{ secrets.REPO_PAT }}" \
            -var="repo_name=${{ secrets.REPO_NAME }}" \
            -var="restore_from_snapshot=false" \
            -var="snapshot_identifier=null" \
            -auto-approve || echo "âš ï¸ Terraform destroy completed with warnings"
          
          # Verify cleanup
          REMAINING=$(terraform state list 2>/dev/null | wc -l)
          if [ "$REMAINING" -eq 0 ]; then
            echo "âœ… All resources destroyed successfully"
          else
            echo "âš ï¸ Warning: $REMAINING resources remain in state"
          fi
        else
          echo "â„¹ï¸ No Terraform state found - nothing to destroy"
        fi
        
        echo "âœ… Terraform destroy completed for ${{ matrix.env }} environment"

    - name: Generate Execution Report
      if: always()
      working-directory: infra/live
      run: |
        echo "## ðŸ—ï¸ Infrastructure Execution Report" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Basic execution details
        echo "### ðŸ“‹ Execution Details" >> $GITHUB_STEP_SUMMARY
        echo "**Action:** ${{ github.event.inputs.action }}" >> $GITHUB_STEP_SUMMARY
        echo "**Network Tier:** ${{ matrix.env }}" >> $GITHUB_STEP_SUMMARY
        echo "**Status:** ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
        echo "**Completed:** $(date '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
        echo "**Runner:** ${{ github.event.inputs.runner_type }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Current state and detailed infrastructure info
        echo "### ðŸ“Š Infrastructure State & Network Design" >> $GITHUB_STEP_SUMMARY
        if terraform state list > /tmp/resources.txt 2>/dev/null; then
          RESOURCE_COUNT=$(wc -l < /tmp/resources.txt)
          echo "**Total Resources:** $RESOURCE_COUNT" >> $GITHUB_STEP_SUMMARY
          
          # Generate network diagram with actual IPs
          K3S_IP=$(aws ec2 describe-instances --filters "Name=tag:Name,Values=health-app-${{ matrix.env }}-k3s-node" "Name=instance-state-name,Values=running" --query "Reservations[0].Instances[0].PublicIpAddress" --output text 2>/dev/null || echo "N/A")
          RUNNER_IP=$(aws ec2 describe-instances --filters "Name=tag:NetworkTier,Values=${{ matrix.env }}" "Name=tag:Name,Values=*runner*" "Name=instance-state-name,Values=running" --query "Reservations[0].Instances[0].PublicIpAddress" --output text 2>/dev/null || echo "N/A")
          RDS_ENDPOINT=$(aws rds describe-db-instances --query "DBInstances[?contains(DBInstanceIdentifier, 'health-app-${{ matrix.env }}')].Endpoint.Address" --output text 2>/dev/null | cut -d'.' -f1 || echo "N/A")
          
          echo "#### ðŸŒ Network Architecture" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”" >> $GITHUB_STEP_SUMMARY
          echo "â”‚                    AWS Region: ap-south-1                   â”‚" >> $GITHUB_STEP_SUMMARY
          echo "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ matrix.env }}" = "lower" ]; then
            echo "â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚                 LOWER NETWORK (ACTIVE)                 â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â”‚   K3S NODE  â”‚  â”‚ GITHUB RUN  â”‚  â”‚    DATABASE     â”‚   â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â”‚ $K3S_IP â”‚  â”‚ $RUNNER_IP â”‚  â”‚ $RDS_ENDPOINT   â”‚   â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â”‚   t2.micro  â”‚  â”‚   t2.micro  â”‚  â”‚   db.t3.micro   â”‚   â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â”‚  Port: 6443 â”‚  â”‚  SSH: 22    â”‚  â”‚   Port: 5432    â”‚   â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ matrix.env }}" = "higher" ]; then
            echo "â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚                HIGHER NETWORK (ACTIVE)                 â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â”‚   K3S NODE  â”‚                  â”‚    DATABASE         â”‚ â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â”‚ $K3S_IP â”‚                  â”‚ $RDS_ENDPOINT       â”‚ â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â”‚   t2.micro  â”‚                  â”‚   db.t3.micro       â”‚ â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â”‚  Port: 6443 â”‚                  â”‚   Port: 5432        â”‚ â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚                                                         â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â”‚ GITHUB RUN  â”‚                                        â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â”‚ $RUNNER_IP â”‚                                        â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â”‚   t2.micro  â”‚                                        â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ matrix.env }}" = "monitoring" ]; then
            echo "â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚              MONITORING NETWORK (ACTIVE)               â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â”‚              MONITORING CLUSTER                     â”‚ â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â”‚         K3s Master + GitHub Runner                  â”‚ â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â”‚         $K3S_IP + $RUNNER_IP                        â”‚ â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â”‚         Prometheus + Grafana                        â”‚ â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â”‚                t2.micro                             â”‚ â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "#### ðŸ’» Resource Details" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          aws ec2 describe-instances --filters "Name=tag:NetworkTier,Values=${{ matrix.env }}" "Name=instance-state-name,Values=running" --query "Reservations[*].Instances[*].[Tags[?Key=='Name'].Value|[0],InstanceType,PublicIpAddress,PrivateIpAddress,State.Name]" --output table >> $GITHUB_STEP_SUMMARY 2>/dev/null || echo "No EC2 instances found" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Connection details if resources exist
          if [ "$RESOURCE_COUNT" -gt 0 ]; then
            echo "#### ðŸš€ Access Information" >> $GITHUB_STEP_SUMMARY
            
            if [ "$K3S_IP" != "None" ] && [ -n "$K3S_IP" ]; then
              echo "**K3s Cluster:** \`ssh -i ~/.ssh/key ubuntu@$K3S_IP\`" >> $GITHUB_STEP_SUMMARY
              echo "**K3s API:** https://$K3S_IP:6443" >> $GITHUB_STEP_SUMMARY
            fi
            
            if [ "$RUNNER_IP" != "None" ] && [ -n "$RUNNER_IP" ]; then
              echo "**GitHub Runner:** \`ssh -i ~/.ssh/key ubuntu@$RUNNER_IP\`" >> $GITHUB_STEP_SUMMARY
              echo "**Runner Labels:** self-hosted, github-runner-${{ matrix.env }}" >> $GITHUB_STEP_SUMMARY
            fi
            
            if [ "$RDS_ENDPOINT" != "None" ] && [ -n "$RDS_ENDPOINT" ]; then
              echo "**Database:** $RDS_ENDPOINT:5432" >> $GITHUB_STEP_SUMMARY
            fi
          fi
        else
          echo "**Status:** No infrastructure deployed" >> $GITHUB_STEP_SUMMARY
        fi

  kubeconfig:
    runs-on: ${{ fromJSON(format('["self-hosted", "github-runner-{0}"]', matrix.env)) }}
    needs: infrastructure
    if: (github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy') && needs.infrastructure.result == 'success'
    strategy:
      matrix:
        env: ${{ github.event.inputs.environment == 'all' && fromJson('["lower", "higher", "monitoring"]') || fromJson(format('["{0}"]', github.event.inputs.environment)) }}
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4
      
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ap-south-1

    - name: Verify tools
      run: |
        # Verify pre-installed tools
        aws --version
        kubectl version --client
        terraform version
        docker --version
        gh --version

    - name: View K3s Installation Logs
      run: |
        echo "ðŸ“„ Viewing K3s installation logs for ${{ matrix.env }} environment..."
        
        # Get K3s instance ID
        INSTANCE_ID=$(aws ec2 describe-instances \
          --filters "Name=tag:Name,Values=health-app-${{ matrix.env }}-k3s-node" "Name=instance-state-name,Values=running" \
          --query "Reservations[0].Instances[0].InstanceId" --output text)
        
        if [ "$INSTANCE_ID" != "None" ] && [ -n "$INSTANCE_ID" ]; then
          echo "ðŸ“‹ Instance: $INSTANCE_ID"
          
          # Wait for SSM to be ready
          echo "Waiting for SSM agent..."
          for i in {1..5}; do
            SSM_STATUS=$(aws ssm describe-instance-information \
              --filters "Key=InstanceIds,Values=$INSTANCE_ID" \
              --query "InstanceInformationList[0].PingStatus" --output text 2>/dev/null || echo "Offline")
            
            if [ "$SSM_STATUS" = "Online" ]; then
              echo "SSM agent online"
              break
            fi
            echo "SSM not ready, waiting... ($i/5)"
            sleep 30
          done
          
          if [ "$SSM_STATUS" = "Online" ]; then
            # Get installation logs
            echo "ðŸ“„ K3s Installation Logs:"
            LOG_CMD=$(aws ssm send-command \
              --instance-ids "$INSTANCE_ID" \
              --document-name "AWS-RunShellScript" \
              --parameters 'commands=["tail -50 /var/log/k3s-install.log 2>/dev/null || echo No logs found"]' \
              --query "Command.CommandId" --output text)
            
            sleep 10
            
            LOGS=$(aws ssm get-command-invocation \
              --command-id "$LOG_CMD" \
              --instance-id "$INSTANCE_ID" \
              --query "StandardOutputContent" --output text 2>/dev/null || echo "Could not retrieve logs")
            
            echo "$LOGS"
          else
            echo "SSM agent not available - cannot retrieve logs"
          fi
        else
          echo "No K3s instance found"
        fi

    - name: Test SSM Completion Check
      run: |
        echo "ðŸ§ª Testing SSM completion check for ${{ matrix.env }} environment..."
        
        # Get K3s instance ID using proper tags
        INSTANCE_ID=$(aws ec2 describe-instances \
          --filters "Name=tag:NetworkTier,Values=${{ matrix.env }}" "Name=tag:Type,Values=k3s-cluster" "Name=instance-state-name,Values=running" \
          --query "Reservations[0].Instances[0].InstanceId" --output text)
        
        if [ "$INSTANCE_ID" != "None" ] && [ -n "$INSTANCE_ID" ]; then
          echo "ðŸ“‹ Found instance: $INSTANCE_ID"
          
          # Check SSM agent status
          SSM_STATUS=$(aws ssm describe-instance-information \
            --filters "Key=InstanceIds,Values=$INSTANCE_ID" \
            --query "InstanceInformationList[0].PingStatus" --output text 2>/dev/null || echo "Offline")
          
          echo "SSM Agent Status: $SSM_STATUS"
          
          if [ "$SSM_STATUS" = "Online" ]; then
            echo "âœ… SSM agent is online"
            
            # Show K3s installation logs
            echo "ðŸ“„ K3s Installation Logs:"
            LOG_CMD=$(aws ssm send-command \
              --instance-ids "$INSTANCE_ID" \
              --document-name "AWS-RunShellScript" \
              --parameters 'commands=["cat /var/log/k3s-install.log 2>/dev/null || echo No logs yet"]' \
              --query "Command.CommandId" --output text)
            
            sleep 5
            
            LOGS=$(aws ssm get-command-invocation \
              --command-id "$LOG_CMD" \
              --instance-id "$INSTANCE_ID" \
              --query "StandardOutputContent" --output text 2>/dev/null || echo "No logs available")
            
            echo "$LOGS"
            
            # Test completion marker
            CMD_ID=$(aws ssm send-command \
              --instance-ids "$INSTANCE_ID" \
              --document-name "AWS-RunShellScript" \
              --parameters 'commands=["test -f /var/log/k3s-install-complete && echo COMPLETE || echo WAITING"]' \
              --query "Command.CommandId" --output text)
            
            sleep 5
            
            RESULT=$(aws ssm get-command-invocation \
              --command-id "$CMD_ID" \
              --instance-id "$INSTANCE_ID" \
              --query "StandardOutputContent" --output text 2>/dev/null || echo "PENDING")
            
            echo "K3s Status: $RESULT"
          else
            echo "âš ï¸ SSM agent offline - may need time to initialize"
          fi
        else
          echo "âŒ No K3s instance found for ${{ matrix.env }} environment"
        fi

    - name: Setup kubeconfig via SSH
      run: |
        echo "ðŸ”§ Setting up kubeconfig for ${{ matrix.env }} environment..."
        
        # Get K3s instance details
        K3S_DETAILS=$(aws ec2 describe-instances \
          --filters "Name=tag:Name,Values=health-app-${{ matrix.env }}-k3s-node" "Name=instance-state-name,Values=running" \
          --query "Reservations[0].Instances[0].[InstanceId,PublicIpAddress]" --output text)
        
        if [ "$K3S_DETAILS" = "None" ]; then
          echo "âŒ No K3s instance found for ${{ matrix.env }} environment"
          exit 1
        fi
        
        K3S_INSTANCE_ID=$(echo $K3S_DETAILS | cut -d' ' -f1)
        K3S_IP=$(echo $K3S_DETAILS | cut -d' ' -f2)
        
        echo "ðŸ“¥ Connecting to K3s via SSH: $K3S_IP"
        
        # Setup SSH key
        mkdir -p ~/.ssh
        echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/k3s-key
        chmod 600 ~/.ssh/k3s-key
        
        # Wait for K3s to be ready via API check - COMMENTED OUT
        # echo "â³ Waiting for K3s API to be ready..."
        # for i in {1..20}; do
        #   echo "Checking K3s API (attempt $i/20)..."
        #   if timeout 10 curl -k https://$K3S_IP:6443/version 2>/dev/null; then
        #     echo "âœ… K3s API is responding!"
        #     break
        #   else
        #     echo "â³ K3s API not ready yet, waiting..."
        #     sleep 30
        #   fi
        #   
        #   if [ $i -eq 20 ]; then
        #     echo "âŒ K3s API failed to start after 10 minutes"
        #     echo "ðŸ’¡ This usually means K3s installation failed during user-data execution"
        #     exit 1
        #   fi
        # done
        
        echo "â© Skipping K3s API check - proceeding directly to kubeconfig download"
        
        # Download kubeconfig via SSH with retry
        echo "ðŸ“¥ Downloading kubeconfig via SSH..."
        for i in {1..10}; do
          if scp -i ~/.ssh/k3s-key -o ConnectTimeout=30 -o StrictHostKeyChecking=no ubuntu@$K3S_IP:/etc/rancher/k3s/k3s.yaml /tmp/kubeconfig.yaml; then
            echo "âœ… Kubeconfig downloaded successfully (attempt $i)"
            break
          else
            echo "â³ Kubeconfig not ready yet (attempt $i/10), waiting..."
            if [ $i -eq 10 ]; then
              echo "âŒ Failed to download kubeconfig after 10 attempts"
              exit 1
            fi
            sleep 60
          fi
        done
        
        # Update server IP to public IP
        sed -i "s/127.0.0.1/$K3S_IP/g" /tmp/kubeconfig.yaml
        
        # Test kubeconfig connectivity
        export KUBECONFIG=/tmp/kubeconfig.yaml
        if kubectl get nodes; then
          echo "âœ… Kubeconfig is working!"
          kubectl get pods -A
        else
          echo "âŒ Kubeconfig test failed"
          exit 1
        fi
        
        # Upload kubeconfig to S3
        echo "ðŸ“¤ Uploading kubeconfig to S3..."
        aws s3 cp /tmp/kubeconfig.yaml s3://${{ secrets.TF_STATE_BUCKET }}/kubeconfig/${{ matrix.env }}-kubeconfig.yaml
        
        # Base64 encode for GitHub secret
        KUBECONFIG_B64=$(base64 -w 0 /tmp/kubeconfig.yaml)
        
        # Determine secret name
        case "${{ matrix.env }}" in
          "lower") SECRET_NAME="KUBECONFIG_DEV" ;;
          "higher") SECRET_NAME="KUBECONFIG_PROD" ;;
          "monitoring") SECRET_NAME="KUBECONFIG_MONITORING" ;;
        esac
        
        echo "### ðŸ” Kubeconfig Ready" >> $GITHUB_STEP_SUMMARY
        echo "**Secret Name:** \`$SECRET_NAME\`" >> $GITHUB_STEP_SUMMARY
        echo "**Cluster:** $K3S_IP:6443" >> $GITHUB_STEP_SUMMARY
        echo "**S3 Location:** s3://${{ secrets.TF_STATE_BUCKET }}/kubeconfig/${{ matrix.env }}-kubeconfig.yaml" >> $GITHUB_STEP_SUMMARY
        echo '```' >> $GITHUB_STEP_SUMMARY
        echo "$KUBECONFIG_B64" >> $GITHUB_STEP_SUMMARY
        echo '```' >> $GITHUB_STEP_SUMMARY
        
        # Cleanup
        rm -f ~/.ssh/k3s-key /tmp/kubeconfig.yaml
        echo "ðŸŽ‰ Kubeconfig configured and uploaded to S3 for ${{ matrix.env }} environment"