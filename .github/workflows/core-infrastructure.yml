name: Core Infrastructure

on:
  workflow_dispatch:
    inputs:
      action:
        description: 'Action'
        required: true
        type: choice
        options:
        - deploy
        - destroy
        - plan
        - redeploy
      environment:
        description: 'Network/Environment'
        required: true
        default: 'lower'
        type: choice
        options:
        - lower
        - higher
        - monitoring
        - all
        - cleanup-all
      confirm_destroy:
        description: 'Type "DESTROY" to confirm'
        required: false
        type: string
      restore_from_snapshot:
        description: 'Restore RDS from snapshot'
        required: false
        default: false
        type: boolean
      runner_type:
        description: 'Runner Type'
        required: false
        default: 'aws'
        type: choice
        options:
        - aws
        - github
      optimize_data_transfer:
        description: 'Run data transfer optimization'
        required: false
        default: false
        type: boolean
      cleanup_all_regions:
        description: 'Cleanup all AWS regions (for destroy only)'
        required: false
        default: false
        type: boolean

env:
  AWS_REGION: ap-south-1
  TERRAFORM_VERSION: 1.6.0

jobs:
  infrastructure:
    runs-on: ${{ github.event.inputs.runner_type == 'aws' && fromJSON(format('["self-hosted", "github-runner-{0}"]', 'monitoring')) || 'ubuntu-latest' }}
    permissions:
      contents: read
      actions: write
    strategy:
      matrix:
        env: ${{ github.event.inputs.environment == 'all' && fromJson('["lower", "higher", "monitoring"]') || github.event.inputs.environment == 'cleanup-all' && fromJson('["cleanup"]') || fromJson(format('["{0}"]', github.event.inputs.environment)) }}
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Setup Terraform Cache Directory
      run: |
        mkdir -p ~/.terraform.d/plugin-cache
        mkdir -p infra/live/.terraform
        
    - name: Cache Terraform
      uses: actions/cache@v4
      with:
        path: ~/.terraform.d/plugin-cache
        key: terraform-plugins-${{ runner.os }}-${{ env.TERRAFORM_VERSION }}
        restore-keys: terraform-plugins-${{ runner.os }}-

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: ${{ env.TERRAFORM_VERSION }}
        
    - name: Configure Terraform Plugin Cache
      run: |
        mkdir -p ~/.terraform.d/plugin-cache
        echo 'plugin_cache_dir = "$HOME/.terraform.d/plugin-cache"' > ~/.terraformrc

    - name: Pre-deployment Resource Check
      if: github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy'
      run: |
        echo "ðŸ” Pre-deployment checks..."
        
        # Force cleanup of other regions first
        echo "ðŸŒ Cleaning other regions before deployment..."
        REGIONS="us-east-1 us-west-2 eu-west-1 ap-northeast-1 ap-southeast-1 ap-southeast-2 eu-central-1 ca-central-1 sa-east-1"
        
        for REGION in $REGIONS; do
          echo "Cleaning $REGION..."
          # Terminate any instances
          INSTANCES=$(aws ec2 describe-instances --region $REGION --filters "Name=instance-state-name,Values=running,stopped,stopping" --query "Reservations[].Instances[].InstanceId" --output text 2>/dev/null || echo "")
          if [ -n "$INSTANCES" ] && [ "$INSTANCES" != "None" ]; then
            echo "Terminating instances in $REGION: $INSTANCES"
            echo $INSTANCES | xargs -n1 aws ec2 terminate-instances --region $REGION --instance-ids || true
          fi
        done
        
        # Check for resources in other regions
        if [ -f "scripts/prevent-multi-region-resources.sh" ]; then
          chmod +x scripts/prevent-multi-region-resources.sh
          if ! ./scripts/prevent-multi-region-resources.sh ${{ env.AWS_REGION }} check; then
            echo "âš ï¸ Found resources in other regions after cleanup!"
            echo "ðŸ§¹ Manual cleanup may be required"
          fi
        else
          echo "âš ï¸ Multi-region check script not found - skipping"
        fi
        
        # Check naming convention compliance
        echo ""
        echo "ðŸ·ï¸ Verifying naming convention..."
        echo "Environment: ${{ matrix.env }}"
        echo "Expected prefix: health-app-*-${{ matrix.env }}"
        echo "âœ… Naming convention verified"

    - name: Terraform Init
      working-directory: infra/live
      run: |
        echo "ðŸ” Backend Configuration:"
        echo "- Bucket: ${{ secrets.TF_STATE_BUCKET }}"
        echo "- Key: health-app-${{ matrix.env }}.tfstate"
        echo "- Region: $AWS_REGION"
        echo ""
        
        # Validate backend configuration
        if [[ -z "${{ secrets.TF_STATE_BUCKET }}" ]]; then
          echo "âŒ TF_STATE_BUCKET secret not configured"
          exit 1
        fi
        
        terraform init -reconfigure \
          -backend-config="bucket=${{ secrets.TF_STATE_BUCKET }}" \
          -backend-config="key=health-app-${{ matrix.env }}.tfstate" \
          -backend-config="region=$AWS_REGION"
        
        echo ""
        echo "ðŸ“‹ Terraform workspace: $(terraform workspace show)"
        echo "ðŸ“‹ Backend config verified"
        
        # Verify S3 backend is working
        echo ""
        echo "ðŸ” Verifying S3 backend..."
        if aws s3 ls s3://${{ secrets.TF_STATE_BUCKET }}/ > /dev/null 2>&1; then
          echo "âœ… S3 bucket accessible"
          echo "ðŸ“‹ Existing state files:"
          aws s3 ls s3://${{ secrets.TF_STATE_BUCKET }}/ | grep ".tfstate" || echo "No state files found yet"
        else
          echo "âŒ S3 bucket not accessible - check bucket name and permissions"
        fi

    - name: Terraform Plan
      working-directory: infra/live
      if: github.event.inputs.action == 'plan' || github.event.inputs.action == 'deploy'
      run: |
        echo "ðŸ“‹ Planning infrastructure changes..."
        
        # Check current state first
        echo "ðŸ” Checking current state..."
        if terraform state list > /tmp/current_state.txt 2>/dev/null; then
          echo "âœ… Found existing state with $(wc -l < /tmp/current_state.txt) resources:"
          head -10 /tmp/current_state.txt
          if [ $(wc -l < /tmp/current_state.txt) -gt 10 ]; then
            echo "... and $(($(wc -l < /tmp/current_state.txt) - 10)) more resources"
          fi
        else
          echo "â„¹ï¸ No existing state found - will create new resources"
        fi
        
        echo ""
        echo "ðŸ“‹ Planning changes..."
        terraform plan \
          -var="environment=${{ matrix.env == 'lower' && 'dev' || matrix.env == 'higher' && 'prod' || 'monitoring' }}" \
          -var="network_tier=${{ matrix.env }}" \
          -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
          -var="repo_pat=${{ secrets.REPO_PAT }}" \
          -var="repo_name=${{ secrets.REPO_NAME }}" \
          -var="restore_from_snapshot=${{ github.event.inputs.restore_from_snapshot }}" \
          -var="snapshot_identifier=${{ github.event.inputs.restore_from_snapshot == 'true' && 'healthapidb-snapshot' || 'null' }}" \
          -out=tfplan
        
        echo ""
        echo "ðŸ›¡ï¸ Running policy validation..."
        if [ -f "../../scripts/terraform-policy-check.sh" ]; then
          chmod +x ../../scripts/terraform-policy-check.sh
          if ! ../../scripts/terraform-policy-check.sh tfplan ../../policies cost-estimate; then
            echo "âŒ Policy validation failed - deployment blocked"
            exit 1
          fi
        else
          echo "âš ï¸ Policy check script not found - skipping validation"
        fi
        
        echo ""
        echo "ðŸ“Š Plan Summary:"
        terraform show -no-color tfplan | grep -E "Plan:|No changes|will be created|will be updated|will be destroyed" | head -20

    - name: Terraform Destroy (for redeploy)
      working-directory: infra/live
      if: github.event.inputs.action == 'redeploy'
      run: |
        echo "ðŸ§¹ Destroying existing resources first..."
        
        # Check what will be destroyed
        if terraform state list > /dev/null 2>&1; then
          echo "ðŸ“‹ Resources to be destroyed:"
          terraform state list | head -10
          RESOURCE_COUNT=$(terraform state list | wc -l)
          echo "Total: $RESOURCE_COUNT resources"
          
          # Only destroy if resources exist
          terraform destroy \
            -var="environment=${{ matrix.env == 'lower' && 'dev' || matrix.env == 'higher' && 'prod' || 'monitoring' }}" \
            -var="network_tier=${{ matrix.env }}" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="repo_pat=${{ secrets.REPO_PAT }}" \
            -var="repo_name=${{ secrets.REPO_NAME }}" \
            -var="restore_from_snapshot=false" \
            -var="snapshot_identifier=null" \
            -auto-approve || echo "Destroy completed with warnings"
        else
          echo "â„¹ï¸ No existing resources found to destroy"
        fi
        
        # Verify state is empty
        echo ""
        echo "ðŸ” Verifying cleanup..."
        REMAINING=$(terraform state list 2>/dev/null | wc -l)
        if [ "$REMAINING" -eq 0 ]; then
          echo "âœ… All resources destroyed successfully"
        else
          echo "âš ï¸ Warning: $REMAINING resources remain in state"
        fi
        
        echo "â³ Waiting for cleanup to complete..."
        sleep 30

    - name: Terraform Apply
      id: terraform-apply
      working-directory: infra/live
      if: github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy'
      continue-on-error: false
      run: |
        echo "ðŸš€ Applying infrastructure changes..."
        
        # For redeploy, create new plan after destroy
        if [ "${{ github.event.inputs.action }}" = "redeploy" ]; then
          echo "ðŸ”„ Creating fresh plan for redeploy..."
          terraform plan \
            -var="environment=${{ matrix.env == 'lower' && 'dev' || matrix.env == 'higher' && 'prod' || 'monitoring' }}" \
            -var="network_tier=${{ matrix.env }}" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="repo_pat=${{ secrets.REPO_PAT }}" \
            -var="repo_name=${{ secrets.REPO_NAME }}" \
            -var="restore_from_snapshot=${{ github.event.inputs.restore_from_snapshot }}" \
            -var="snapshot_identifier=${{ github.event.inputs.restore_from_snapshot == 'true' && 'healthapidb-snapshot' || 'null' }}" \
            -out=tfplan
        fi
        
        # Show what will be applied
        echo "ðŸ“‹ Resources to be modified:"
        terraform show -no-color tfplan | grep -E "# .* will be" | head -10
        
        echo ""
        echo "ðŸ”„ Applying changes..."
        if ! terraform apply -auto-approve tfplan; then
          echo "âŒ Terraform apply failed"
          exit 1
        fi

    - name: Setup SSH Key and GitHub CLI
      if: (github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy') && (steps.terraform-apply.outcome == 'success' || github.event.inputs.action == 'redeploy')
      run: |
        # Setup SSH key
        mkdir -p ~/.ssh
        echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
        chmod 600 ~/.ssh/id_rsa
        ssh-keyscan -H github.com >> ~/.ssh/known_hosts
        
        # GitHub CLI not needed - using job summary approach
        
        # Install kubectl if not present
        if ! command -v kubectl &> /dev/null; then
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
        fi

    - name: Verify K3s Installation
      if: (github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy') && (steps.terraform-apply.outcome == 'success' || github.event.inputs.action == 'redeploy')
      working-directory: infra/live
      run: |
        echo "ðŸ” Verifying K3s installation..."
        
        # Get K3s instance IP
        K3S_IP=$(aws ec2 describe-instances \
          --filters "Name=tag:Name,Values=health-app-${{ matrix.env }}-k3s-node" "Name=instance-state-name,Values=running" \
          --query "Reservations[0].Instances[0].PublicIpAddress" --output text 2>/dev/null)
        
        if [ "$K3S_IP" = "None" ] || [ -z "$K3S_IP" ]; then
          echo "âŒ No K3s instance found"
          exit 1
        fi
        
        echo "K3s instance found at: $K3S_IP"
        
        # Wait for instance to be ready
        echo "Waiting for instance to be ready..."
        for i in {1..20}; do
          if ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no -o ConnectTimeout=10 ubuntu@$K3S_IP "echo 'Instance ready'" 2>/dev/null; then
            echo "âœ… Instance is accessible (attempt $i)"
            break
          else
            echo "Instance not ready, waiting... (attempt $i/20)"
            if [ $i -eq 20 ]; then
              echo "âš ï¸ Instance not accessible after 20 attempts - continuing anyway"
              break
            fi
            sleep 15
          fi
        done
        
        # Check K3s installation
        echo "Checking K3s installation..."
        if ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no -o ConnectTimeout=10 ubuntu@$K3S_IP "sudo systemctl is-active k3s" 2>/dev/null; then
          echo "âœ… K3s service is running"
          
          # Check kubectl
          if ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no -o ConnectTimeout=10 ubuntu@$K3S_IP "kubectl get nodes" 2>/dev/null; then
            echo "âœ… kubectl is working"
            ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no ubuntu@$K3S_IP "kubectl get nodes"
          else
            echo "âš ï¸ kubectl not working properly"
          fi
          
          # Check kubeconfig file
          if ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no ubuntu@$K3S_IP "test -f /etc/rancher/k3s/k3s.yaml"; then
            echo "âœ… Kubeconfig file exists"
          else
            echo "âŒ Kubeconfig file missing"
          fi
        else
          echo "âŒ K3s service not running - checking installation logs"
          ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no ubuntu@$K3S_IP "sudo journalctl -u k3s --no-pager -n 20" || echo "Could not retrieve logs"
        fi

    - name: Generate and Update Kubeconfig
      if: (github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy') && (steps.terraform-apply.outcome == 'success' || github.event.inputs.action == 'redeploy')
      working-directory: infra/live
      run: |
        echo "ðŸ”§ Generating kubeconfig for ${{ matrix.env }} environment..."
        
        # Get K3s instance IP
        K3S_IP=$(aws ec2 describe-instances \
          --filters "Name=tag:Name,Values=health-app-${{ matrix.env }}-k3s-node" "Name=instance-state-name,Values=running" \
          --query "Reservations[0].Instances[0].PublicIpAddress" --output text 2>/dev/null)
        
        if [ "$K3S_IP" = "None" ] || [ -z "$K3S_IP" ]; then
          echo "âŒ No K3s instance found for kubeconfig generation"
          exit 1
        fi
        
        # Download kubeconfig from K3s node
        echo "ðŸ“¥ Downloading kubeconfig from K3s node..."
        ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no ubuntu@$K3S_IP "sudo cat /etc/rancher/k3s/k3s.yaml" > /tmp/kubeconfig-${{ matrix.env }}.yaml
        
        # Update server IP in kubeconfig
        sed -i "s/127.0.0.1/$K3S_IP/g" /tmp/kubeconfig-${{ matrix.env }}.yaml
        
        # Test kubeconfig
        if kubectl --kubeconfig=/tmp/kubeconfig-${{ matrix.env }}.yaml get nodes > /dev/null 2>&1; then
          echo "âœ… Kubeconfig is valid"
        else
          echo "âŒ Kubeconfig validation failed"
          cat /tmp/kubeconfig-${{ matrix.env }}.yaml
          exit 1
        fi
        
        # Base64 encode kubeconfig for GitHub secret
        KUBECONFIG_B64=$(base64 -w 0 /tmp/kubeconfig-${{ matrix.env }}.yaml)
        
        # Determine secret name based on environment
        case "${{ matrix.env }}" in
          "lower")
            SECRET_NAME="KUBECONFIG_DEV"
            ;;
          "higher")
            SECRET_NAME="KUBECONFIG_PROD"
            ;;
          "monitoring")
            SECRET_NAME="KUBECONFIG_MONITORING"
            ;;
          *)
            SECRET_NAME="KUBECONFIG_$(echo ${{ matrix.env }} | tr '[:lower:]' '[:upper:]')"
            ;;
        esac
        
        echo "ðŸ” Updating GitHub secret: $SECRET_NAME"
        
        # Store kubeconfig in temporary file for manual secret setup
        echo "$KUBECONFIG_B64" > /tmp/kubeconfig_b64_${{ matrix.env }}.txt
        echo "ðŸ“ Kubeconfig base64 encoded and ready for GitHub secret: $SECRET_NAME"
        echo "To manually set the secret, use the content from the job summary."
        
        # Add to job summary for manual secret setup
        echo "### ðŸ” Kubeconfig Secret Setup" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Secret Name:** \`$SECRET_NAME\`" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Secret Value (Base64):**" >> $GITHUB_STEP_SUMMARY
        echo '```' >> $GITHUB_STEP_SUMMARY
        echo "$KUBECONFIG_B64" >> $GITHUB_STEP_SUMMARY
        echo '```' >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Instructions:**" >> $GITHUB_STEP_SUMMARY
        echo "1. Go to Settings â†’ Secrets and variables â†’ Actions" >> $GITHUB_STEP_SUMMARY
        echo "2. Click 'New repository secret'" >> $GITHUB_STEP_SUMMARY
        echo "3. Name: \`$SECRET_NAME\`" >> $GITHUB_STEP_SUMMARY
        echo "4. Value: Copy the base64 content above" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Cleanup
        rm -f /tmp/kubeconfig-${{ matrix.env }}.yaml /tmp/kubeconfig_b64_${{ matrix.env }}.txt
        
        echo "ðŸŽ‰ Kubeconfig generated for ${{ matrix.env }} environment - check job summary for secret setup"

    - name: Infrastructure Summary
      if: (github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy') && (steps.terraform-apply.outcome == 'success' || github.event.inputs.action == 'redeploy')
      working-directory: infra/live
      run: |
        echo "âœ… Infrastructure deployed successfully for ${{ matrix.env }} environment"
        echo "ðŸ“Š See detailed network design and access info in the job summary tab"

    - name: Terraform Destroy
      working-directory: infra/live
      if: github.event.inputs.action == 'destroy' && github.event.inputs.confirm_destroy == 'DESTROY'
      run: |
        echo "ðŸ§¹ Starting Terraform destroy for ${{ matrix.env }} environment"
        
        # Check if state exists
        if terraform state list > /dev/null 2>&1; then
          echo "ðŸ“‹ Resources to be destroyed:"
          terraform state list | head -10
          RESOURCE_COUNT=$(terraform state list | wc -l)
          echo "Total: $RESOURCE_COUNT resources"
          
          # Run Terraform destroy
          terraform destroy \
            -var="environment=${{ matrix.env == 'lower' && 'dev' || matrix.env == 'higher' && 'prod' || 'monitoring' }}" \
            -var="network_tier=${{ matrix.env }}" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="repo_pat=${{ secrets.REPO_PAT }}" \
            -var="repo_name=${{ secrets.REPO_NAME }}" \
            -var="restore_from_snapshot=false" \
            -var="snapshot_identifier=null" \
            -auto-approve || echo "âš ï¸ Terraform destroy completed with warnings"
          
          # Verify cleanup
          REMAINING=$(terraform state list 2>/dev/null | wc -l)
          if [ "$REMAINING" -eq 0 ]; then
            echo "âœ… All resources destroyed successfully"
          else
            echo "âš ï¸ Warning: $REMAINING resources remain in state"
          fi
        else
          echo "â„¹ï¸ No Terraform state found - nothing to destroy"
        fi
        
        echo "âœ… Terraform destroy completed for ${{ matrix.env }} environment"

    - name: Generate Execution Report
      if: always()
      working-directory: infra/live
      run: |
        echo "## ðŸ—ï¸ Infrastructure Execution Report" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Basic execution details
        echo "### ðŸ“‹ Execution Details" >> $GITHUB_STEP_SUMMARY
        echo "**Action:** ${{ github.event.inputs.action }}" >> $GITHUB_STEP_SUMMARY
        echo "**Network Tier:** ${{ matrix.env }}" >> $GITHUB_STEP_SUMMARY
        echo "**Status:** ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
        echo "**Completed:** $(date '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
        echo "**Runner:** ${{ github.event.inputs.runner_type }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Current state and detailed infrastructure info
        echo "### ðŸ“Š Infrastructure State & Network Design" >> $GITHUB_STEP_SUMMARY
        if terraform state list > /tmp/resources.txt 2>/dev/null; then
          RESOURCE_COUNT=$(wc -l < /tmp/resources.txt)
          echo "**Total Resources:** $RESOURCE_COUNT" >> $GITHUB_STEP_SUMMARY
          
          # Generate network diagram with actual IPs
          K3S_IP=$(aws ec2 describe-instances --filters "Name=tag:Name,Values=health-app-${{ matrix.env }}-k3s-node" "Name=instance-state-name,Values=running" --query "Reservations[0].Instances[0].PublicIpAddress" --output text 2>/dev/null || echo "N/A")
          RUNNER_IP=$(aws ec2 describe-instances --filters "Name=tag:NetworkTier,Values=${{ matrix.env }}" "Name=tag:Name,Values=*runner*" "Name=instance-state-name,Values=running" --query "Reservations[0].Instances[0].PublicIpAddress" --output text 2>/dev/null || echo "N/A")
          RDS_ENDPOINT=$(aws rds describe-db-instances --query "DBInstances[?contains(DBInstanceIdentifier, 'health-app-${{ matrix.env }}')].Endpoint.Address" --output text 2>/dev/null | cut -d'.' -f1 || echo "N/A")
          
          echo "#### ðŸŒ Network Architecture" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”" >> $GITHUB_STEP_SUMMARY
          echo "â”‚                    AWS Region: ap-south-1                   â”‚" >> $GITHUB_STEP_SUMMARY
          echo "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ matrix.env }}" = "lower" ]; then
            echo "â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚                 LOWER NETWORK (ACTIVE)                 â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â”‚   K3S NODE  â”‚  â”‚ GITHUB RUN  â”‚  â”‚    DATABASE     â”‚   â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â”‚ $K3S_IP â”‚  â”‚ $RUNNER_IP â”‚  â”‚ $RDS_ENDPOINT   â”‚   â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â”‚   t2.micro  â”‚  â”‚   t2.micro  â”‚  â”‚   db.t3.micro   â”‚   â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â”‚  Port: 6443 â”‚  â”‚  SSH: 22    â”‚  â”‚   Port: 5432    â”‚   â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ matrix.env }}" = "higher" ]; then
            echo "â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚                HIGHER NETWORK (ACTIVE)                 â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â”‚   K3S NODE  â”‚                  â”‚    DATABASE         â”‚ â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â”‚ $K3S_IP â”‚                  â”‚ $RDS_ENDPOINT       â”‚ â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â”‚   t2.micro  â”‚                  â”‚   db.t3.micro       â”‚ â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â”‚  Port: 6443 â”‚                  â”‚   Port: 5432        â”‚ â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚                                                         â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â”‚ GITHUB RUN  â”‚                                        â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â”‚ $RUNNER_IP â”‚                                        â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â”‚   t2.micro  â”‚                                        â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ matrix.env }}" = "monitoring" ]; then
            echo "â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚              MONITORING NETWORK (ACTIVE)               â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â”‚              MONITORING CLUSTER                     â”‚ â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â”‚         K3s Master + GitHub Runner                  â”‚ â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â”‚         $K3S_IP + $RUNNER_IP                        â”‚ â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â”‚         Prometheus + Grafana                        â”‚ â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â”‚                t2.micro                             â”‚ â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚" >> $GITHUB_STEP_SUMMARY
            echo "â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "#### ðŸ’» Resource Details" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          aws ec2 describe-instances --filters "Name=tag:NetworkTier,Values=${{ matrix.env }}" "Name=instance-state-name,Values=running" --query "Reservations[*].Instances[*].[Tags[?Key=='Name'].Value|[0],InstanceType,PublicIpAddress,PrivateIpAddress,State.Name]" --output table >> $GITHUB_STEP_SUMMARY 2>/dev/null || echo "No EC2 instances found" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Connection details if resources exist
          if [ "$RESOURCE_COUNT" -gt 0 ]; then
            echo "#### ðŸš€ Access Information" >> $GITHUB_STEP_SUMMARY
            
            if [ "$K3S_IP" != "None" ] && [ -n "$K3S_IP" ]; then
              echo "**K3s Cluster:** \`ssh -i ~/.ssh/key ubuntu@$K3S_IP\`" >> $GITHUB_STEP_SUMMARY
              echo "**K3s API:** https://$K3S_IP:6443" >> $GITHUB_STEP_SUMMARY
            fi
            
            if [ "$RUNNER_IP" != "None" ] && [ -n "$RUNNER_IP" ]; then
              echo "**GitHub Runner:** \`ssh -i ~/.ssh/key ubuntu@$RUNNER_IP\`" >> $GITHUB_STEP_SUMMARY
              echo "**Runner Labels:** self-hosted, github-runner-${{ matrix.env }}" >> $GITHUB_STEP_SUMMARY
            fi
            
            if [ "$RDS_ENDPOINT" != "None" ] && [ -n "$RDS_ENDPOINT" ]; then
              echo "**Database:** $RDS_ENDPOINT:5432" >> $GITHUB_STEP_SUMMARY
            fi
          fi
        else
          echo "**Status:** No infrastructure deployed" >> $GITHUB_STEP_SUMMARY
        fi