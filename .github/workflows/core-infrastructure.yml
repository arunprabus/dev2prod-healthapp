name: Core Infrastructure

on:
  workflow_dispatch:
    inputs:
      action:
        description: 'Action'
        required: true
        type: choice
        options:
          - deploy
          - destroy
          - plan
          - redeploy
      network:
        description: 'Network Tier'
        required: true
        default: 'lower'
        type: choice
        options:
          - lower
          - higher
          - monitoring
      confirm_destroy:
        description: 'Type "DESTROY" to confirm'
        required: false
        type: string

env:
  AWS_REGION: ap-south-1
  TERRAFORM_VERSION: 1.6.0
  DEFAULT_ACTION: redeploy
  DEFAULT_NETWORK: lower

jobs:
  # Step 1: Terraform ‚Üí Creates instances
  terraform-deploy:
    runs-on: ubuntu-latest
    outputs:
      cluster_ip: ${{ steps.redeploy.outputs.cluster_ip || steps.deploy.outputs.cluster_ip }}
      dev_cluster_ip: ${{ steps.redeploy.outputs.dev_cluster_ip || steps.deploy.outputs.dev_cluster_ip }}
      test_cluster_ip: ${{ steps.redeploy.outputs.test_cluster_ip || steps.deploy.outputs.test_cluster_ip }}

    steps:
      - uses: actions/checkout@v4

      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}
          terraform_wrapper: false

      - name: Cache Terraform
        uses: actions/cache@v3
        with:
          path: |
            infra/.terraform
            infra/.terraform.lock.hcl
            ~/.terraform.d/plugin-cache
          key: terraform-${{ env.TERRAFORM_VERSION }}-${{ github.event.inputs.network || env.DEFAULT_NETWORK }}-${{ hashFiles('infra/**/*.tf', 'infra/**/*.tfvars') }}
          restore-keys: |
            terraform-${{ env.TERRAFORM_VERSION }}-${{ github.event.inputs.network || env.DEFAULT_NETWORK }}-
            terraform-${{ env.TERRAFORM_VERSION }}-

      - name: Setup Terraform Plugin Cache
        run: |
          mkdir -p ~/.terraform.d/plugin-cache
          echo 'plugin_cache_dir = "$HOME/.terraform.d/plugin-cache"' > ~/.terraformrc
      
      - name: Terraform Init
        working-directory: infra
        run: |
          terraform init -reconfigure \
            -backend-config="bucket=${{ secrets.TF_STATE_BUCKET }}" \
            -backend-config="key=health-app-${{ github.event.inputs.network || env.DEFAULT_NETWORK }}.tfstate" \
            -backend-config="region=$AWS_REGION"

      - name: Terraform Plan
        if: github.event.inputs.action == 'plan'
        working-directory: infra
        run: |
          terraform plan \
            -var-file="environments/${{ github.event.inputs.network || env.DEFAULT_NETWORK }}.tfvars" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="github_pat=${{ secrets.REPO_PAT }}" \
            -var="aws_region=${{ env.AWS_REGION }}"

      - name: Complete Redeploy
        id: redeploy
        if: github.event.inputs.action == 'redeploy'
        working-directory: infra
        run: |
          echo "üî• NUCLEAR CLEANUP - deleting everything..."
          
          # Nuclear cleanup script
          chmod +x scripts/nuclear-cleanup.sh
          ./scripts/nuclear-cleanup.sh ${{ github.event.inputs.network || env.DEFAULT_NETWORK }}
          
          # Now destroy everything else
          terraform destroy \
            -var-file="environments/${{ github.event.inputs.network || env.DEFAULT_NETWORK }}.tfvars" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="github_pat=${{ secrets.REPO_PAT }}" \
            -var="aws_region=${{ env.AWS_REGION }}" \
            -auto-approve || echo "Some resources may have been manually deleted"
          
          echo "‚è≥ Waiting 60 seconds for complete cleanup..."
          sleep 60
          
          # NUCLEAR CLEANUP - Delete everything
          echo "‚ò¢Ô∏è NUCLEAR CLEANUP - Force delete all resources..."
          
          # Delete EC2 instances for specific network
          aws ec2 describe-instances --filters "Name=tag:NetworkTier,Values=${{ github.event.inputs.network || env.DEFAULT_NETWORK }}" "Name=instance-state-name,Values=running,stopped" --query "Reservations[].Instances[].InstanceId" --output text | xargs -n1 aws ec2 terminate-instances --instance-ids || true
          
          # Delete RDS instances for specific network
          aws rds describe-db-instances --query 'DBInstances[?contains(DBInstanceIdentifier, `health-app-${{ github.event.inputs.network || env.DEFAULT_NETWORK }}`)].DBInstanceIdentifier' --output text | xargs -I {} aws rds delete-db-instance --db-instance-identifier {} --skip-final-snapshot --delete-automated-backups || true
          
          # Wait for deletions
          sleep 120
          
          # Delete VPC for specific network
          VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=health-app-${{ github.event.inputs.network || env.DEFAULT_NETWORK }}-vpc" --query "Vpcs[0].VpcId" --output text)
          if [ "$VPC_ID" != "None" ] && [ "$VPC_ID" != "" ]; then
            echo "Deleting VPC: $VPC_ID"
            
            # Delete security groups
            aws ec2 describe-security-groups --filters "Name=vpc-id,Values=$VPC_ID" --query 'SecurityGroups[?GroupName!=`default`].GroupId' --output text | xargs -I {} aws ec2 delete-security-group --group-id {} || true
            
            # Delete route tables
            aws ec2 describe-route-tables --filters "Name=vpc-id,Values=$VPC_ID" --query 'RouteTables[?Associations[0].Main!=`true`].RouteTableId' --output text | xargs -I {} aws ec2 delete-route-table --route-table-id {} || true
            
            # Delete subnets
            aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" --query 'Subnets[].SubnetId' --output text | xargs -I {} aws ec2 delete-subnet --subnet-id {} || true
            
            # Delete internet gateways
            aws ec2 describe-internet-gateways --filters "Name=attachment.vpc-id,Values=$VPC_ID" --query 'InternetGateways[].InternetGatewayId' --output text | xargs -I {} sh -c 'aws ec2 detach-internet-gateway --internet-gateway-id $1 --vpc-id '$VPC_ID' && aws ec2 delete-internet-gateway --internet-gateway-id $1' _ {} || true
            
            # Delete VPC
            aws ec2 delete-vpc --vpc-id "$VPC_ID" || true
          fi
          
          # Delete key pairs and IAM for specific network
          aws ec2 describe-key-pairs --filters "Name=key-name,Values=health-app-${{ github.event.inputs.network || env.DEFAULT_NETWORK }}-*" --query "KeyPairs[].KeyName" --output text | xargs -I {} aws ec2 delete-key-pair --key-name {} || true
          aws iam list-roles --query "Roles[?contains(RoleName, 'health-app-${{ github.event.inputs.network || env.DEFAULT_NETWORK }}')].RoleName" --output text | xargs -I {} sh -c 'aws iam list-attached-role-policies --role-name $1 --query "AttachedPolicies[].PolicyArn" --output text | xargs -I {} aws iam detach-role-policy --role-name $1 --policy-arn {} || true; aws iam list-role-policies --role-name $1 --query "PolicyNames[]" --output text | xargs -I {} aws iam delete-role-policy --role-name $1 --policy-name {} || true; aws iam list-instance-profiles-for-role --role-name $1 --query "InstanceProfiles[].InstanceProfileName" --output text | xargs -I {} sh -c "aws iam remove-role-from-instance-profile --instance-profile-name {} --role-name $1 || true; aws iam delete-instance-profile --instance-profile-name {} || true"; aws iam delete-role --role-name $1 || true' _ {} || true
          
          # Clean Terraform state
          aws s3 rm "s3://${{ secrets.TF_STATE_BUCKET }}/health-app-${{ github.event.inputs.network || env.DEFAULT_NETWORK }}.tfstate" || true
          
          # Verification for specific network
          echo "üîç Verifying cleanup for ${{ github.event.inputs.network || env.DEFAULT_NETWORK }} network..."
          VPC_COUNT=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=health-app-${{ github.event.inputs.network || env.DEFAULT_NETWORK }}-vpc" --query 'length(Vpcs)' --output text)
          INSTANCE_COUNT=$(aws ec2 describe-instances --filters "Name=tag:NetworkTier,Values=${{ github.event.inputs.network || env.DEFAULT_NETWORK }}" "Name=instance-state-name,Values=running,stopped" --query 'length(Reservations[].Instances[])' --output text)
          RDS_COUNT=$(aws rds describe-db-instances --query 'length(DBInstances[?contains(DBInstanceIdentifier, `health-app-${{ github.event.inputs.network || env.DEFAULT_NETWORK }}`)])' --output text)
          
          echo "Remaining resources:"
          echo "- VPCs: $VPC_COUNT"
          echo "- EC2 Instances: $INSTANCE_COUNT"
          echo "- RDS Instances: $RDS_COUNT"
          
          if [ "$VPC_COUNT" == "0" ] && [ "$INSTANCE_COUNT" == "0" ] && [ "$RDS_COUNT" == "0" ]; then
            echo "üéâ NUCLEAR CLEANUP SUCCESSFUL - All resources deleted!"
          else
            echo "‚ö†Ô∏è Some resources remain - continuing anyway..."
          fi
          
          echo "‚úÖ Complete cleanup done, deploying fresh infrastructure..."
          
          # Fresh deployment
          terraform apply \
            -var-file="environments/${{ github.event.inputs.network || env.DEFAULT_NETWORK }}.tfvars" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="github_pat=${{ secrets.REPO_PAT }}" \
            -var="aws_region=${{ env.AWS_REGION }}" \
            -var="network_tier=${{ github.event.inputs.network || env.DEFAULT_NETWORK }}" \
            -auto-approve
          
          # Output cluster IPs
          if [ "${{ github.event.inputs.network || env.DEFAULT_NETWORK }}" == "lower" ]; then
            echo "dev_cluster_ip=$(terraform output -raw dev_cluster_ip)" >> $GITHUB_OUTPUT
            echo "test_cluster_ip=$(terraform output -raw test_cluster_ip)" >> $GITHUB_OUTPUT
          else
            echo "cluster_ip=$(terraform output -raw k3s_instance_ip)" >> $GITHUB_OUTPUT
          fi
      
      - name: Import Existing Resources
        if: github.event.inputs.action == 'deploy'
        working-directory: infra
        run: |
          echo "üîÑ Importing existing RDS resources if they exist..."
          
          # Import existing RDS resources
          terraform import 'module.rds[0].aws_db_subnet_group.health_db' health-app-shared-db-subnet-group 2>/dev/null || echo "Subnet group doesn't exist, will create"
          terraform import 'module.rds[0].aws_db_parameter_group.health_db' health-app-shared-db-params 2>/dev/null || echo "Parameter group doesn't exist, will create"
          terraform import 'module.rds[0].aws_db_instance.health_db' health-app-shared-db 2>/dev/null || echo "RDS instance doesn't exist, will create"
          
          echo "‚úÖ Import step completed"
      
      - name: Incremental Deploy
        id: deploy
        if: github.event.inputs.action == 'deploy'
        working-directory: infra
        run: |
          echo "üöÄ Incremental deployment - applying changes only..."
          
          terraform apply \
            -var-file="environments/${{ github.event.inputs.network || env.DEFAULT_NETWORK }}.tfvars" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="github_pat=${{ secrets.REPO_PAT }}" \
            -var="aws_region=${{ env.AWS_REGION }}" \
            -var="network_tier=${{ github.event.inputs.network || env.DEFAULT_NETWORK }}" \
            -auto-approve
          
          # Output cluster IPs
          if [ "${{ github.event.inputs.network || env.DEFAULT_NETWORK }}" == "lower" ]; then
            echo "dev_cluster_ip=$(terraform output -raw dev_cluster_ip)" >> $GITHUB_OUTPUT
            echo "test_cluster_ip=$(terraform output -raw test_cluster_ip)" >> $GITHUB_OUTPUT
          else
            echo "cluster_ip=$(terraform output -raw k3s_instance_ip)" >> $GITHUB_OUTPUT
          fi

      - name: Terraform Destroy
        if: github.event.inputs.action == 'destroy'
        working-directory: infra
        run: |
          if [ "${{ github.event.inputs.action || env.DEFAULT_ACTION }}" == "destroy" ] && [ "${{ github.event.inputs.confirm_destroy }}" != "DESTROY" ]; then
            echo "‚ùå Type 'DESTROY' to confirm explicit destroy action"
            exit 1
          fi
          
          echo "üî• Destroying infrastructure..."
          
          terraform destroy \
            -var-file="environments/${{ github.event.inputs.network || env.DEFAULT_NETWORK }}.tfvars" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="github_pat=${{ secrets.REPO_PAT }}" \
            -var="aws_region=${{ env.AWS_REGION }}" \
            -var="network_tier=${{ github.event.inputs.network || env.DEFAULT_NETWORK }}" \
            -auto-approve   
  # Step 3: Setup Runner Service
  setup-runner-service:
    needs: [terraform-deploy]
    if: (github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy')
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
      
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Fix Runner Service
        run: |
          echo "üîß Fixing runner service..."
          
          # Get runner instance
          INSTANCE_ID=$(aws ec2 describe-instances \
            --filters "Name=tag:Name,Values=*runner*${{ github.event.inputs.network || env.DEFAULT_NETWORK }}*" "Name=instance-state-name,Values=running" \
            --query 'Reservations[0].Instances[0].InstanceId' --output text)
          
          if [ "$INSTANCE_ID" != "None" ] && [ -n "$INSTANCE_ID" ]; then
            echo "Found runner instance: $INSTANCE_ID"
            
            # Simple command to fix runner service
            aws ssm send-command \
              --instance-ids "$INSTANCE_ID" \
              --document-name "AWS-RunShellScript" \
              --parameters 'commands=["cd /home/ubuntu/actions-runner && sudo ./svc.sh install ubuntu && sudo ./svc.sh start"]' \
              --region ${{ env.AWS_REGION }}
            
            echo "‚úÖ Runner service fix command sent"
          else
            echo "‚ö†Ô∏è No runner instance found"
          fi
      
      - name: Get Instance IDs
        id: instances
        run: |
          # Wait for instances to be ready
          echo "‚è≥ Waiting 30 seconds for instances to initialize..."
          sleep 30
          
          # List all instances to debug
          echo "üîç Debugging instance discovery..."
          aws ec2 describe-instances --query 'Reservations[].Instances[].[InstanceId,State.Name,Tags[?Key==`Name`].Value|[0]]' --output table
          
          if [ "${{ github.event.inputs.network || env.DEFAULT_NETWORK }}" == "lower" ]; then
            # Try multiple tag patterns for lower tier
            DEV_INSTANCE=$(aws ec2 describe-instances \
              --filters "Name=tag:Name,Values=health-app-*dev*" "Name=instance-state-name,Values=running" \
              --query 'Reservations[0].Instances[0].InstanceId' --output text 2>/dev/null)
            
            if [ "$DEV_INSTANCE" = "None" ] || [ -z "$DEV_INSTANCE" ]; then
              DEV_INSTANCE=$(aws ec2 describe-instances \
                --filters "Name=tag:Name,Values=*dev*k3s*" "Name=instance-state-name,Values=running" \
                --query 'Reservations[0].Instances[0].InstanceId' --output text 2>/dev/null || echo "None")
            fi
            
            TEST_INSTANCE=$(aws ec2 describe-instances \
              --filters "Name=tag:Name,Values=health-app-*test*" "Name=instance-state-name,Values=running" \
              --query 'Reservations[0].Instances[0].InstanceId' --output text 2>/dev/null)
            
            if [ "$TEST_INSTANCE" = "None" ] || [ -z "$TEST_INSTANCE" ]; then
              TEST_INSTANCE=$(aws ec2 describe-instances \
                --filters "Name=tag:Name,Values=*test*k3s*" "Name=instance-state-name,Values=running" \
                --query 'Reservations[0].Instances[0].InstanceId' --output text 2>/dev/null || echo "None")
            fi
            
            echo "Found dev instance: $DEV_INSTANCE"
            echo "Found test instance: $TEST_INSTANCE"
            echo "dev_instance=$DEV_INSTANCE" >> $GITHUB_OUTPUT
            echo "test_instance=$TEST_INSTANCE" >> $GITHUB_OUTPUT
          else
            INSTANCE=$(aws ec2 describe-instances \
              --filters "Name=tag:Name,Values=health-app-*k3s*" "Name=instance-state-name,Values=running" \
              --query 'Reservations[0].Instances[0].InstanceId' --output text 2>/dev/null || echo "None")
            
            echo "Found instance: $INSTANCE"
            echo "instance=$INSTANCE" >> $GITHUB_OUTPUT
          fi
      
      - name: Fix Runner Service (Lower Tier)
        if: (github.event.inputs.network || env.DEFAULT_NETWORK) == 'lower'
        run: |
          RUNNER_FIX_SCRIPT='#!/bin/bash
          set -e
          cd /home/ubuntu/actions-runner || exit 1
          if [ ! -f "svc.sh" ]; then
            cat > svc.sh << "SVCEOF"
          #!/bin/bash
          SVC_NAME="actions.runner.$(cat .runner | jq -r ".gitHubUrl" | sed "s/https:\/\/github.com\///").$(cat .runner | jq -r ".runnerName").service"
          USER_ID=${2:-ubuntu}
          RUNNER_ROOT=$(pwd)
          case $1 in
              install)
                  sudo tee /etc/systemd/system/$SVC_NAME > /dev/null << UNIT
          [Unit]
          Description=GitHub Actions Runner
          After=network.target
          [Service]
          ExecStart=$RUNNER_ROOT/run.sh
          User=$USER_ID
          WorkingDirectory=$RUNNER_ROOT
          KillMode=process
          KillSignal=SIGTERM
          TimeoutStopSec=5min
          [Install]
          WantedBy=multi-user.target
          UNIT
                  sudo systemctl daemon-reload
                  sudo systemctl enable $SVC_NAME
                  ;;
              start) sudo systemctl start $SVC_NAME ;;
              stop) sudo systemctl stop $SVC_NAME ;;
              status) sudo systemctl status $SVC_NAME ;;
          esac
          SVCEOF
            chmod +x svc.sh
          fi
          ./svc.sh install ubuntu
          ./svc.sh start'
          
          # Fix dev instance
          if [ "${{ steps.instances.outputs.dev_instance }}" != "None" ] && [ "${{ steps.instances.outputs.dev_instance }}" != "" ]; then
            echo "üîß Fixing dev runner service..."
            
            COMMAND_ID=$(aws ssm send-command \
              --instance-ids "${{ steps.instances.outputs.dev_instance }}" \
              --document-name "AWS-RunShellScript" \
              --parameters "commands=[\"$RUNNER_FIX_SCRIPT\"]" \
              --query 'Command.CommandId' --output text)
            echo "‚úÖ Runner fix command sent for dev instance"
          fi
          
          # Fix test instance
          if [ "${{ steps.instances.outputs.test_instance }}" != "None" ] && [ "${{ steps.instances.outputs.test_instance }}" != "" ]; then
            echo "üîß Fixing test runner service..."
            
            COMMAND_ID=$(aws ssm send-command \
              --instance-ids "${{ steps.instances.outputs.test_instance }}" \
              --document-name "AWS-RunShellScript" \
              --parameters "commands=[\"$RUNNER_FIX_SCRIPT\"]" \
              --query 'Command.CommandId' --output text)
            echo "‚úÖ Runner fix command sent for test instance"
          fi
      
      - name: Fix Runner Service (Higher Tier)
        if: (github.event.inputs.network || env.DEFAULT_NETWORK) != 'lower'
        run: |
          RUNNER_FIX_SCRIPT='#!/bin/bash
          cd /home/ubuntu/actions-runner || exit 1
          sudo ./svc.sh install ubuntu
          sudo ./svc.sh start'
          
          if [ "${{ steps.instances.outputs.instance }}" != "None" ] && [ "${{ steps.instances.outputs.instance }}" != "" ]; then
            echo "üîß Fixing runner service..."
            COMMAND_ID=$(aws ssm send-command \
              --instance-ids "${{ steps.instances.outputs.instance }}" \
              --document-name "AWS-RunShellScript" \
              --parameters "commands=[\"$RUNNER_FIX_SCRIPT\"]" \
              --query 'Command.CommandId' --output text)
            echo "‚úÖ Runner fix command sent"
          else
            echo "‚ö†Ô∏è No instance found for higher tier"
          fi

  # Step 3: Configure Security Groups ‚Üí Sets up cross-SG references
  configure-security-groups:
    needs: [terraform-deploy, setup-runner-service]
    if: (github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy')
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
      
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Configure Security Groups
        run: |
          echo "üîß Step 2: Configuring security groups for cluster access..."
          
          # Wait for instances to be ready
          echo "‚è≥ Waiting 60 seconds for instances to initialize..."
          sleep 60
          
          # Configure cross-SG references for runner access
          echo "üîê Setting up cross-SG references for secure access..."
          if [ "${{ github.event.inputs.network || env.DEFAULT_NETWORK }}" == "lower" ]; then
            # Get security group IDs
            DEV_SG=$(aws ec2 describe-instances \
              --filters "Name=tag:Name,Values=health-app-lower-dev-k3s-node-v2" \
              --query 'Reservations[0].Instances[0].SecurityGroups[0].GroupId' \
              --output text 2>/dev/null)
            
            TEST_SG=$(aws ec2 describe-instances \
              --filters "Name=tag:Name,Values=health-app-lower-test-k3s-node-v2" \
              --query 'Reservations[0].Instances[0].SecurityGroups[0].GroupId' \
              --output text 2>/dev/null)
            
            RUNNER_SG=$(aws ec2 describe-instances \
              --filters "Name=tag:Name,Values=health-app-runner-lower" \
              --query 'Reservations[0].Instances[0].SecurityGroups[0].GroupId' \
              --output text 2>/dev/null)
            
            # Add cross-SG references for secure access
            for SG in "$DEV_SG" "$TEST_SG"; do
              if [ "$SG" != "None" ] && [ -n "$SG" ] && [ "$RUNNER_SG" != "None" ] && [ -n "$RUNNER_SG" ]; then
                echo "Adding runner access to K3s API on $SG..."
                aws ec2 authorize-security-group-ingress \
                  --group-id "$SG" \
                  --ip-permissions '[{"IpProtocol":"tcp","FromPort":6443,"ToPort":6443,"UserIdGroupPairs":[{"GroupId":"'$RUNNER_SG'","Description":"K3s API access from runner"}]}]' 2>/dev/null || echo "Runner API rule may already exist"
                
                aws ec2 authorize-security-group-ingress \
                  --group-id "$SG" \
                  --ip-permissions '[{"IpProtocol":"tcp","FromPort":22,"ToPort":22,"UserIdGroupPairs":[{"GroupId":"'$RUNNER_SG'","Description":"SSH access from runner"}]}]' 2>/dev/null || echo "Runner SSH rule may already exist"
              fi
            done
            
            echo "‚úÖ Cross-SG references configured for secure access"
          fi
          
          echo "‚úÖ Step 2 Complete: Security group configuration done!"

  # Step 4: Install and Configure K3s ‚Üí Install K3s using the newly created GitHub runner (inside the VPC)
  install-k3s:
    needs: [terraform-deploy, configure-security-groups]
    if: (github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy')
    runs-on: self-hosted
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Install and Configure K3s
        env:
          DEV_CLUSTER_IP: ${{ needs.terraform-deploy.outputs.dev_cluster_ip }}
          TEST_CLUSTER_IP: ${{ needs.terraform-deploy.outputs.test_cluster_ip }}
          CLUSTER_IP: ${{ needs.terraform-deploy.outputs.cluster_ip }}
        run: |
          echo "‚ò∏Ô∏è Step 4: Installing and configuring K3s on cluster nodes using self-hosted runner..."
          
          # Function to install k3s on a node using script
          install_k3s() {
            local IP=$1
            local ENV=$2
            
            echo "Installing K3s on $ENV cluster ($IP)..."
            
            # Test SSH connectivity with retries
            echo "Testing SSH connectivity to $IP..."
            for retry in {1..5}; do
              if timeout 15 ssh -o StrictHostKeyChecking=no -o ConnectTimeout=10 ubuntu@$IP "echo 'SSH connection successful'" 2>/dev/null; then
                echo "‚úÖ SSH connection established to $IP"
                break
              fi
              echo "‚è≥ SSH attempt $retry/5 failed, retrying in 10 seconds..."
              sleep 10
              if [ $retry -eq 5 ]; then
                echo "‚ùå Cannot connect to $IP via SSH after 5 attempts"
                return 1
              fi
            done
            
            # Copy and execute K3s installation script
            echo "Copying K3s installation script to $IP..."
            scp -o StrictHostKeyChecking=no scripts/install-k3s.sh ubuntu@$IP:/tmp/
            
            echo "Executing K3s installation on $IP..."
            ssh -o StrictHostKeyChecking=no ubuntu@$IP "chmod +x /tmp/install-k3s.sh && sudo /tmp/install-k3s.sh"
            
            if [ $? -eq 0 ]; then
              echo "‚úÖ K3s installation completed on $ENV cluster"
            else
              echo "‚ùå K3s installation failed on $ENV cluster"
              return 1
            fi
          }
          
          # Install k3s based on network tier
          if [ "${{ github.event.inputs.network || env.DEFAULT_NETWORK }}" == "lower" ]; then
            success=true
            
            if [ -n "$DEV_CLUSTER_IP" ]; then
              install_k3s "$DEV_CLUSTER_IP" "dev" || success=false
            fi
            
            if [ -n "$TEST_CLUSTER_IP" ]; then
              install_k3s "$TEST_CLUSTER_IP" "test" || success=false
            fi
            
            if [ "$success" = false ]; then
              echo "‚ùå Some K3s installations failed"
              exit 1
            fi
          else
            if [ -n "$CLUSTER_IP" ]; then
              install_k3s "$CLUSTER_IP" "${{ github.event.inputs.network || env.DEFAULT_NETWORK }}"
            else
              echo "‚ùå Cluster IP not available"
              exit 1
            fi
          fi
          
          echo "üéâ Step 4 Complete: K3s installation and configuration done!"

  # Step 5: Verify K3s API Availability ‚Üí Tests if K3s is responding
  verify-k3s-api:
    needs: [terraform-deploy, install-k3s]
    if: (github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy')
    runs-on: self-hosted
    timeout-minutes: 20
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Verify K3s API Availability
        env:
          DEV_CLUSTER_IP: ${{ needs.terraform-deploy.outputs.dev_cluster_ip }}
          TEST_CLUSTER_IP: ${{ needs.terraform-deploy.outputs.test_cluster_ip }}
          CLUSTER_IP: ${{ needs.terraform-deploy.outputs.cluster_ip }}
        run: |
          echo "üîç Step 5: Verifying K3s API availability..."
          
          # Function to test K3s API
          test_k3s_api() {
            local IP=$1
            local NAME=$2
            echo "Testing $NAME cluster at $IP..."
            
            for i in {1..20}; do
              if timeout 10 curl -k -s "https://$IP:6443/version" >/dev/null 2>&1; then
                echo "‚úÖ $NAME cluster API is ready"
                return 0
              fi
              echo "‚è≥ $NAME cluster not ready, waiting... ($i/20)"
              sleep 30
            done
            echo "‚ùå $NAME cluster API not ready after 10 minutes"
            return 1
          }
          
          # Test clusters based on network tier
          if [ "${{ github.event.inputs.network || env.DEFAULT_NETWORK }}" == "lower" ]; then
            success=true
            
            if [ -n "$DEV_CLUSTER_IP" ]; then
              test_k3s_api "$DEV_CLUSTER_IP" "dev" || success=false
            fi
            
            if [ -n "$TEST_CLUSTER_IP" ]; then
              test_k3s_api "$TEST_CLUSTER_IP" "test" || success=false
            fi
            
            if [ "$success" = false ]; then
              echo "‚ö†Ô∏è Some clusters not ready, but continuing..."
            fi
          else
            if [ -n "$CLUSTER_IP" ]; then
              test_k3s_api "$CLUSTER_IP" "${{ github.event.inputs.network || env.DEFAULT_NETWORK }}"
            fi
          fi
          
          echo "‚úÖ Step 5 Complete: K3s API verification done!"

  # Step 6: Setup Kubeconfig ‚Üí Configures cluster access
  setup-kubeconfig:
    needs: [terraform-deploy, verify-k3s-api]
    if: (github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy')
    runs-on: self-hosted
    
    steps:
      - uses: actions/checkout@v4
      
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Setup Kubeconfig
        env:
          SSH_PRIVATE_KEY: ${{ secrets.SSH_PRIVATE_KEY }}
          DEV_CLUSTER_IP: ${{ needs.terraform-deploy.outputs.dev_cluster_ip }}
          TEST_CLUSTER_IP: ${{ needs.terraform-deploy.outputs.test_cluster_ip }}
          CLUSTER_IP: ${{ needs.terraform-deploy.outputs.cluster_ip }}
        run: |
          echo "‚öôÔ∏è Step 5: Setting up kubeconfig for cluster access..."
          
          # Setup SSH key
          mkdir -p ~/.ssh
          echo "$SSH_PRIVATE_KEY" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          
          # Function to setup kubeconfig
          setup_kubeconfig() {
            local IP=$1
            local ENV=$2
            
            echo "Setting up kubeconfig for $ENV cluster..."
            
            # Get kubeconfig from cluster
            ssh -o StrictHostKeyChecking=no ubuntu@$IP "sudo cat /etc/rancher/k3s/k3s.yaml" > /tmp/kubeconfig-$ENV.yaml
            
            # Replace localhost with actual IP
            sed -i "s/127.0.0.1/$IP/g" /tmp/kubeconfig-$ENV.yaml
            
            # Store in Parameter Store
            aws ssm put-parameter \
              --name "/health-app/kubeconfig/$ENV" \
              --value "$(cat /tmp/kubeconfig-$ENV.yaml)" \
              --type "SecureString" \
              --overwrite
            
            echo "‚úÖ Kubeconfig stored in Parameter Store for $ENV"
          }
          
          # Setup kubeconfig based on network tier
          if [ "${{ github.event.inputs.network || env.DEFAULT_NETWORK }}" == "lower" ]; then
            if [ -n "$DEV_CLUSTER_IP" ]; then
              setup_kubeconfig "$DEV_CLUSTER_IP" "dev"
            fi
            
            if [ -n "$TEST_CLUSTER_IP" ]; then
              setup_kubeconfig "$TEST_CLUSTER_IP" "test"
            fi
          else
            if [ -n "$CLUSTER_IP" ]; then
              setup_kubeconfig "$CLUSTER_IP" "${{ github.event.inputs.network }}"
            fi
          fi
          
          # Cleanup SSH key
          rm -f ~/.ssh/id_rsa
          
          echo "‚úÖ Step 6 Complete: Kubeconfig setup done!"

  # Step 7: Validate Cluster Deployment ‚Üí Final validation
  validate-cluster:
    needs: [terraform-deploy, setup-kubeconfig]
    if: (github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy')
    runs-on: self-hosted
    
    steps:
      - uses: actions/checkout@v4
      
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Validate Cluster Deployment
        env:
          DEV_CLUSTER_IP: ${{ needs.terraform-deploy.outputs.dev_cluster_ip }}
          TEST_CLUSTER_IP: ${{ needs.terraform-deploy.outputs.test_cluster_ip }}
          CLUSTER_IP: ${{ needs.terraform-deploy.outputs.cluster_ip }}
        run: |
          echo "‚úÖ Step 6: Validating cluster deployment..."
          
          # Function to validate cluster
          validate_cluster() {
            local ENV=$1
            
            echo "Validating $ENV cluster..."
            
            # Get kubeconfig from Parameter Store
            aws ssm get-parameter \
              --name "/health-app/kubeconfig/$ENV" \
              --with-decryption \
              --query 'Parameter.Value' \
              --output text > /tmp/kubeconfig-$ENV.yaml
            
            # Test kubectl connectivity
            if kubectl --kubeconfig=/tmp/kubeconfig-$ENV.yaml get nodes >/dev/null 2>&1; then
              echo "‚úÖ $ENV cluster validation successful"
              kubectl --kubeconfig=/tmp/kubeconfig-$ENV.yaml get nodes
            else
              echo "‚ùå $ENV cluster validation failed"
              return 1
            fi
          }
          
          # Install kubectl
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/
          
          # Validate clusters based on network tier
          if [ "${{ github.event.inputs.network || env.DEFAULT_NETWORK }}" == "lower" ]; then
            success=true
            
            if [ -n "$DEV_CLUSTER_IP" ]; then
              validate_cluster "dev" || success=false
            fi
            
            if [ -n "$TEST_CLUSTER_IP" ]; then
              validate_cluster "test" || success=false
            fi
            
            if [ "$success" = false ]; then
              echo "‚ùå Some cluster validations failed"
              exit 1
            fi
          else
            if [ -n "$CLUSTER_IP" ]; then
              validate_cluster "${{ github.event.inputs.network }}"
            fi
          fi
          
          echo "‚úÖ Step 7 Complete: Cluster validation done!"

  # Step 8: Setup K8s Secrets ‚Üí Creates Kubernetes secrets
  setup-k8s-secrets:
    needs: [terraform-deploy, validate-cluster]
    if: (github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy')
    runs-on: self-hosted
    
    steps:
      - uses: actions/checkout@v4
      
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Setup K8s Secrets
        env:
          DEV_CLUSTER_IP: ${{ needs.terraform-deploy.outputs.dev_cluster_ip }}
          TEST_CLUSTER_IP: ${{ needs.terraform-deploy.outputs.test_cluster_ip }}
          CLUSTER_IP: ${{ needs.terraform-deploy.outputs.cluster_ip }}
        run: |
          echo "üîê Step 7: Setting up Kubernetes secrets..."
          
          # Function to setup secrets
          setup_secrets() {
            local ENV=$1
            
            echo "Setting up secrets for $ENV cluster..."
            
            # Get kubeconfig from Parameter Store
            aws ssm get-parameter \
              --name "/health-app/kubeconfig/$ENV" \
              --with-decryption \
              --query 'Parameter.Value' \
              --output text > /tmp/kubeconfig-$ENV.yaml
            
            # Create namespace if it doesn't exist
            kubectl --kubeconfig=/tmp/kubeconfig-$ENV.yaml create namespace $ENV || echo "Namespace $ENV already exists"
            
            # Create database secret
            kubectl --kubeconfig=/tmp/kubeconfig-$ENV.yaml create secret generic db-credentials \
              --from-literal=username=healthapp \
              --from-literal=password=healthapp123 \
              --from-literal=host=health-app-shared-db.cluster-cqtoaos7cxkp.ap-south-1.rds.amazonaws.com \
              --from-literal=port=3306 \
              --from-literal=database=healthappdb \
              --namespace=$ENV \
              --dry-run=client -o yaml | kubectl --kubeconfig=/tmp/kubeconfig-$ENV.yaml apply -f -
            
            echo "‚úÖ Secrets created for $ENV cluster"
          }
          
          # Install kubectl
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/
          
          # Setup secrets based on network tier
          if [ "${{ github.event.inputs.network || env.DEFAULT_NETWORK }}" == "lower" ]; then
            if [ -n "$DEV_CLUSTER_IP" ]; then
              setup_secrets "dev"
            fi
            
            if [ -n "$TEST_CLUSTER_IP" ]; then
              setup_secrets "test"
            fi
          else
            if [ -n "$CLUSTER_IP" ]; then
              setup_secrets "${{ github.event.inputs.network }}"
            fi
          fi
          
          echo "‚úÖ Step 8 Complete: K8s secrets setup done!"

  # Step 9: Configure Network Security ‚Üí Additional security rules
  configure-network-security:
    needs: [terraform-deploy, setup-k8s-secrets]
    if: (github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy')
    runs-on: self-hosted
    
    steps:
      - uses: actions/checkout@v4
      
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Configure Network Security
        run: |
          echo "üõ°Ô∏è Step 8: Configuring additional network security..."
          
          # Run network security configuration script
          if [ -f "scripts/setup-cross-sg-references.sh" ]; then
            chmod +x scripts/setup-cross-sg-references.sh
            ./scripts/setup-cross-sg-references.sh ${{ github.event.inputs.network || env.DEFAULT_NETWORK }}
          else
            echo "Network security script not found, skipping..."
          fi
          
          echo "‚úÖ Step 9 Complete: Network security configuration done!"

  # Step 10: Test Connectivity ‚Üí End-to-end testing
  test-connectivity:
    needs: [terraform-deploy, configure-network-security]
    if: (github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy')
    runs-on: self-hosted
    
    steps:
      - uses: actions/checkout@v4
      
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Test Connectivity
        env:
          DEV_CLUSTER_IP: ${{ needs.terraform-deploy.outputs.dev_cluster_ip }}
          TEST_CLUSTER_IP: ${{ needs.terraform-deploy.outputs.test_cluster_ip }}
          CLUSTER_IP: ${{ needs.terraform-deploy.outputs.cluster_ip }}
        run: |
          echo "üîç Step 9: Testing end-to-end connectivity..."
          
          # Run connectivity tests
          if [ -f "scripts/test-network-connectivity.sh" ]; then
            chmod +x scripts/test-network-connectivity.sh
            ./scripts/test-network-connectivity.sh ${{ github.event.inputs.network || env.DEFAULT_NETWORK }}
          else
            echo "Connectivity test script not found, performing basic tests..."
            
            # Basic connectivity tests
            if [ "${{ github.event.inputs.network || env.DEFAULT_NETWORK }}" == "lower" ]; then
              if [ -n "$DEV_CLUSTER_IP" ]; then
                echo "Testing dev cluster connectivity..."
                timeout 10 curl -k -s "https://$DEV_CLUSTER_IP:6443/version" && echo "‚úÖ Dev cluster API accessible" || echo "‚ùå Dev cluster API not accessible"
              fi
              
              if [ -n "$TEST_CLUSTER_IP" ]; then
                echo "Testing test cluster connectivity..."
                timeout 10 curl -k -s "https://$TEST_CLUSTER_IP:6443/version" && echo "‚úÖ Test cluster API accessible" || echo "‚ùå Test cluster API not accessible"
              fi
            else
              if [ -n "$CLUSTER_IP" ]; then
                echo "Testing cluster connectivity..."
                timeout 10 curl -k -s "https://$CLUSTER_IP:6443/version" && echo "‚úÖ Cluster API accessible" || echo "‚ùå Cluster API not accessible"
              fi
            fi
          fi
          
          echo "üéâ Step 10 Complete: Infrastructure deployment pipeline finished!"
          echo "üìã Summary: All 10 steps completed successfully"
          echo "   1. ‚úÖ Terraform ‚Üí Created instances"
          echo "   2. ‚úÖ Configure Security Groups ‚Üí Set up cross-SG references"
          echo "   3. ‚úÖ Create GitHub Runner ‚Üí Launched EC2 and registered as self-hosted runner"
          echo "   4. ‚úÖ Install and Configure K3s ‚Üí Installed K3s software using self-hosted runner"
          echo "   5. ‚úÖ Verify K3s API Availability ‚Üí Tested K3s response"
          echo "   6. ‚úÖ Setup Kubeconfig ‚Üí Configured cluster access"
          echo "   7. ‚úÖ Validate Cluster Deployment ‚Üí Final validation"
          echo "   8. ‚úÖ Setup K8s Secrets ‚Üí Created Kubernetes secrets"
          echo "   9. ‚úÖ Configure Network Security ‚Üí Additional security rules"
          echo "   10. ‚úÖ Test Connectivity ‚Üí End-to-end testing"