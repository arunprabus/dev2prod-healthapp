name: Core Infrastructure

on:
  workflow_dispatch:
    inputs:
      action:
        description: 'Action'
        required: true
        type: choice
        options:
          - deploy
          - destroy
          - plan
          - redeploy
      network:
        description: 'Network Tier'
        required: true
        default: 'lower'
        type: choice
        options:
          - lower
          - higher
          - monitoring
      confirm_destroy:
        description: 'Type "DESTROY" to confirm'
        required: false
        type: string

env:
  AWS_REGION: ap-south-1
  TERRAFORM_VERSION: 1.6.0

jobs:
  infrastructure:
    runs-on: ubuntu-latest
    outputs:
      cluster_ip: ${{ steps.redeploy.outputs.cluster_ip || steps.deploy.outputs.cluster_ip }}
      dev_cluster_ip: ${{ steps.redeploy.outputs.dev_cluster_ip || steps.deploy.outputs.dev_cluster_ip }}
      test_cluster_ip: ${{ steps.redeploy.outputs.test_cluster_ip || steps.deploy.outputs.test_cluster_ip }}

    steps:
      - uses: actions/checkout@v4

      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}
          terraform_wrapper: false

      - name: Cache Terraform
        uses: actions/cache@v3
        with:
          path: |
            infra/.terraform
            infra/.terraform.lock.hcl
            ~/.terraform.d/plugin-cache
          key: terraform-${{ env.TERRAFORM_VERSION }}-${{ github.event.inputs.network }}-${{ hashFiles('infra/**/*.tf', 'infra/**/*.tfvars') }}
          restore-keys: |
            terraform-${{ env.TERRAFORM_VERSION }}-${{ github.event.inputs.network }}-
            terraform-${{ env.TERRAFORM_VERSION }}-

      - name: Setup Terraform Plugin Cache
        run: |
          mkdir -p ~/.terraform.d/plugin-cache
          echo 'plugin_cache_dir = "$HOME/.terraform.d/plugin-cache"' > ~/.terraformrc
      
      - name: Terraform Init
        working-directory: infra
        run: |
          terraform init -reconfigure \
            -backend-config="bucket=${{ secrets.TF_STATE_BUCKET }}" \
            -backend-config="key=health-app-${{ github.event.inputs.network }}.tfstate" \
            -backend-config="region=$AWS_REGION"

      - name: Terraform Plan
        if: github.event.inputs.action == 'plan'
        working-directory: infra
        run: |
          terraform plan \
            -var-file="environments/${{ github.event.inputs.network }}.tfvars" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="github_pat=${{ secrets.REPO_PAT }}" \
            -var="aws_region=${{ env.AWS_REGION }}"

      - name: Complete Redeploy
        id: redeploy
        if: github.event.inputs.action == 'redeploy'
        working-directory: infra
        run: |
          echo "üî• Complete infrastructure redeploy - deleting everything..."
          
          # Manual cleanup first
          echo "üßπ Manual cleanup of persistent resources..."
          aws rds delete-db-instance --db-instance-identifier health-app-shared-db --skip-final-snapshot --delete-automated-backups || true
          
          # Wait for RDS deletion
          echo "‚è≥ Waiting for RDS deletion..."
          for i in {1..15}; do
            if ! aws rds describe-db-instances --db-instance-identifier health-app-shared-db >/dev/null 2>&1; then
              echo "‚úÖ RDS instance deleted"
              break
            fi
            echo "Waiting for RDS deletion... ($i/15)"
            sleep 20
          done
          
          # Delete parameter group and subnet group
          aws rds delete-db-parameter-group --db-parameter-group-name health-app-shared-db-params || true
          aws rds delete-db-subnet-group --db-subnet-group-name health-app-shared-db-subnet-group || true
          aws kms delete-alias --alias-name alias/health-app-rds-export || true
          
          # Force remove Terraform state for problematic resources
          terraform state rm 'module.rds[0].aws_db_parameter_group.health_db' || true
          terraform state rm 'module.rds[0].aws_db_subnet_group.health_db' || true
          
          # Now destroy everything else
          terraform destroy \
            -var-file="environments/${{ github.event.inputs.network }}.tfvars" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="github_pat=${{ secrets.REPO_PAT }}" \
            -var="aws_region=${{ env.AWS_REGION }}" \
            -auto-approve || echo "Some resources may have been manually deleted"
          
          echo "‚è≥ Waiting 60 seconds for complete cleanup..."
          sleep 60
          
          echo "‚úÖ Complete cleanup done, deploying fresh infrastructure..."
          
          # Fresh deployment
          terraform apply \
            -var-file="environments/${{ github.event.inputs.network }}.tfvars" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="github_pat=${{ secrets.REPO_PAT }}" \
            -var="aws_region=${{ env.AWS_REGION }}" \
            -auto-approve
          
          # Output cluster IPs
          if [ "${{ github.event.inputs.network }}" == "lower" ]; then
            echo "dev_cluster_ip=$(terraform output -raw dev_cluster_ip)" >> $GITHUB_OUTPUT
            echo "test_cluster_ip=$(terraform output -raw test_cluster_ip)" >> $GITHUB_OUTPUT
          else
            echo "cluster_ip=$(terraform output -raw k3s_instance_ip)" >> $GITHUB_OUTPUT
          fi
      
      - name: Import Existing Resources
        if: github.event.inputs.action == 'deploy'
        working-directory: infra
        run: |
          echo "üîÑ Importing existing RDS resources if they exist..."
          
          # Import existing RDS resources
          terraform import 'module.rds[0].aws_db_subnet_group.health_db' health-app-shared-db-subnet-group 2>/dev/null || echo "Subnet group doesn't exist, will create"
          terraform import 'module.rds[0].aws_db_parameter_group.health_db' health-app-shared-db-params 2>/dev/null || echo "Parameter group doesn't exist, will create"
          terraform import 'module.rds[0].aws_db_instance.health_db' health-app-shared-db 2>/dev/null || echo "RDS instance doesn't exist, will create"
          
          echo "‚úÖ Import step completed"
      
      - name: Incremental Deploy
        id: deploy
        if: github.event.inputs.action == 'deploy'
        working-directory: infra
        run: |
          echo "üöÄ Incremental deployment - applying changes only..."
          
          terraform apply \
            -var-file="environments/${{ github.event.inputs.network }}.tfvars" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="github_pat=${{ secrets.REPO_PAT }}" \
            -var="aws_region=${{ env.AWS_REGION }}" \
            -auto-approve
          
          # Output cluster IPs
          if [ "${{ github.event.inputs.network }}" == "lower" ]; then
            echo "dev_cluster_ip=$(terraform output -raw dev_cluster_ip)" >> $GITHUB_OUTPUT
            echo "test_cluster_ip=$(terraform output -raw test_cluster_ip)" >> $GITHUB_OUTPUT
          else
            echo "cluster_ip=$(terraform output -raw k3s_instance_ip)" >> $GITHUB_OUTPUT
          fi

      - name: Configure Security Groups
        if: github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy'
        run: |
          echo "üîß Configuring security groups for cluster access..."
          
          # Wait for instances to be ready
          echo "‚è≥ Waiting 60 seconds for instances to initialize..."
          sleep 60
          
          # Configure cross-SG references for runner access
          echo "üîê Setting up cross-SG references for secure access..."
          if [ "${{ github.event.inputs.network }}" == "lower" ]; then
            # Get security group IDs
            DEV_SG=$(aws ec2 describe-instances \
              --filters "Name=tag:Name,Values=health-app-lower-dev-k3s-node-v2" \
              --query 'Reservations[0].Instances[0].SecurityGroups[0].GroupId' \
              --output text 2>/dev/null)
            
            TEST_SG=$(aws ec2 describe-instances \
              --filters "Name=tag:Name,Values=health-app-lower-test-k3s-node-v2" \
              --query 'Reservations[0].Instances[0].SecurityGroups[0].GroupId' \
              --output text 2>/dev/null)
            
            RUNNER_SG=$(aws ec2 describe-instances \
              --filters "Name=tag:Name,Values=health-app-runner-lower" \
              --query 'Reservations[0].Instances[0].SecurityGroups[0].GroupId' \
              --output text 2>/dev/null)
            
            # Add cross-SG references for secure access
            for SG in "$DEV_SG" "$TEST_SG"; do
              if [ "$SG" != "None" ] && [ -n "$SG" ] && [ "$RUNNER_SG" != "None" ] && [ -n "$RUNNER_SG" ]; then
                echo "Adding runner access to K3s API on $SG..."
                aws ec2 authorize-security-group-ingress \
                  --group-id "$SG" \
                  --ip-permissions '[{"IpProtocol":"tcp","FromPort":6443,"ToPort":6443,"UserIdGroupPairs":[{"GroupId":"'$RUNNER_SG'","Description":"K3s API access from runner"}]}]' 2>/dev/null || echo "Runner API rule may already exist"
                
                aws ec2 authorize-security-group-ingress \
                  --group-id "$SG" \
                  --ip-permissions '[{"IpProtocol":"tcp","FromPort":22,"ToPort":22,"UserIdGroupPairs":[{"GroupId":"'$RUNNER_SG'","Description":"SSH access from runner"}]}]' 2>/dev/null || echo "Runner SSH rule may already exist"
              fi
            done
            
            echo "‚úÖ Cross-SG references configured for secure access"
          fi
          
          echo "‚úÖ Security group configuration complete!"

      - name: Install and Configure K3s
        if: github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy'
        env:
          SSH_PRIVATE_KEY: ${{ secrets.SSH_PRIVATE_KEY }}
          DEV_CLUSTER_IP: ${{ needs.infrastructure.outputs.dev_cluster_ip }}
          TEST_CLUSTER_IP: ${{ needs.infrastructure.outputs.test_cluster_ip }}
          CLUSTER_IP: ${{ needs.infrastructure.outputs.cluster_ip }}
        run: |
          echo "‚ò∏Ô∏è Installing and configuring K3s on cluster nodes..."
          
          # Setup SSH key
          mkdir -p ~/.ssh
          echo "$SSH_PRIVATE_KEY" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          
          # Function to install k3s on a node using script
          install_k3s() {
            local IP=$1
            local ENV=$2
            
            echo "Installing K3s on $ENV cluster ($IP)..."
            
            # Test SSH connectivity first
            if ! timeout 10 ssh -o StrictHostKeyChecking=no -o ConnectTimeout=10 ubuntu@$IP "echo 'SSH connection successful'" 2>/dev/null; then
              echo "‚ùå Cannot connect to $IP via SSH"
              return 1
            fi
            
            # Copy and execute K3s installation script
            scp -o StrictHostKeyChecking=no scripts/install-k3s.sh ubuntu@$IP:/tmp/
            ssh -o StrictHostKeyChecking=no ubuntu@$IP "chmod +x /tmp/install-k3s.sh && sudo /tmp/install-k3s.sh"
            
            if [ $? -eq 0 ]; then
              echo "‚úÖ K3s installation completed on $ENV cluster"
            else
              echo "‚ùå K3s installation failed on $ENV cluster"
              return 1
            fi
          }
          
          # Install k3s based on network tier
          if [ "${{ github.event.inputs.network }}" == "lower" ]; then
            success=true
            
            if [ -n "$DEV_CLUSTER_IP" ]; then
              install_k3s "$DEV_CLUSTER_IP" "dev" || success=false
            fi
            
            if [ -n "$TEST_CLUSTER_IP" ]; then
              install_k3s "$TEST_CLUSTER_IP" "test" || success=false
            fi
            
            if [ "$success" = false ]; then
              echo "‚ùå Some K3s installations failed"
              exit 1
            fi
          else
            if [ -n "$CLUSTER_IP" ]; then
              install_k3s "$CLUSTER_IP" "${{ github.event.inputs.network }}"
            else
              echo "‚ùå Cluster IP not available"
              exit 1
            fi
          fi
          
          # Cleanup SSH key
          rm -f ~/.ssh/id_rsa
          
          echo "üéâ K3s installation and configuration complete!"

      - name: Terraform Destroy
        if: github.event.inputs.action == 'destroy'
        working-directory: infra
        run: |
          if [ "${{ github.event.inputs.action }}" == "destroy" ] && [ "${{ github.event.inputs.confirm_destroy }}" != "DESTROY" ]; then
            echo "‚ùå Type 'DESTROY' to confirm explicit destroy action"
            exit 1
          fi
          
          echo "üî• Destroying infrastructure..."
          
          terraform destroy \
            -var-file="environments/${{ github.event.inputs.network }}.tfvars" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="github_pat=${{ secrets.REPO_PAT }}" \
            -var="aws_region=${{ env.AWS_REGION }}" \
            -auto-approve

  verify-k3s-clusters:
    needs: infrastructure
    if: github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy'
    runs-on: ubuntu-latest
    timeout-minutes: 20
    
    steps:
      - uses: actions/checkout@v4
      
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Verify K3s API Availability
        env:
          DEV_CLUSTER_IP: ${{ needs.infrastructure.outputs.dev_cluster_ip }}
          TEST_CLUSTER_IP: ${{ needs.infrastructure.outputs.test_cluster_ip }}
          CLUSTER_IP: ${{ needs.infrastructure.outputs.cluster_ip }}
        run: |
          echo "‚è≥ Waiting for K3s clusters to be ready..."
          
          # Function to test K3s API
          test_k3s_api() {
            local IP=$1
            local NAME=$2
            echo "Testing $NAME cluster at $IP..."
            
            for i in {1..20}; do
              if timeout 10 curl -k -s "https://$IP:6443/version" >/dev/null 2>&1; then
                echo "‚úÖ $NAME cluster API is ready"
                return 0
              fi
              echo "‚è≥ $NAME cluster not ready, waiting... ($i/20)"
              sleep 30
            done
            echo "‚ùå $NAME cluster API not ready after 10 minutes"
            return 1
          }
          
          # Test clusters based on network tier
          if [ "${{ github.event.inputs.network }}" == "lower" ]; then
            success=true
            
            if [ -n "$DEV_CLUSTER_IP" ]; then
              test_k3s_api "$DEV_CLUSTER_IP" "dev" || success=false
            fi
            
            if [ -n "$TEST_CLUSTER_IP" ]; then
              test_k3s_api "$TEST_CLUSTER_IP" "test" || success=false
            fi
            
            if [ "$success" = false ]; then
              echo "‚ö†Ô∏è Some clusters not ready, but continuing..."
            fi
          else
            if [ -n "$CLUSTER_IP" ]; then
              test_k3s_api "$CLUSTER_IP" "${{ github.event.inputs.network }}"
            fi
          fi