name: Core Infrastructure

on:
  workflow_dispatch:
    inputs:
      action:
        description: 'Action'
        required: true
        type: choice
        options:
          - deploy
          - destroy
          - plan
          - redeploy
      network:
        description: 'Network Tier'
        required: true
        default: 'lower'
        type: choice
        options:
          - lower
          - higher
          - monitoring
      confirm_destroy:
        description: 'Type "DESTROY" to confirm'
        required: false
        type: string

env:
  AWS_REGION: ap-south-1
  TERRAFORM_VERSION: 1.6.0

jobs:
  infrastructure:
    runs-on: ubuntu-latest
    outputs:
      cluster_ip: ${{ steps.apply.outputs.cluster_ip }}
      dev_cluster_ip: ${{ steps.apply.outputs.dev_cluster_ip }}
      test_cluster_ip: ${{ steps.apply.outputs.test_cluster_ip }}

    steps:
      - uses: actions/checkout@v4

      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}

      - name: Terraform Init
        working-directory: infra
        run: |
          terraform init \
            -backend-config="bucket=${{ secrets.TF_STATE_BUCKET }}" \
            -backend-config="key=health-app-${{ github.event.inputs.network }}.tfstate" \
            -backend-config="region=$AWS_REGION"

      - name: Terraform Plan
        if: github.event.inputs.action == 'plan'
        working-directory: infra
        run: |
          terraform plan \
            -var-file="environments/${{ github.event.inputs.network }}.tfvars" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="github_pat=${{ secrets.REPO_PAT }}"



      - name: Terraform Apply
        id: apply
        if: github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy'
        working-directory: infra
        run: |
          if [ "${{ github.event.inputs.action }}" == "redeploy" ]; then
            echo "ðŸ§¹ Destroying existing resources..."
            terraform destroy \
              -var-file="environments/${{ github.event.inputs.network }}.tfvars" \
              -var-file="environments/parameter-store.tfvars" \
              -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
              -var="github_pat=${{ secrets.REPO_PAT }}" \
              -auto-approve
            
            echo "â³ Waiting 30 seconds for AWS cleanup..."
            sleep 30
            
            echo "ðŸ§¹ Manual cleanup of persistent resources..."
            aws rds delete-db-parameter-group --db-parameter-group-name health-app-shared-db-params || true
            aws kms delete-alias --alias-name alias/health-app-rds-export || true
            
            echo "âœ… Cleanup complete, starting fresh deployment..."
          fi
          
          terraform apply \
            -var-file="environments/${{ github.event.inputs.network }}.tfvars" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="github_pat=${{ secrets.REPO_PAT }}" \
            -auto-approve
          
          # Output cluster IPs for lower environment (multiple clusters)
          if [ "${{ github.event.inputs.network }}" == "lower" ]; then
            echo "dev_cluster_ip=$(terraform output -raw dev_cluster_ip)" >> $GITHUB_OUTPUT
            echo "test_cluster_ip=$(terraform output -raw test_cluster_ip)" >> $GITHUB_OUTPUT
          else
            echo "cluster_ip=$(terraform output -raw k3s_instance_ip)" >> $GITHUB_OUTPUT
          fi

      - name: Terraform Destroy
        if: github.event.inputs.action == 'destroy'
        working-directory: infra
        run: |
          if [ "${{ github.event.inputs.confirm_destroy }}" != "DESTROY" ]; then
            echo "âŒ Type 'DESTROY' to confirm"
            exit 1
          fi
          
          terraform destroy \
            -var-file="environments/${{ github.event.inputs.network }}.tfvars" \
            -var-file="environments/parameter-store.tfvars" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="github_pat=${{ secrets.REPO_PAT }}" \
            -auto-approve

  kubeconfig:
    needs: infrastructure
    if: github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy'
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
      - name: Setup Kubeconfig
        env:
          SSH_PRIVATE_KEY: ${{ secrets.SSH_PRIVATE_KEY }}
          GITHUB_TOKEN: ${{ secrets.REPO_PAT }}
          DEV_CLUSTER_IP: ${{ needs.infrastructure.outputs.dev_cluster_ip }}
          TEST_CLUSTER_IP: ${{ needs.infrastructure.outputs.test_cluster_ip }}
          CLUSTER_IP: ${{ needs.infrastructure.outputs.cluster_ip }}
        run: |
          echo "$SSH_PRIVATE_KEY" > /tmp/ssh_key
          chmod 600 /tmp/ssh_key
          
          curl -fsSL https://cli.github.com/packages/githubcli-archive-keyring.gpg | sudo dd of=/usr/share/keyrings/githubcli-archive-keyring.gpg 2>/dev/null
          echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main" | sudo tee /etc/apt/sources.list.d/github-cli.list > /dev/null
          sudo apt update && sudo apt install gh -y
          
          if [ "${{ github.event.inputs.network }}" == "lower" ]; then
            # Handle lower environment with dev and test clusters
            if [ -n "$DEV_CLUSTER_IP" ]; then
              echo "â³ Setting up Dev kubeconfig at $DEV_CLUSTER_IP..."
              echo "â³ Waiting for K3s installation to complete..."
              sleep 60
              for i in {1..30}; do
                echo "Attempt $i/30 for Dev cluster..."
                if timeout 20 ssh -i /tmp/ssh_key -o StrictHostKeyChecking=no ubuntu@$DEV_CLUSTER_IP "sudo test -f /etc/rancher/k3s/k3s.yaml && sudo systemctl is-active k3s" 2>/dev/null; then
                  ssh -i /tmp/ssh_key -o StrictHostKeyChecking=no ubuntu@$DEV_CLUSTER_IP "sudo cat /etc/rancher/k3s/k3s.yaml" > /tmp/kubeconfig-dev
                  # Use public IP with insecure connection for external access
                  sed "s/127.0.0.1/$DEV_CLUSTER_IP/g" /tmp/kubeconfig-dev > /tmp/kubeconfig-dev-temp
                  sed '/server:/a\    insecure-skip-tls-verify: true' /tmp/kubeconfig-dev-temp > /tmp/kubeconfig-dev-fixed
                  base64 -w 0 /tmp/kubeconfig-dev-fixed | gh secret set KUBECONFIG_DEV --repo $GITHUB_REPOSITORY
                  echo "âœ… KUBECONFIG_DEV updated"
                  break
                fi
                sleep 10
              done
            fi
            
            if [ -n "$TEST_CLUSTER_IP" ]; then
              echo "â³ Setting up Test kubeconfig at $TEST_CLUSTER_IP..."
              echo "â³ Waiting for K3s installation to complete..."
              sleep 120
              for i in {1..30}; do
                echo "Attempt $i/30 for Test cluster..."
                if timeout 20 ssh -i /tmp/ssh_key -o StrictHostKeyChecking=no ubuntu@$TEST_CLUSTER_IP "sudo test -f /etc/rancher/k3s/k3s.yaml && sudo systemctl is-active k3s" 2>/dev/null; then
                  ssh -i /tmp/ssh_key -o StrictHostKeyChecking=no ubuntu@$TEST_CLUSTER_IP "sudo cat /etc/rancher/k3s/k3s.yaml" > /tmp/kubeconfig-test
                  # Use public IP with insecure connection for external access
                  sed "s/127.0.0.1/$TEST_CLUSTER_IP/g" /tmp/kubeconfig-test > /tmp/kubeconfig-test-temp
                  sed '/server:/a\    insecure-skip-tls-verify: true' /tmp/kubeconfig-test-temp > /tmp/kubeconfig-test-fixed
                  base64 -w 0 /tmp/kubeconfig-test-fixed | gh secret set KUBECONFIG_TEST --repo $GITHUB_REPOSITORY
                  echo "âœ… KUBECONFIG_TEST updated"
                  break
                fi
                sleep 10
              done
            fi
          else
            # Handle single cluster environments (higher, monitoring)
            if [ -z "$CLUSTER_IP" ]; then
              echo "âŒ Cluster IP not available"
              exit 1
            fi
            
            echo "â³ Waiting for K3s cluster at $CLUSTER_IP..."
            echo "â³ Waiting for K3s installation to complete..."
            sleep 60
            for i in {1..60}; do
              if timeout 15 ssh -i /tmp/ssh_key -o StrictHostKeyChecking=no ubuntu@$CLUSTER_IP "sudo test -f /etc/rancher/k3s/k3s.yaml && sudo systemctl is-active k3s" 2>/dev/null; then
                ssh -i /tmp/ssh_key -o StrictHostKeyChecking=no ubuntu@$CLUSTER_IP "sudo cat /etc/rancher/k3s/k3s.yaml" > /tmp/kubeconfig
                sed "s/127.0.0.1/$CLUSTER_IP/g" /tmp/kubeconfig > /tmp/kubeconfig-fixed
                
                ENV_NAME="${{ github.event.inputs.network == 'higher' && 'PROD' || 'MONITORING' }}"
                SECRET_NAME="KUBECONFIG_$ENV_NAME"
                base64 -w 0 /tmp/kubeconfig-fixed | gh secret set $SECRET_NAME --repo $GITHUB_REPOSITORY
                echo "âœ… GitHub secret $SECRET_NAME updated"
                break
              fi
              if [ $i -eq 30 ]; then
                echo "âŒ K3s not ready after 5 minutes"
                exit 1
              fi
              sleep 10
            done
          fi
          
          rm -f /tmp/ssh_key /tmp/kubeconfig* 2>/dev/null || true

  setup-parameter-store:
    needs: kubeconfig
    if: github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy'
    runs-on: self-hosted
    timeout-minutes: 5
    
    steps:
      - uses: actions/checkout@v4
      
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Setup Parameter Store Integration
        run: |
          echo "ðŸ”§ Setting up Parameter Store integration..."
          
          # Install External Secrets Operator CRDs
          if [ "${{ github.event.inputs.network }}" == "lower" ]; then
            # Setup for dev environment
            if [ -n "${{ secrets.KUBECONFIG_DEV }}" ]; then
              echo "${{ secrets.KUBECONFIG_DEV }}" | base64 -d > /tmp/kubeconfig-dev
              export KUBECONFIG=/tmp/kubeconfig-dev
              
              if timeout 30 kubectl get nodes --insecure-skip-tls-verify >/dev/null 2>&1; then
                echo "ðŸ“¦ Installing External Secrets Operator for dev..."
                kubectl apply -f https://raw.githubusercontent.com/external-secrets/external-secrets/main/deploy/crds/bundle.yaml --insecure-skip-tls-verify || true
                kubectl create namespace external-secrets --dry-run=client -o yaml | kubectl apply -f - --insecure-skip-tls-verify || true
                
                # Apply Parameter Store configuration
                envsubst < kubernetes-manifests/components/external-secrets/parameter-store-secret.yaml | kubectl apply -f - --insecure-skip-tls-verify || true
                echo "âœ… Parameter Store setup complete for dev"
              fi
            fi
            
            # Setup for test environment
            if [ -n "${{ secrets.KUBECONFIG_TEST }}" ]; then
              echo "${{ secrets.KUBECONFIG_TEST }}" | base64 -d > /tmp/kubeconfig-test
              export KUBECONFIG=/tmp/kubeconfig-test
              
              if timeout 30 kubectl get nodes --insecure-skip-tls-verify >/dev/null 2>&1; then
                echo "ðŸ“¦ Installing External Secrets Operator for test..."
                kubectl apply -f https://raw.githubusercontent.com/external-secrets/external-secrets/main/deploy/crds/bundle.yaml --insecure-skip-tls-verify || true
                kubectl create namespace external-secrets --dry-run=client -o yaml | kubectl apply -f - --insecure-skip-tls-verify || true
                
                # Apply Parameter Store configuration for test
                sed 's|/dev/health-app/|/test/health-app/|g' kubernetes-manifests/components/external-secrets/parameter-store-secret.yaml | kubectl apply -f - --insecure-skip-tls-verify || true
                echo "âœ… Parameter Store setup complete for test"
              fi
            fi
          fi
          
          rm -f /tmp/kubeconfig* 2>/dev/null || true
          
          # Populate Parameter Store with kubeconfig data
          echo "ðŸ“ Populating Parameter Store with kubeconfig data..."
          
          if [ "${{ github.event.inputs.network }}" == "lower" ]; then
            # Extract and store dev kubeconfig data
            if [ -n "${{ secrets.KUBECONFIG_DEV }}" ]; then
              echo "${{ secrets.KUBECONFIG_DEV }}" | base64 -d > /tmp/kubeconfig-dev
              
              # Extract server from kubeconfig
              SERVER=$(grep "server:" /tmp/kubeconfig-dev | awk '{print $2}')
              
              # Test cluster connectivity and get real token
              if [ -n "$SERVER" ]; then
                export KUBECONFIG=/tmp/kubeconfig-dev
                
                # Create service account and get token
                if timeout 30 kubectl get nodes --insecure-skip-tls-verify >/dev/null 2>&1; then
                  kubectl create namespace gha-access --dry-run=client -o yaml --insecure-skip-tls-verify | kubectl apply -f - --insecure-skip-tls-verify || true
                  kubectl create serviceaccount gha-deployer -n gha-access --dry-run=client -o yaml --insecure-skip-tls-verify | kubectl apply -f - --insecure-skip-tls-verify || true
                  
                  # Get real token from cluster
                  TOKEN=$(kubectl create token gha-deployer -n gha-access --duration=24h --insecure-skip-tls-verify 2>/dev/null || echo "")
                  
                  # Store in Parameter Store
                  aws ssm put-parameter \
                    --name "/dev/health-app/kubeconfig/server" \
                    --value "$SERVER" \
                    --type "String" \
                    --overwrite \
                    --region $AWS_REGION
                  
                  if [ -n "$TOKEN" ]; then
                    aws ssm put-parameter \
                      --name "/dev/health-app/kubeconfig/token" \
                      --value "$TOKEN" \
                      --type "SecureString" \
                      --overwrite \
                      --region $AWS_REGION
                    echo "âœ… Real token stored for dev"
                  fi
                  
                  aws ssm put-parameter \
                    --name "/dev/health-app/kubeconfig/cluster-name" \
                    --value "k3s-cluster" \
                    --type "String" \
                    --overwrite \
                    --region $AWS_REGION
                  
                  echo "âœ… Dev kubeconfig data stored in Parameter Store"
                else
                  echo "âš ï¸ Dev cluster not accessible - storing server only"
                  aws ssm put-parameter \
                    --name "/dev/health-app/kubeconfig/server" \
                    --value "$SERVER" \
                    --type "String" \
                    --overwrite \
                    --region $AWS_REGION || true
                fi
              fi
            fi
            
            # Extract and store test kubeconfig data
            if [ -n "${{ secrets.KUBECONFIG_TEST }}" ]; then
              echo "${{ secrets.KUBECONFIG_TEST }}" | base64 -d > /tmp/kubeconfig-test
              
              SERVER=$(grep "server:" /tmp/kubeconfig-test | awk '{print $2}')
              
              if [ -n "$SERVER" ]; then
                export KUBECONFIG=/tmp/kubeconfig-test
                
                if timeout 30 kubectl get nodes --insecure-skip-tls-verify >/dev/null 2>&1; then
                  kubectl create namespace gha-access --dry-run=client -o yaml --insecure-skip-tls-verify | kubectl apply -f - --insecure-skip-tls-verify || true
                  kubectl create serviceaccount gha-deployer -n gha-access --dry-run=client -o yaml --insecure-skip-tls-verify | kubectl apply -f - --insecure-skip-tls-verify || true
                  
                  TOKEN=$(kubectl create token gha-deployer -n gha-access --duration=24h --insecure-skip-tls-verify 2>/dev/null || echo "")
                  
                  aws ssm put-parameter \
                    --name "/test/health-app/kubeconfig/server" \
                    --value "$SERVER" \
                    --type "String" \
                    --overwrite \
                    --region $AWS_REGION
                  
                  if [ -n "$TOKEN" ]; then
                    aws ssm put-parameter \
                      --name "/test/health-app/kubeconfig/token" \
                      --value "$TOKEN" \
                      --type "SecureString" \
                      --overwrite \
                      --region $AWS_REGION
                    echo "âœ… Real token stored for test"
                  fi
                  
                  aws ssm put-parameter \
                    --name "/test/health-app/kubeconfig/cluster-name" \
                    --value "k3s-cluster" \
                    --type "String" \
                    --overwrite \
                    --region $AWS_REGION
                  
                  echo "âœ… Test kubeconfig data stored in Parameter Store"
                else
                  echo "âš ï¸ Test cluster not accessible - storing server only"
                  aws ssm put-parameter \
                    --name "/test/health-app/kubeconfig/server" \
                    --value "$SERVER" \
                    --type "String" \
                    --overwrite \
                    --region $AWS_REGION || true
                fi
              fi
            fi
          fi
          
          echo "ðŸŽ‰ Parameter Store population complete!"

  setup-k8s-secrets:
    needs: [kubeconfig, setup-parameter-store]
    if: github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy'
    runs-on: self-hosted
    timeout-minutes: 10
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Kubernetes Secrets
        run: |
          echo "ðŸ” Setting up Kubernetes secrets for kubeconfig..."
          
          if [ "${{ github.event.inputs.network }}" == "lower" ]; then
            # Setup secrets for dev environment
            if [ -n "${{ secrets.KUBECONFIG_DEV }}" ]; then
              echo "${{ secrets.KUBECONFIG_DEV }}" | base64 -d > /tmp/kubeconfig-dev
              export KUBECONFIG=/tmp/kubeconfig-dev
              
              # Test cluster connectivity first
              if timeout 10 kubectl get nodes --insecure-skip-tls-verify >/dev/null 2>&1; then
                kubectl create namespace health-app-dev --dry-run=client -o yaml --insecure-skip-tls-verify | kubectl apply -f - --insecure-skip-tls-verify
                kubectl create secret generic kubeconfig-dev \
                  --from-file=config=/tmp/kubeconfig-dev \
                  --namespace=health-app-dev \
                  --dry-run=client -o yaml --insecure-skip-tls-verify | kubectl apply -f - --insecure-skip-tls-verify
              else
                echo "âš ï¸ Dev cluster not reachable - skipping secret creation"
              fi
              echo "âœ… Dev kubeconfig secret created"
            fi
            
            # Setup secrets for test environment  
            if [ -n "${{ secrets.KUBECONFIG_TEST }}" ]; then
              echo "${{ secrets.KUBECONFIG_TEST }}" | base64 -d > /tmp/kubeconfig-test
              export KUBECONFIG=/tmp/kubeconfig-test
              
              # Test cluster connectivity first
              if timeout 10 kubectl get nodes --insecure-skip-tls-verify >/dev/null 2>&1; then
                kubectl create namespace health-app-test --dry-run=client -o yaml --insecure-skip-tls-verify | kubectl apply -f - --insecure-skip-tls-verify
                kubectl create secret generic kubeconfig-test \
                  --from-file=config=/tmp/kubeconfig-test \
                  --namespace=health-app-test \
                  --dry-run=client -o yaml --insecure-skip-tls-verify | kubectl apply -f - --insecure-skip-tls-verify
              else
                echo "âš ï¸ Test cluster not reachable - skipping secret creation"
              fi
              echo "âœ… Test kubeconfig secret created"
            fi
            
          else
            # Setup secrets for single cluster environments
            ENV_NAME="${{ github.event.inputs.network == 'higher' && 'prod' || 'monitoring' }}"
            NAMESPACE="health-app-$ENV_NAME"
            
            if [ "$ENV_NAME" == "prod" ] && [ -n "${{ secrets.KUBECONFIG_PROD }}" ]; then
              echo "${{ secrets.KUBECONFIG_PROD }}" | base64 -d > /tmp/kubeconfig
            elif [ "$ENV_NAME" == "monitoring" ] && [ -n "${{ secrets.KUBECONFIG_MONITORING }}" ]; then
              echo "${{ secrets.KUBECONFIG_MONITORING }}" | base64 -d > /tmp/kubeconfig
            else
              echo "âš ï¸ Kubeconfig secret not found for $ENV_NAME"
              exit 0
            fi
            
            export KUBECONFIG=/tmp/kubeconfig
            
            # Test cluster connectivity first
            if timeout 10 kubectl get nodes --insecure-skip-tls-verify >/dev/null 2>&1; then
              kubectl create namespace $NAMESPACE --dry-run=client -o yaml --insecure-skip-tls-verify | kubectl apply -f - --insecure-skip-tls-verify
              kubectl create secret generic kubeconfig-$ENV_NAME \
                --from-file=config=/tmp/kubeconfig \
                --namespace=$NAMESPACE \
                --dry-run=client -o yaml --insecure-skip-tls-verify | kubectl apply -f - --insecure-skip-tls-verify
            else
              echo "âš ï¸ $ENV_NAME cluster not reachable - skipping secret creation"
              exit 0
            fi
            echo "âœ… $ENV_NAME kubeconfig secret created"
          fi
          
          # Cleanup
          rm -f /tmp/kubeconfig* 2>/dev/null || true
          
          echo "ðŸŽ‰ Kubernetes secrets setup complete!"

  test-deployment:
    needs: [kubeconfig, setup-parameter-store, setup-k8s-secrets]
    if: github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy'
    runs-on: self-hosted
    timeout-minutes: 10
    
    steps:
      - name: Test Cluster Connectivity
        run: |
          if [ "${{ github.event.inputs.network }}" == "lower" ]; then
            echo "ðŸ§ª Testing Lower Environment Clusters..."
            
            # Test Dev Environment using K8s secret
            if [ -n "${{ secrets.KUBECONFIG_DEV }}" ]; then
              echo "Testing Dev Environment..."
              echo "${{ secrets.KUBECONFIG_DEV }}" | base64 -d > /tmp/kubeconfig-dev
              export KUBECONFIG=/tmp/kubeconfig-dev
              
              if timeout 30 kubectl get nodes --insecure-skip-tls-verify; then
                echo "âœ… Dev cluster accessible"
                
                # Test using kubeconfig from K8s secret
                if kubectl get secret kubeconfig-dev -n health-app-dev --insecure-skip-tls-verify; then
                  echo "âœ… Kubeconfig secret exists - testing with it"
                  kubectl get secret kubeconfig-dev -n health-app-dev -o jsonpath='{.data.config}' --insecure-skip-tls-verify | base64 -d > /tmp/k8s-secret-config
                  export KUBECONFIG=/tmp/k8s-secret-config
                  
                  # Test connectivity with K8s secret config
                  if timeout 30 kubectl get nodes --insecure-skip-tls-verify; then
                    echo "âœ… K8s secret kubeconfig works"
                    
                    # Test network connectivity
                    echo "ðŸŒ Testing network connectivity..."
                    kubectl get services --all-namespaces --insecure-skip-tls-verify
                    kubectl get endpoints --all-namespaces --insecure-skip-tls-verify
                    
                    # Test port connectivity
                    echo "ðŸ”Œ Testing port connectivity..."
                    kubectl get nodes -o wide --insecure-skip-tls-verify
                    
                  else
                    echo "âŒ K8s secret kubeconfig failed"
                  fi
                else
                  echo "âŒ Kubeconfig secret missing in health-app-dev namespace"
                fi
                
                kubectl get namespaces --insecure-skip-tls-verify
                kubectl get pods -A --insecure-skip-tls-verify
              else
                echo "âŒ Dev cluster connection failed"
              fi
            else
              echo "âš ï¸ KUBECONFIG_DEV secret not found"
            fi
            
            echo ""
            
            # Test Test Environment using K8s secret
            if [ -n "${{ secrets.KUBECONFIG_TEST }}" ]; then
              echo "Testing Test Environment..."
              echo "${{ secrets.KUBECONFIG_TEST }}" | base64 -d > /tmp/kubeconfig-test
              export KUBECONFIG=/tmp/kubeconfig-test
              
              if timeout 30 kubectl get nodes --insecure-skip-tls-verify; then
                echo "âœ… Test cluster accessible"
                
                # Test using kubeconfig from K8s secret
                if kubectl get secret kubeconfig-test -n health-app-test --insecure-skip-tls-verify; then
                  echo "âœ… Kubeconfig secret exists - testing with it"
                  kubectl get secret kubeconfig-test -n health-app-test -o jsonpath='{.data.config}' --insecure-skip-tls-verify | base64 -d > /tmp/k8s-secret-config-test
                  export KUBECONFIG=/tmp/k8s-secret-config-test
                  
                  # Test connectivity with K8s secret config
                  if timeout 30 kubectl get nodes --insecure-skip-tls-verify; then
                    echo "âœ… K8s secret kubeconfig works"
                    
                    # Test network connectivity
                    echo "ðŸŒ Testing network connectivity..."
                    kubectl get services --all-namespaces --insecure-skip-tls-verify
                    kubectl get endpoints --all-namespaces --insecure-skip-tls-verify
                    
                    # Test port connectivity
                    echo "ðŸ”Œ Testing port connectivity..."
                    kubectl get nodes -o wide --insecure-skip-tls-verify
                    
                  else
                    echo "âŒ K8s secret kubeconfig failed"
                  fi
                else
                  echo "âŒ Kubeconfig secret missing in health-app-test namespace"
                fi
                
                kubectl get namespaces --insecure-skip-tls-verify
                kubectl get pods -A --insecure-skip-tls-verify
              else
                echo "âŒ Test cluster connection failed"
              fi
            else
              echo "âš ï¸ KUBECONFIG_TEST secret not found"
            fi
            
          else
            echo "ðŸ§ª Testing Single Cluster Environment..."
            
            ENV_NAME="${{ github.event.inputs.network == 'higher' && 'PROD' || 'MONITORING' }}"
            KUBECONFIG_SECRET="KUBECONFIG_$ENV_NAME"
            
            if [ "$ENV_NAME" == "PROD" ] && [ -n "${{ secrets.KUBECONFIG_PROD }}" ]; then
              echo "${{ secrets.KUBECONFIG_PROD }}" | base64 -d > /tmp/kubeconfig
            elif [ "$ENV_NAME" == "MONITORING" ] && [ -n "${{ secrets.KUBECONFIG_MONITORING }}" ]; then
              echo "${{ secrets.KUBECONFIG_MONITORING }}" | base64 -d > /tmp/kubeconfig
            else
              echo "âš ï¸ $KUBECONFIG_SECRET secret not found"
              exit 0
            fi
            
            export KUBECONFIG=/tmp/kubeconfig
            
            if timeout 30 kubectl get nodes --insecure-skip-tls-verify; then
              echo "âœ… $ENV_NAME cluster accessible"
              kubectl get namespaces --insecure-skip-tls-verify
              kubectl get pods -A --field-selector=status.phase=Running --insecure-skip-tls-verify
            else
              echo "âŒ $ENV_NAME cluster connection failed"
            fi
          fi
          
          # Cleanup
          rm -f /tmp/kubeconfig* 2>/dev/null || true
          
          echo "ðŸŽ‰ Deployment testing complete!"