name: Core Infrastructure

on:
  workflow_dispatch:
    inputs:
      action:
        description: 'Action'
        default: 'deploy'
        required: true
        type: choice
        options:
        - redeploy
        - deploy
        - destroy
        - plan
      environment:
        description: 'Network/Environment'
        required: true
        default: 'lower'
        type: choice
        options:
        - lower
        - higher
        - monitoring
        - all
        - cleanup-all
      confirm_destroy:
        description: 'Type "DESTROY" to confirm'
        required: false
        type: string
      restore_from_snapshot:
        description: 'Restore RDS from snapshot'
        required: false
        default: false
        type: boolean
      runner_type:
        description: 'Runner Type'
        required: false
        default: 'github'
        type: choice
        options:
        - github
        - aws

env:
  AWS_REGION: ap-south-1
  TERRAFORM_VERSION: 1.6.0

jobs:
  infrastructure:
    runs-on: ${{ github.event.inputs.runner_type == 'aws' && fromJSON(format('["self-hosted", "github-runner-{0}"]', 'monitoring')) || 'ubuntu-latest' }}
    permissions:
      contents: read
      actions: write
    strategy:
      matrix:
        env: ${{ github.event.inputs.environment == 'all' && fromJson('["lower", "higher", "monitoring"]') || github.event.inputs.environment == 'cleanup-all' && fromJson('["cleanup"]') || fromJson(format('["{0}"]', github.event.inputs.environment)) }}
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: ${{ env.TERRAFORM_VERSION }}

    - name: Pre-deployment Resource Cleanup
      if: github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy'
      continue-on-error: true
      env:
        DEPLOYMENT_ACTION: ${{ github.event.inputs.action }}
        TARGET_ENVIRONMENT: ${{ matrix.env }}
        RUNNER_TYPE: ${{ github.event.inputs.runner_type }}
      run: |
        echo "ðŸ§¹ Pre-deployment resource cleanup..."
        
        if [ -f "scripts/cleanup-resources.sh" ]; then
          chmod +x scripts/cleanup-resources.sh
          ./scripts/cleanup-resources.sh || echo "âš ï¸ Cleanup had issues but continuing"
        fi
        
        # Clean up old kubeconfig files to ensure fresh deployment
        echo "ðŸ§¹ Cleaning up old kubeconfig files..."
        aws s3 rm "s3://${{ secrets.TF_STATE_BUCKET }}/kubeconfig/${{ matrix.env }}-kubeconfig.yaml" 2>/dev/null || echo "No existing kubeconfig found"
        aws s3 rm "s3://${{ secrets.TF_STATE_BUCKET }}/kubeconfig/${{ matrix.env }}-gha.yaml" 2>/dev/null || echo "No existing GHA kubeconfig found"
        
        echo "âœ… Pre-deployment cleanup completed"

    - name: Terraform Init
      working-directory: infra/live
      run: |
        echo "ðŸ” Backend Configuration:"
        echo "- Bucket: ${{ secrets.TF_STATE_BUCKET }}"
        echo "- Key: health-app-${{ matrix.env }}.tfstate"
        echo "- Region: $AWS_REGION"
        
        terraform init -reconfigure \
          -backend-config="bucket=${{ secrets.TF_STATE_BUCKET }}" \
          -backend-config="key=health-app-${{ matrix.env }}.tfstate" \
          -backend-config="region=$AWS_REGION"

    - name: Terraform Plan
      working-directory: infra/live
      if: github.event.inputs.action == 'plan' || github.event.inputs.action == 'deploy'
      run: |
        echo "ðŸ“‹ Planning infrastructure changes..."
        
        terraform plan \
          -var="environment=${{ matrix.env == 'lower' && 'dev' || matrix.env == 'higher' && 'prod' || 'monitoring' }}" \
          -var="network_tier=${{ matrix.env }}" \
          -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
          -var="repo_pat=${{ secrets.REPO_PAT }}" \
          -var="repo_name=${{ secrets.REPO_NAME }}" \
          -var="restore_from_snapshot=${{ github.event.inputs.restore_from_snapshot }}" \
          -var="snapshot_identifier=${{ github.event.inputs.restore_from_snapshot == 'true' && 'healthapidb-snapshot' || 'null' }}" \
          -out=tfplan

    - name: Terraform Apply
      working-directory: infra/live
      if: github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy'
      run: |
        echo "ðŸš€ Applying infrastructure changes..."
        
        if [ "${{ github.event.inputs.action }}" = "redeploy" ]; then
          terraform plan \
            -var="environment=${{ matrix.env == 'lower' && 'dev' || matrix.env == 'higher' && 'prod' || 'monitoring' }}" \
            -var="network_tier=${{ matrix.env }}" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="repo_pat=${{ secrets.REPO_PAT }}" \
            -var="repo_name=${{ secrets.REPO_NAME }}" \
            -var="restore_from_snapshot=${{ github.event.inputs.restore_from_snapshot }}" \
            -var="snapshot_identifier=${{ github.event.inputs.restore_from_snapshot == 'true' && 'healthapidb-snapshot' || 'null' }}" \
            -out=tfplan
        fi
        
        terraform apply -auto-approve tfplan

    - name: Wait for K3s Initialization
      if: (github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy') && matrix.env != 'cleanup'
      run: |
        echo "â³ Waiting for K3s cluster to initialize..."
        
        # Get K3s instance
        K3S_IP=$(aws ec2 describe-instances \
          --filters "Name=tag:Name,Values=health-app-${{ matrix.env }}-k3s-node" "Name=instance-state-name,Values=running" \
          --query "Reservations[0].Instances[0].PublicIpAddress" --output text)
        
        if [ "$K3S_IP" != "None" ] && [ -n "$K3S_IP" ]; then
          echo "ðŸ“ K3s cluster found at: $K3S_IP"
          echo "â³ Waiting 3 minutes for K3s installation to complete..."
          sleep 180
          
          # Check if kubeconfig is available in S3
          echo "ðŸ” Checking for kubeconfig in S3..."
          for i in {1..10}; do
            if aws s3 ls "s3://${{ secrets.TF_STATE_BUCKET }}/kubeconfig/${{ matrix.env }}-kubeconfig.yaml" >/dev/null 2>&1; then
              echo "âœ… Kubeconfig found in S3 (attempt $i)"
              break
            else
              echo "â³ Kubeconfig not ready yet (attempt $i/10), waiting 30s..."
              sleep 30
            fi
          done
        else
          echo "âŒ No K3s instance found"
          exit 1
        fi

    - name: Setup kubectl
      if: (github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy') && matrix.env != 'cleanup'
      run: |
        echo "ðŸ”§ Setting up kubectl..."
        
        # Install kubectl if not present
        if ! command -v kubectl &> /dev/null; then
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/
        fi
        
        # Download kubeconfig from S3
        if aws s3 cp "s3://${{ secrets.TF_STATE_BUCKET }}/kubeconfig/${{ matrix.env }}-kubeconfig.yaml" ./kubeconfig 2>/dev/null; then
          export KUBECONFIG=./kubeconfig
          
          # Test kubectl connection with retry
          echo "ðŸ” Testing kubectl connection..."
          for i in {1..12}; do
            if kubectl get nodes --request-timeout=10s >/dev/null 2>&1; then
              echo "âœ… kubectl configured successfully (attempt $i)"
              kubectl get nodes
              kubectl get namespaces
              break
            else
              echo "â³ kubectl connection failed (attempt $i/12), waiting 30s..."
              if [ $i -eq 12 ]; then
                echo "âš ï¸ kubectl connection failed after 6 minutes - cluster may have issues"
              else
                sleep 30
              fi
            fi
          done
        else
          echo "âš ï¸ Could not download kubeconfig from S3"
        fi

    - name: Infrastructure Summary
      if: always()
      run: |
        # Use comprehensive job summary generator
        if [ -f "scripts/generate-job-summary.sh" ]; then
          chmod +x scripts/generate-job-summary.sh
          ./scripts/generate-job-summary.sh "${{ matrix.env }}" "${{ github.event.inputs.action }}" "${{ job.status }}"
        else
          # Fallback simple summary
          echo "## ðŸ—ï¸ Infrastructure Summary" >> $GITHUB_STEP_SUMMARY
          echo "**Environment:** ${{ matrix.env }}" >> $GITHUB_STEP_SUMMARY
          echo "**Action:** ${{ github.event.inputs.action }}" >> $GITHUB_STEP_SUMMARY
          echo "**Status:** ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
        fi

    - name: Terraform Destroy
      working-directory: infra/live
      if: github.event.inputs.action == 'destroy' && github.event.inputs.confirm_destroy == 'DESTROY'
      run: |
        echo "ðŸ§¹ Starting Terraform destroy for ${{ matrix.env }} environment"
        
        terraform destroy \
          -var="environment=${{ matrix.env == 'lower' && 'dev' || matrix.env == 'higher' && 'prod' || 'monitoring' }}" \
          -var="network_tier=${{ matrix.env }}" \
          -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
          -var="repo_pat=${{ secrets.REPO_PAT }}" \
          -var="repo_name=${{ secrets.REPO_NAME }}" \
          -var="restore_from_snapshot=false" \
          -var="snapshot_identifier=null" \
          -auto-approve
        
        echo "âœ… Terraform destroy completed for ${{ matrix.env }} environment"