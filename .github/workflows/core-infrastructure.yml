name: Core Infrastructure

on:
  workflow_dispatch:
    inputs:
      action:
        description: 'Action'
        required: true
        type: choice
        options:
        - deploy
        - destroy
        - plan
      environment:
        description: 'Network/Environment'
        required: true
        default: 'lower'
        type: choice
        options:
        - lower
        - higher
        - monitoring
        - all
      confirm_destroy:
        description: 'Type "DESTROY" to confirm'
        required: false
        type: string
      restore_from_snapshot:
        description: 'Restore RDS from snapshot'
        required: false
        default: false
        type: boolean

env:
  AWS_REGION: ap-south-1
  TERRAFORM_VERSION: 1.6.0

jobs:
  infrastructure:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      actions: write
    strategy:
      matrix:
        env: ${{ github.event.inputs.environment == 'all' && fromJson('["lower", "higher", "monitoring"]') || fromJson(format('["{0}"]', github.event.inputs.environment)) }}
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: ${{ env.TERRAFORM_VERSION }}



    - name: Terraform Init
      working-directory: infra
      run: |
        terraform init \
          -backend-config="bucket=${{ secrets.TF_STATE_BUCKET }}" \
          -backend-config="key=health-app-${{ matrix.env }}.tfstate" \
          -backend-config="region=$AWS_REGION"

    - name: Terraform Plan
      working-directory: infra
      if: github.event.inputs.action == 'plan' || github.event.inputs.action == 'deploy'
      run: |
        terraform plan \
          -var-file="environments/${{ matrix.env }}.tfvars" \
          -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
          -var="restore_from_snapshot=${{ github.event.inputs.restore_from_snapshot }}" \
          -out=tfplan

    - name: Terraform Apply
      id: terraform-apply
      working-directory: infra
      if: github.event.inputs.action == 'deploy'
      run: terraform apply -auto-approve tfplan

    - name: Terraform Destroy
      working-directory: infra
      if: github.event.inputs.action == 'destroy' && github.event.inputs.confirm_destroy == 'DESTROY'
      run: |
        terraform destroy \
          -var-file="environments/${{ matrix.env }}.tfvars" \
          -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
          -var="restore_from_snapshot=${{ github.event.inputs.restore_from_snapshot }}" \
          -auto-approve

    - name: Cleanup kubeconfig from S3
      if: github.event.inputs.action == 'destroy' && github.event.inputs.confirm_destroy == 'DESTROY'
      run: |
        echo "ğŸ§¹ Cleaning up kubeconfig files from S3..."
        
        # Remove main kubeconfig
        aws s3 rm s3://${{ secrets.TF_STATE_BUCKET }}/kubeconfig/${{ matrix.env }}-network.yaml 2>/dev/null || echo "Main kubeconfig not found"
        
        # Remove environment-specific copies
        case "${{ matrix.env }}" in
          "lower")
            aws s3 rm s3://${{ secrets.TF_STATE_BUCKET }}/kubeconfig/dev-network.yaml 2>/dev/null || echo "Dev kubeconfig not found"
            aws s3 rm s3://${{ secrets.TF_STATE_BUCKET }}/kubeconfig/test-network.yaml 2>/dev/null || echo "Test kubeconfig not found"
            ;;
          "higher")
            aws s3 rm s3://${{ secrets.TF_STATE_BUCKET }}/kubeconfig/prod-network.yaml 2>/dev/null || echo "Prod kubeconfig not found"
            ;;
          "monitoring")
            # No additional copies for monitoring
            ;;
        esac
        
        echo "âœ… Kubeconfig cleanup completed"



    - name: Generate and upload kubeconfig to S3
      if: github.event.inputs.action == 'deploy' && steps.terraform-apply.outcome == 'success'
      working-directory: infra
      run: |
        echo "ğŸ”§ Starting kubeconfig generation for ${{ matrix.env }} environment"
        
        # Debug: Check if terraform outputs exist
        echo "ğŸ“‹ Available Terraform outputs:"
        terraform output || echo "No outputs available"
        
        CLUSTER_IP=$(terraform output -raw k3s_instance_ip 2>/dev/null || echo "")
        TOKEN="K10..."
        OUTPUT="/tmp/kubeconfig.yaml"
        
        # Debug: Test S3 access
        echo "ğŸ§ª Testing S3 access:"
        echo "test" > /tmp/test.txt
        aws s3 cp /tmp/test.txt s3://${{ secrets.TF_STATE_BUCKET }}/kubeconfig/test.txt
        aws s3 rm s3://${{ secrets.TF_STATE_BUCKET }}/kubeconfig/test.txt
        
        echo "ğŸ“¡ Cluster IP: $CLUSTER_IP"
        echo "ğŸ“ Output file: $OUTPUT"
        
        # Wait for EC2 instance to be fully ready
        if [[ -n "$CLUSTER_IP" && "$CLUSTER_IP" != "None" ]]; then
          echo "â³ Waiting for EC2 instance to be fully ready..."
          sleep 30
          
          # Check instance status
          INSTANCE_STATE=$(aws ec2 describe-instances --filters "Name=ip-address,Values=$CLUSTER_IP" --query "Reservations[0].Instances[0].State.Name" --output text 2>/dev/null || echo "unknown")
          echo "Instance state: $INSTANCE_STATE"
          
          # Check system status
          INSTANCE_ID=$(aws ec2 describe-instances --filters "Name=ip-address,Values=$CLUSTER_IP" --query "Reservations[0].Instances[0].InstanceId" --output text 2>/dev/null || echo "unknown")
          if [[ "$INSTANCE_ID" != "unknown" && "$INSTANCE_ID" != "None" ]]; then
            echo "Instance ID: $INSTANCE_ID"
            SYSTEM_STATUS=$(aws ec2 describe-instance-status --instance-ids $INSTANCE_ID --query "InstanceStatuses[0].SystemStatus.Status" --output text 2>/dev/null || echo "unknown")
            INSTANCE_STATUS=$(aws ec2 describe-instance-status --instance-ids $INSTANCE_ID --query "InstanceStatuses[0].InstanceStatus.Status" --output text 2>/dev/null || echo "unknown")
            echo "System status: $SYSTEM_STATUS"
            echo "Instance status: $INSTANCE_STATUS"
          fi
        fi
        
        # Make script executable
        chmod +x ../scripts/generate-kubeconfig.sh
        
        # Force token-based kubeconfig generation (ignore Terraform output)
        echo "ğŸ”§ Creating token-based kubeconfig..."
        chmod +x ../scripts/generate-kubeconfig.sh
        
        # Create token-based kubeconfig directly
        mkdir -p "$(dirname "$OUTPUT")"
        cat > "$OUTPUT" << EOF
        apiVersion: v1
        clusters:
        - cluster:
            insecure-skip-tls-verify: true
            server: https://${CLUSTER_IP}:6443
          name: k3s-cluster
        contexts:
        - context:
            cluster: k3s-cluster
            user: k3s-user
          name: k3s-context
        current-context: k3s-context
        kind: Config
        preferences: {}
        users:
        - name: k3s-user
          user:
            token: TOKEN_PLACEHOLDER
        EOF
        chmod 600 "$OUTPUT"
        echo "âœ… Token-based kubeconfig template created"
        
        # If kubeconfig has TOKEN_PLACEHOLDER, try to get real token via SSH
        if grep -q "TOKEN_PLACEHOLDER" "$OUTPUT" 2>/dev/null; then
          echo "ğŸ”‘ Getting K3s token via SSH..."
          
          # Setup SSH key from secret
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/aws-key
          chmod 600 ~/.ssh/aws-key
          
          # Debug SSH key setup
          echo "ğŸ” SSH key fingerprint:"
          ssh-keygen -lf ~/.ssh/aws-key.pub 2>/dev/null || echo "Could not get fingerprint"
          
          # Test basic connectivity first
          echo "ğŸŒ Testing basic connectivity to $CLUSTER_IP..."
          if ping -c 2 $CLUSTER_IP >/dev/null 2>&1; then
            echo "âœ… Ping successful"
          else
            echo "âŒ Ping failed - network issue?"
          fi
          
          # Test SSH port
          echo "ğŸšª Testing SSH port 22..."
          if timeout 10 bash -c "</dev/tcp/$CLUSTER_IP/22"; then
            echo "âœ… SSH port 22 is open"
          else
            echo "âŒ SSH port 22 is not accessible"
          fi
          
          # Test SSH connection without command
          echo "ğŸ”‘ Testing SSH authentication..."
          if ssh -i ~/.ssh/aws-key -o StrictHostKeyChecking=no -o ConnectTimeout=10 ubuntu@$CLUSTER_IP "echo 'SSH connection successful'" 2>/dev/null; then
            echo "âœ… SSH authentication successful"
          else
            echo "âŒ SSH authentication failed"
            echo "SSH debug output:"
            ssh -i ~/.ssh/aws-key -o StrictHostKeyChecking=no -o ConnectTimeout=10 -v ubuntu@$CLUSTER_IP "echo test" 2>&1 | head -20
          fi
          
          # Check if K3s is running
          echo "ğŸ” Checking K3s status..."
          K3S_STATUS=$(ssh -i ~/.ssh/aws-key -o StrictHostKeyChecking=no -o ConnectTimeout=10 ubuntu@$CLUSTER_IP "sudo systemctl is-active k3s" 2>/dev/null || echo "unknown")
          echo "K3s service status: $K3S_STATUS"
          
          # Check if token file exists
          echo "ğŸ“„ Checking K3s token file..."
          TOKEN_EXISTS=$(ssh -i ~/.ssh/aws-key -o StrictHostKeyChecking=no -o ConnectTimeout=10 ubuntu@$CLUSTER_IP "sudo test -f /var/lib/rancher/k3s/server/node-token && echo 'exists' || echo 'missing'" 2>/dev/null || echo "unknown")
          echo "Token file status: $TOKEN_EXISTS"
          
          # Get token with retries and better error handling
          TOKEN=""
          for i in {1..5}; do
            echo "Attempt $i/5: Getting K3s token..."
            TOKEN=$(ssh -i ~/.ssh/aws-key -o StrictHostKeyChecking=no -o ConnectTimeout=15 ubuntu@$CLUSTER_IP "sudo cat /var/lib/rancher/k3s/server/node-token" 2>/dev/null || echo "")
            
            if [[ -n "$TOKEN" && "$TOKEN" != "unknown" ]]; then
              echo "âœ… Token retrieved successfully (length: ${#TOKEN})"
              echo "Token preview: ${TOKEN:0:20}..."
              break
            else
              echo "âŒ Attempt $i failed, token empty or error"
              if [[ $i -lt 5 ]]; then
                echo "Waiting 15 seconds before retry..."
                sleep 15
              fi
            fi
          done
          
          if [[ -n "$TOKEN" && "$TOKEN" != "unknown" ]]; then
            sed -i "s/TOKEN_PLACEHOLDER/$TOKEN/g" "$OUTPUT"
            echo "âœ… Token added to kubeconfig successfully"
          else
            echo "âŒ Could not get token after 5 attempts"
            echo "Kubeconfig will use template without token (may not work)"
          fi
        else
          echo "â„¹ï¸ No TOKEN_PLACEHOLDER found, using existing kubeconfig"
        fi
        
        # Verify file was created
        if [[ ! -f "$OUTPUT" ]]; then
          echo "âŒ Kubeconfig file not created"
          exit 1
        fi
        
        echo "ğŸ“„ Generated kubeconfig content:"
        cat "$OUTPUT"
        
        # Upload main file
        echo "â˜ï¸ Uploading main kubeconfig: ${{ matrix.env }}-network.yaml"
        aws s3 cp "$OUTPUT" s3://${{ secrets.TF_STATE_BUCKET }}/kubeconfig/${{ matrix.env }}-network.yaml
        
        # Upload environment-specific copies
        case "${{ matrix.env }}" in
          "lower")
            echo "â˜ï¸ Uploading dev and test copies"
            aws s3 cp "$OUTPUT" s3://${{ secrets.TF_STATE_BUCKET }}/kubeconfig/dev-network.yaml
            aws s3 cp "$OUTPUT" s3://${{ secrets.TF_STATE_BUCKET }}/kubeconfig/test-network.yaml
            ;;
          "higher")
            echo "â˜ï¸ Uploading prod copy"
            aws s3 cp "$OUTPUT" s3://${{ secrets.TF_STATE_BUCKET }}/kubeconfig/prod-network.yaml
            ;;
        esac
        
        # Verify uploads
        echo "ğŸ” Verifying S3 uploads:"
        aws s3 ls s3://${{ secrets.TF_STATE_BUCKET }}/kubeconfig/
        
        echo "âœ… Kubeconfig upload completed for ${{ matrix.env }} environment"

    - name: Cleanup on Failure
      if: failure() && github.event.inputs.action == 'deploy' && steps.terraform-apply.outcome == 'failure'
      working-directory: infra
      run: |
        echo "ğŸ§¹ Terraform deployment failed - cleaning up partial resources"
        
        terraform destroy \
          -var-file="environments/${{ matrix.env }}.tfvars" \
          -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
          -auto-approve || echo "Cleanup completed (some errors expected)"
        
        echo "âœ… Cleanup completed - safe to retry deployment"

    - name: Summary
      if: always()
      working-directory: infra
      run: |
        echo "## ğŸ—ï¸ Infrastructure ${{ github.event.inputs.action }} - ${{ matrix.env }}" >> $GITHUB_STEP_SUMMARY
        echo "**Status:** ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
        echo "**Environment:** ${{ matrix.env }}" >> $GITHUB_STEP_SUMMARY
        echo "**Date:** $(date)" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [[ "${{ job.status }}" == "success" && "${{ github.event.inputs.action }}" == "deploy" ]]; then
          echo "### âœ… Resources Created" >> $GITHUB_STEP_SUMMARY
          
          # Get Terraform outputs
          CLUSTER_IP=$(terraform output -raw k3s_instance_ip 2>/dev/null || echo "Not available")
          DB_ENDPOINT=$(terraform output -raw db_instance_endpoint 2>/dev/null || echo "Not available")
          VPC_ID=$(terraform output -raw vpc_id 2>/dev/null || echo "Not available")
          
          echo "- **VPC:** $VPC_ID" >> $GITHUB_STEP_SUMMARY
          echo "- **K3s Cluster:** $CLUSTER_IP:6443" >> $GITHUB_STEP_SUMMARY
          echo "- **Database:** $DB_ENDPOINT" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "### ğŸ“ S3 Kubeconfig Files" >> $GITHUB_STEP_SUMMARY
          
          # Check kubeconfig files in S3
          S3_BUCKET="s3://${{ secrets.TF_STATE_BUCKET }}/kubeconfig"
          
          if aws s3 ls "$S3_BUCKET/${{ matrix.env }}-network.yaml" >/dev/null 2>&1; then
            echo "- âœ… **${{ matrix.env }}-network.yaml** - Uploaded successfully" >> $GITHUB_STEP_SUMMARY
          else
            echo "- âŒ **${{ matrix.env }}-network.yaml** - Upload failed" >> $GITHUB_STEP_SUMMARY
          fi
          
          case "${{ matrix.env }}" in
            "lower")
              if aws s3 ls "$S3_BUCKET/dev-network.yaml" >/dev/null 2>&1; then
                echo "- âœ… **dev-network.yaml** - Copy created" >> $GITHUB_STEP_SUMMARY
              else
                echo "- âŒ **dev-network.yaml** - Copy failed" >> $GITHUB_STEP_SUMMARY
              fi
              if aws s3 ls "$S3_BUCKET/test-network.yaml" >/dev/null 2>&1; then
                echo "- âœ… **test-network.yaml** - Copy created" >> $GITHUB_STEP_SUMMARY
              else
                echo "- âŒ **test-network.yaml** - Copy failed" >> $GITHUB_STEP_SUMMARY
              fi
              ;;
            "higher")
              if aws s3 ls "$S3_BUCKET/prod-network.yaml" >/dev/null 2>&1; then
                echo "- âœ… **prod-network.yaml** - Copy created" >> $GITHUB_STEP_SUMMARY
              else
                echo "- âŒ **prod-network.yaml** - Copy failed" >> $GITHUB_STEP_SUMMARY
              fi
              ;;
          esac
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ğŸš€ Next Steps" >> $GITHUB_STEP_SUMMARY
          echo "1. Run **Core Deployment** workflow" >> $GITHUB_STEP_SUMMARY
          echo "2. Select environment: dev/test/prod" >> $GITHUB_STEP_SUMMARY
          echo "3. Kubeconfig will be auto-downloaded from S3" >> $GITHUB_STEP_SUMMARY
          
        elif [[ "${{ job.status }}" == "success" && "${{ github.event.inputs.action }}" == "destroy" ]]; then
          echo "### ğŸ—‘ï¸ Resources Destroyed" >> $GITHUB_STEP_SUMMARY
          echo "- All infrastructure resources removed" >> $GITHUB_STEP_SUMMARY
          echo "- Kubeconfig files cleaned from S3" >> $GITHUB_STEP_SUMMARY
          
        elif [[ "${{ job.status }}" == "success" && "${{ github.event.inputs.action }}" == "plan" ]]; then
          echo "### ğŸ“‹ Plan Generated" >> $GITHUB_STEP_SUMMARY
          echo "- Review the plan output above" >> $GITHUB_STEP_SUMMARY
          echo "- Run deploy action to apply changes" >> $GITHUB_STEP_SUMMARY
          
        elif [[ "${{ job.status }}" == "failure" ]]; then
          echo "### âŒ Deployment Failed" >> $GITHUB_STEP_SUMMARY
          echo "- Check logs for error details" >> $GITHUB_STEP_SUMMARY
          echo "- Partial resources may have been cleaned up" >> $GITHUB_STEP_SUMMARY
          echo "- Safe to retry deployment" >> $GITHUB_STEP_SUMMARY
        fi