name: Core Infrastructure

on:
  workflow_dispatch:
    inputs:
      action:
        description: 'Action'
        required: true
        type: choice
        options:
        - deploy
        - destroy
        - plan
        - redeploy
      environment:
        description: 'Network/Environment'
        required: true
        default: 'lower'
        type: choice
        options:
        - lower
        - higher
        - monitoring
        - all
        - cleanup-all
      confirm_destroy:
        description: 'Type "DESTROY" to confirm'
        required: false
        type: string
      restore_from_snapshot:
        description: 'Restore RDS from snapshot'
        required: false
        default: false
        type: boolean
      runner_type:
        description: 'Runner Type'
        required: false
        default: 'aws'
        type: choice
        options:
        - aws
        - github
      optimize_data_transfer:
        description: 'Run data transfer optimization'
        required: false
        default: false
        type: boolean
      cleanup_all_regions:
        description: 'Cleanup all AWS regions (for destroy only)'
        required: false
        default: false
        type: boolean

env:
  AWS_REGION: ap-south-1
  TERRAFORM_VERSION: 1.6.0

jobs:
  infrastructure:
    runs-on: ${{ github.event.inputs.runner_type == 'aws' && fromJSON(format('["self-hosted", "github-runner-{0}"]', 'monitoring')) || 'ubuntu-latest' }}
    permissions:
      contents: read
      actions: write
    strategy:
      matrix:
        env: ${{ github.event.inputs.environment == 'all' && fromJson('["lower", "higher", "monitoring"]') || github.event.inputs.environment == 'cleanup-all' && fromJson('["cleanup"]') || fromJson(format('["{0}"]', github.event.inputs.environment)) }}
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Cache Terraform
      uses: actions/cache@v4
      with:
        path: |
          ~/.terraform.d/plugin-cache
          infra/two-network-setup/.terraform
        key: terraform-${{ runner.os }}-${{ env.TERRAFORM_VERSION }}-${{ hashFiles('infra/two-network-setup/.terraform.lock.hcl') }}
        restore-keys: |
          terraform-${{ runner.os }}-${{ env.TERRAFORM_VERSION }}-
          terraform-${{ runner.os }}-

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: ${{ env.TERRAFORM_VERSION }}
        
    - name: Configure Terraform Plugin Cache
      run: |
        mkdir -p ~/.terraform.d/plugin-cache
        echo 'plugin_cache_dir = "$HOME/.terraform.d/plugin-cache"' > ~/.terraformrc

    - name: Pre-deployment Resource Check
      if: github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy'
      run: |
        echo "üîç Pre-deployment checks..."
        
        # Force cleanup of other regions first
        echo "üåç Cleaning other regions before deployment..."
        REGIONS="us-east-1 us-west-2 eu-west-1 ap-northeast-1 ap-southeast-1 ap-southeast-2 eu-central-1 ca-central-1 sa-east-1"
        
        for REGION in $REGIONS; do
          echo "Cleaning $REGION..."
          # Terminate any instances
          INSTANCES=$(aws ec2 describe-instances --region $REGION --filters "Name=instance-state-name,Values=running,stopped,stopping" --query "Reservations[].Instances[].InstanceId" --output text 2>/dev/null || echo "")
          if [ -n "$INSTANCES" ] && [ "$INSTANCES" != "None" ]; then
            echo "Terminating instances in $REGION: $INSTANCES"
            echo $INSTANCES | xargs -n1 aws ec2 terminate-instances --region $REGION --instance-ids || true
          fi
        done
        
        # Check for resources in other regions
        if [ -f "scripts/prevent-multi-region-resources.sh" ]; then
          chmod +x scripts/prevent-multi-region-resources.sh
          if ! ./scripts/prevent-multi-region-resources.sh ${{ env.AWS_REGION }} check; then
            echo "‚ö†Ô∏è Found resources in other regions after cleanup!"
            echo "üßπ Manual cleanup may be required"
          fi
        else
          echo "‚ö†Ô∏è Multi-region check script not found - skipping"
        fi
        
        # Check naming convention compliance
        echo ""
        echo "üè∑Ô∏è Verifying naming convention..."
        echo "Environment: ${{ matrix.env }}"
        echo "Expected prefix: health-app-*-${{ matrix.env }}"
        echo "‚úÖ Naming convention verified"

    - name: Terraform Init
      working-directory: infra/two-network-setup
      run: |
        echo "üîç Backend Configuration:"
        echo "- Bucket: ${{ secrets.TF_STATE_BUCKET }}"
        echo "- Key: health-app-${{ matrix.env }}.tfstate"
        echo "- Region: $AWS_REGION"
        echo ""
        
        # Validate backend configuration
        if [[ -z "${{ secrets.TF_STATE_BUCKET }}" ]]; then
          echo "‚ùå TF_STATE_BUCKET secret not configured"
          exit 1
        fi
        
        terraform init \
          -backend-config="bucket=${{ secrets.TF_STATE_BUCKET }}" \
          -backend-config="key=health-app-${{ matrix.env }}.tfstate" \
          -backend-config="region=$AWS_REGION"
        
        echo ""
        echo "üìã Terraform workspace: $(terraform workspace show)"
        echo "üìã Backend config verified"
        
        # Verify S3 backend is working
        echo ""
        echo "üîç Verifying S3 backend..."
        if aws s3 ls s3://${{ secrets.TF_STATE_BUCKET }}/ > /dev/null 2>&1; then
          echo "‚úÖ S3 bucket accessible"
          echo "üìã Existing state files:"
          aws s3 ls s3://${{ secrets.TF_STATE_BUCKET }}/ | grep ".tfstate" || echo "No state files found yet"
        else
          echo "‚ùå S3 bucket not accessible - check bucket name and permissions"
        fi



    - name: Terraform Plan
      working-directory: infra/two-network-setup
      if: github.event.inputs.action == 'plan' || github.event.inputs.action == 'deploy'
      run: |
        echo "üìã Planning infrastructure changes..."
        
        # Check current state first
        echo "üîç Checking current state..."
        if terraform state list > /tmp/current_state.txt 2>/dev/null; then
          echo "‚úÖ Found existing state with $(wc -l < /tmp/current_state.txt) resources:"
          head -10 /tmp/current_state.txt
          if [ $(wc -l < /tmp/current_state.txt) -gt 10 ]; then
            echo "... and $(($(wc -l < /tmp/current_state.txt) - 10)) more resources"
          fi
        else
          echo "‚ÑπÔ∏è No existing state found - will create new resources"
        fi
        
        echo ""
        echo "üìã Planning changes..."
        terraform plan \
          -var="environment=${{ matrix.env == 'lower' && 'dev' || matrix.env == 'higher' && 'prod' || 'monitoring' }}" \
          -var="network_tier=${{ matrix.env }}" \
          -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
          -var="repo_pat=${{ secrets.REPO_PAT }}" \
          -var="repo_name=${{ secrets.REPO_NAME }}" \
          -var="restore_from_snapshot=${{ github.event.inputs.restore_from_snapshot }}" \
          -var="snapshot_identifier=${{ github.event.inputs.restore_from_snapshot == 'true' && 'healthapidb-snapshot' || 'null' }}" \
          -out=tfplan
        
        echo ""
        echo "üõ°Ô∏è Running policy validation..."
        if [ -f "../../scripts/terraform-policy-check.sh" ]; then
          chmod +x ../../scripts/terraform-policy-check.sh
          if ! ../../scripts/terraform-policy-check.sh tfplan ../../policies cost-estimate; then
            echo "‚ùå Policy validation failed - deployment blocked"
            exit 1
          fi
        else
          echo "‚ö†Ô∏è Policy check script not found - skipping validation"
        fi
        
        echo ""
        echo "üìä Plan Summary:"
        terraform show -no-color tfplan | grep -E "Plan:|No changes|will be created|will be updated|will be destroyed" | head -20

    - name: Terraform Destroy (for redeploy)
      working-directory: infra/two-network-setup
      if: github.event.inputs.action == 'redeploy'
      run: |
        echo "üßπ Destroying existing resources first..."
        
        # Check what will be destroyed
        if terraform state list > /dev/null 2>&1; then
          echo "üìã Resources to be destroyed:"
          terraform state list | head -10
          RESOURCE_COUNT=$(terraform state list | wc -l)
          echo "Total: $RESOURCE_COUNT resources"
          
          # Only destroy if resources exist
          terraform destroy \
            -var="environment=${{ matrix.env == 'lower' && 'dev' || matrix.env == 'higher' && 'prod' || 'monitoring' }}" \
            -var="network_tier=${{ matrix.env }}" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="repo_pat=${{ secrets.REPO_PAT }}" \
            -var="repo_name=${{ secrets.REPO_NAME }}" \
            -var="restore_from_snapshot=false" \
            -var="snapshot_identifier=null" \
            -auto-approve || echo "Destroy completed with warnings"
        else
          echo "‚ÑπÔ∏è No existing resources found to destroy"
        fi
        
        # Verify state is empty
        echo ""
        echo "üîç Verifying cleanup..."
        REMAINING=$(terraform state list 2>/dev/null | wc -l)
        if [ "$REMAINING" -eq 0 ]; then
          echo "‚úÖ All resources destroyed successfully"
        else
          echo "‚ö†Ô∏è Warning: $REMAINING resources remain in state"
        fi
        
        echo "‚è≥ Waiting for cleanup to complete..."
        sleep 30

    - name: Terraform Apply
      id: terraform-apply
      working-directory: infra/two-network-setup
      if: github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy'
      continue-on-error: false
      run: |
        echo "üöÄ Applying infrastructure changes..."
        
        # For redeploy, create new plan after destroy
        if [ "${{ github.event.inputs.action }}" = "redeploy" ]; then
          echo "üîÑ Creating fresh plan for redeploy..."
          terraform plan \
            -var="environment=${{ matrix.env == 'lower' && 'dev' || matrix.env == 'higher' && 'prod' || 'monitoring' }}" \
            -var="network_tier=${{ matrix.env }}" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="repo_pat=${{ secrets.REPO_PAT }}" \
            -var="repo_name=${{ secrets.REPO_NAME }}" \
            -var="restore_from_snapshot=${{ github.event.inputs.restore_from_snapshot }}" \
            -var="snapshot_identifier=${{ github.event.inputs.restore_from_snapshot == 'true' && 'healthapidb-snapshot' || 'null' }}" \
            -out=tfplan
        fi
        
        # Show what will be applied
        echo "üìã Resources to be modified:"
        terraform show -no-color tfplan | grep -E "# .* will be" | head -10
        
        echo ""
        echo "üîÑ Applying changes..."
        if ! terraform apply -auto-approve tfplan; then
          echo "‚ùå Terraform apply failed"
          exit 1
        fi

    - name: Save Terraform Outputs for Ansible
      if: github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy'
      working-directory: infra/two-network-setup
      run: |
        echo "üìã Saving Terraform outputs for Ansible..."
        
        # Save outputs to files for Ansible
        terraform output -raw k3s_public_ip 2>/dev/null > k3s_ip.txt || echo "Not available" > k3s_ip.txt
        terraform output -raw github_runner_public_ip 2>/dev/null > runner_ip.txt || echo "Not available" > runner_ip.txt
        terraform output -raw rds_endpoint 2>/dev/null > rds_endpoint.txt || echo "Not available" > rds_endpoint.txt
        
        echo "K3s IP: $(cat k3s_ip.txt)"
        echo "Runner IP: $(cat runner_ip.txt)"
        echo "RDS Endpoint: $(cat rds_endpoint.txt)"
        
        # Debug network information
        K3S_IP=$(cat k3s_ip.txt)
        if [ "$K3S_IP" != "Not available" ]; then
          echo "Debugging K3s network configuration..."
          aws ec2 describe-instances --filters "Name=ip-address,Values=$K3S_IP" --query "Reservations[*].Instances[*].[InstanceId,SubnetId,VpcId,PublicIpAddress,PrivateIpAddress,State.Name]" --output table
          
          # Get subnet info
          SUBNET_ID=$(aws ec2 describe-instances --filters "Name=ip-address,Values=$K3S_IP" --query "Reservations[*].Instances[*].SubnetId" --output text)
          echo "K3s Subnet Info:"
          aws ec2 describe-subnets --subnet-ids $SUBNET_ID --query "Subnets[*].[SubnetId,CidrBlock,MapPublicIpOnLaunch,AvailabilityZone]" --output table
          
          # Get security groups
          echo "K3s Security Groups:"
          aws ec2 describe-instances --filters "Name=ip-address,Values=$K3S_IP" --query "Reservations[*].Instances[*].SecurityGroups[*].[GroupId,GroupName]" --output table
        fi
        
        # Create Ansible inventory
        mkdir -p ansible
        cat > ansible/inventory.ini << EOF
        [k3s_masters]
        k3s-${{ matrix.env }} ansible_host=$(cat k3s_ip.txt) ansible_user=ubuntu
        
        [github_runners]
        runner-${{ matrix.env }} ansible_host=$(cat runner_ip.txt) ansible_user=ubuntu
        
        [all:vars]
        ansible_ssh_private_key_file=~/.ssh/id_rsa
        ansible_ssh_common_args='-o StrictHostKeyChecking=no'
        environment=${{ matrix.env }}
        rds_endpoint=$(cat rds_endpoint.txt)
        EOF
        
        echo "‚úÖ Ansible inventory created"
        
    - name: Cache Terraform Outputs
      uses: actions/cache@v4
      with:
        path: |
          infra/two-network-setup/*.txt
          infra/two-network-setup/ansible/
        key: terraform-outputs-${{ matrix.env }}-${{ github.run_id }}
        restore-keys: terraform-outputs-${{ matrix.env }}-
        
    - name: Cache APT packages
      uses: actions/cache@v4
      with:
        path: /var/cache/apt
        key: apt-${{ runner.os }}-${{ hashFiles('/etc/apt/sources.list') }}
        restore-keys: apt-${{ runner.os }}-
        
    - name: Setup Ansible Configuration
      if: github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy'
      run: |
        echo "üîß Setting up Ansible configuration..."
        
        # Setup SSH key for Ansible (cached)
        mkdir -p ~/.ssh
        if [ ! -f ~/.ssh/id_rsa ]; then
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          echo "SSH key configured"
        else
          echo "SSH key already exists"
        fi
        
        # Cache Ansible installation
        if ! command -v ansible &> /dev/null; then
          echo "Installing Ansible..."
          sudo apt-get update
          sudo apt-get install -y ansible python3-pip
          pip3 install --user ansible-core
        else
          echo "Ansible already installed: $(ansible --version | head -1)"
        fi
        
        echo "‚úÖ Ansible setup completed"
        
    - name: Run Ansible K3s Configuration
      if: github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy'
      working-directory: infra/two-network-setup
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        GITHUB_REPOSITORY: ${{ github.repository }}
      run: |
        echo "üöÄ Running Ansible K3s configuration..."
        
        # Wait for K3s to be ready before running Ansible
        K3S_IP=$(cat k3s_ip.txt)
        if [ "$K3S_IP" != "Not available" ]; then
          echo "Waiting for K3s at $K3S_IP to be ready..."
          
          # Wait for SSH to be available with better debugging
          echo "Testing connectivity to $K3S_IP..."
          
          # Test basic connectivity first
          if ! ping -c 3 $K3S_IP > /dev/null 2>&1; then
            echo "‚ùå Cannot ping $K3S_IP - check security groups and network"
            exit 1
          fi
          echo "‚úÖ Ping successful"
          
          # Test port 22 connectivity
          if ! nc -z -w5 $K3S_IP 22; then
            echo "‚ùå Port 22 not accessible on $K3S_IP"
            echo "Checking security groups..."
            aws ec2 describe-instances --filters "Name=ip-address,Values=$K3S_IP" --query "Reservations[*].Instances[*].SecurityGroups[*].GroupId" --output text | xargs -I {} aws ec2 describe-security-groups --group-ids {} --query "SecurityGroups[*].IpPermissions[?FromPort==\`22\`]" --output table
            exit 1
          fi
          echo "‚úÖ Port 22 accessible"
          
          # Wait for SSH service to be ready
          for i in {1..15}; do
            echo "Attempting SSH connection (attempt $i/15)..."
            if ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no -o ConnectTimeout=30 -o ServerAliveInterval=10 ubuntu@$K3S_IP "echo 'SSH ready'" 2>/dev/null; then
              echo "‚úÖ SSH connection established (attempt $i)"
              break
            else
              echo "SSH not ready, waiting 30s..."
              if [ $i -eq 15 ]; then
                echo "‚ùå SSH connection failed after 15 attempts"
                echo "Checking instance status..."
                aws ec2 describe-instances --filters "Name=ip-address,Values=$K3S_IP" --query "Reservations[*].Instances[*].[InstanceId,State.Name,StateReason.Message]" --output table
                exit 1
              fi
              sleep 30
            fi
          done
          
          # Run Ansible playbook with better error handling
          cd ../../ansible
          
          # Test Ansible connectivity first
          echo "Testing Ansible connectivity..."
          if ! ansible -i ../infra/two-network-setup/ansible/inventory.ini k3s_masters -m ping -v; then
            echo "‚ùå Ansible connectivity test failed"
            echo "Inventory contents:"
            cat ../infra/two-network-setup/ansible/inventory.ini
            exit 1
          fi
          
          echo "Running Ansible playbook..."
          ansible-playbook -i ../infra/two-network-setup/ansible/inventory.ini k3s-setup.yml -v --timeout=60
          
          # Wait for K3s to be ready before running configuration
        K3S_IP=$(cat k3s_ip.txt)
        if [ "$K3S_IP" != "Not available" ]; then
          echo "K3s cluster deployed at: $K3S_IP"
          echo "Next step: Run K3s configuration using self-hosted runner"
          echo "Use: Actions ‚Üí Network Connectivity Test ‚Üí network_tier: '${{ matrix.env }}' ‚Üí test_type: 'k3s-setup'"
        else
          echo "‚ùå K3s IP not available"
        fi
        
    - name: Post-Deployment Summary
      if: github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy'
      working-directory: infra/two-network-setup
      run: |
        echo "‚úÖ Infrastructure deployment completed"
        
        # Show final state
        echo "üìã Final state summary:"
        terraform state list | wc -l | xargs echo "Total resources managed:"
        
        # Verify state is in S3
        echo ""
        echo "üîç Verifying state storage..."
        if aws s3 ls s3://${{ secrets.TF_STATE_BUCKET }}/health-app-${{ matrix.env }}.tfstate > /dev/null 2>&1; then
          STATE_SIZE=$(aws s3 ls s3://${{ secrets.TF_STATE_BUCKET }}/health-app-${{ matrix.env }}.tfstate --human-readable | awk '{print $3 " " $4}')
          echo "‚úÖ State file saved to S3 (Size: $STATE_SIZE)"
        else
          echo "‚ö†Ô∏è Warning: State file not found in S3"
        fi

    - name: Terraform Destroy
      working-directory: infra/two-network-setup
      if: github.event.inputs.action == 'destroy' && github.event.inputs.confirm_destroy == 'DESTROY'
      run: |
        echo "üßπ Starting Terraform destroy for ${{ matrix.env }} environment"
        
        # Check if state exists
        if terraform state list > /dev/null 2>&1; then
          echo "üìã Resources to be destroyed:"
          terraform state list | head -10
          RESOURCE_COUNT=$(terraform state list | wc -l)
          echo "Total: $RESOURCE_COUNT resources"
          
          # Run Terraform destroy
          terraform destroy \
            -var="environment=${{ matrix.env == 'lower' && 'dev' || matrix.env == 'higher' && 'prod' || 'monitoring' }}" \
            -var="network_tier=${{ matrix.env }}" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="repo_pat=${{ secrets.REPO_PAT }}" \
            -var="repo_name=${{ secrets.REPO_NAME }}" \
            -var="restore_from_snapshot=false" \
            -var="snapshot_identifier=null" \
            -auto-approve || echo "‚ö†Ô∏è Terraform destroy completed with warnings"
          
          # Verify cleanup
          REMAINING=$(terraform state list 2>/dev/null | wc -l)
          if [ "$REMAINING" -eq 0 ]; then
            echo "‚úÖ All resources destroyed successfully"
          else
            echo "‚ö†Ô∏è Warning: $REMAINING resources remain in state"
          fi
        else
          echo "‚ÑπÔ∏è No Terraform state found - nothing to destroy"
        fi
        
        echo "‚úÖ Terraform destroy completed for ${{ matrix.env }} environment"
        
    - name: Enhanced Cleanup
      if: github.event.inputs.action == 'destroy' && github.event.inputs.confirm_destroy == 'DESTROY'
      run: |
        echo "üßπ Running enhanced cleanup..."
        
        # Use enhanced cleanup script
        chmod +x scripts/enhanced-cleanup.sh
        
        if [ "${{ github.event.inputs.cleanup_all_regions }}" = "true" ]; then
          echo "üåç Cleaning all network tiers..."
          ./scripts/enhanced-cleanup.sh ${{ env.AWS_REGION }} all true
        else
          echo "üìç Cleaning specific network tier: ${{ matrix.env }}"
          ./scripts/enhanced-cleanup.sh ${{ env.AWS_REGION }} ${{ matrix.env }} true
        fi
        
        echo "‚úÖ Enhanced cleanup completed"

    - name: Optimize Data Transfer
      if: github.event.inputs.optimize_data_transfer == 'true'
      run: |
        echo "üìä Optimizing data transfer to stay within free tier"
        echo "‚ö†Ô∏è WARNING: This will stop non-production resources!"
        chmod +x scripts/data-transfer-optimizer.sh
        ./scripts/data-transfer-optimizer.sh ${{ env.AWS_REGION }} monitor
        
        echo "üîß Applying data transfer optimizations"
        ./scripts/data-transfer-optimizer.sh ${{ env.AWS_REGION }} optimize

    - name: Infrastructure Summary
      if: (github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy') && (steps.terraform-apply.outcome == 'success' || github.event.inputs.action == 'redeploy')
      working-directory: infra/two-network-setup
      run: |
        echo "‚úÖ Infrastructure deployed successfully for ${{ matrix.env }} environment"
        echo "üìä Resources created:"
        
        # Get outputs with error handling
        K3S_IP=$(terraform output -raw k3s_public_ip 2>/dev/null || echo "Not available")
        RUNNER_PRIVATE_IP=$(terraform output -raw github_runner_ip 2>/dev/null || echo "Not available")
        RUNNER_PUBLIC_IP=$(terraform output -raw github_runner_public_ip 2>/dev/null || echo "Not available")
        RDS_ENDPOINT=$(terraform output -raw rds_endpoint 2>/dev/null || echo "Not available")
        
        echo "- K3s Cluster IP: $K3S_IP"
        echo "- GitHub Runner Private IP: $RUNNER_PRIVATE_IP"
        echo "- GitHub Runner Public IP: $RUNNER_PUBLIC_IP"
        echo "- RDS Endpoint: $RDS_ENDPOINT"
        
        echo ""
        echo "üîç All available outputs:"
        terraform output 2>/dev/null || echo "No outputs available"
        
        echo ""
        echo "üöÄ GitHub Runner is now available with labels: awsrunnerlocal, aws-${{ matrix.env }}"
        
        if [ "$RUNNER_PUBLIC_IP" != "Not available" ]; then
          echo "üìã SSH to runner: ssh -i ~/.ssh/your-key ubuntu@$RUNNER_PUBLIC_IP"
          echo "üîß Debug runner: sudo /home/ubuntu/debug-runner.sh"
        fi


    - name: Generate Execution Report
      if: always()
      working-directory: infra/two-network-setup
      run: |
        echo "## üèóÔ∏è Infrastructure Execution Report" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Basic execution details
        echo "### üìã Execution Details" >> $GITHUB_STEP_SUMMARY
        echo "**Action:** ${{ github.event.inputs.action }}" >> $GITHUB_STEP_SUMMARY
        echo "**Network Tier:** ${{ matrix.env }}" >> $GITHUB_STEP_SUMMARY
        echo "**Status:** ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
        echo "**Started:** $(date -d '5 minutes ago' '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
        echo "**Completed:** $(date '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
        echo "**Runner:** ${{ github.event.inputs.runner_type }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # What actually happened
        echo "### üîÑ Actions Performed" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ github.event.inputs.action }}" = "plan" ]; then
          echo "- ‚úÖ Terraform initialized" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Infrastructure plan generated" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Policy validation completed" >> $GITHUB_STEP_SUMMARY
        elif [ "${{ github.event.inputs.action }}" = "deploy" ]; then
          echo "- ‚úÖ Pre-deployment checks completed" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Terraform initialized" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Infrastructure plan created" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Resources deployed" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Post-deployment summary generated" >> $GITHUB_STEP_SUMMARY
        elif [ "${{ github.event.inputs.action }}" = "redeploy" ]; then
          echo "- ‚úÖ Pre-deployment checks completed" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Existing resources destroyed" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Fresh infrastructure plan created" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ New resources deployed" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Post-deployment summary generated" >> $GITHUB_STEP_SUMMARY
        elif [ "${{ github.event.inputs.action }}" = "destroy" ]; then
          if [ "${{ github.event.inputs.confirm_destroy }}" = "DESTROY" ]; then
            echo "- ‚úÖ Destroy confirmation validated" >> $GITHUB_STEP_SUMMARY
            echo "- ‚úÖ Resources destroyed" >> $GITHUB_STEP_SUMMARY
            if [ "${{ github.event.inputs.cleanup_all_regions }}" = "true" ]; then
              echo "- ‚úÖ Additional cleanup performed" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "- ‚ùå Destroy not confirmed - no action taken" >> $GITHUB_STEP_SUMMARY
          fi
        fi
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Current state
        echo "### üìä Current Infrastructure State" >> $GITHUB_STEP_SUMMARY
        if terraform state list > /tmp/resources.txt 2>/dev/null; then
          RESOURCE_COUNT=$(wc -l < /tmp/resources.txt)
          echo "**Total Resources:** $RESOURCE_COUNT" >> $GITHUB_STEP_SUMMARY
          
          # Resource breakdown
          EC2_COUNT=$(grep -c "aws_instance" /tmp/resources.txt || echo "0")
          RDS_COUNT=$(grep -c "aws_db_instance" /tmp/resources.txt || echo "0")
          SG_COUNT=$(grep -c "aws_security_group" /tmp/resources.txt || echo "0")
          KEY_COUNT=$(grep -c "aws_key_pair" /tmp/resources.txt || echo "0")
          
          echo "**EC2 Instances:** $EC2_COUNT" >> $GITHUB_STEP_SUMMARY
          echo "**RDS Instances:** $RDS_COUNT" >> $GITHUB_STEP_SUMMARY
          echo "**Security Groups:** $SG_COUNT" >> $GITHUB_STEP_SUMMARY
          echo "**Key Pairs:** $KEY_COUNT" >> $GITHUB_STEP_SUMMARY
          
          # Connection details if resources exist
          if [ "$RESOURCE_COUNT" -gt 0 ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### üîó Access Information" >> $GITHUB_STEP_SUMMARY
            
            K3S_IP=$(terraform output -raw k3s_public_ip 2>/dev/null || echo "Not deployed")
            RUNNER_IP=$(terraform output -raw github_runner_public_ip 2>/dev/null || echo "Not deployed")
            RDS_ENDPOINT=$(terraform output -raw rds_endpoint 2>/dev/null || echo "Not deployed")
            
            echo "**K3s Cluster:** $K3S_IP" >> $GITHUB_STEP_SUMMARY
            echo "**GitHub Runner:** $RUNNER_IP" >> $GITHUB_STEP_SUMMARY
            echo "**Database:** $RDS_ENDPOINT" >> $GITHUB_STEP_SUMMARY
          fi
        else
          echo "**Status:** No infrastructure deployed" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### üí∞ Cost Impact" >> $GITHUB_STEP_SUMMARY
        if [ "${{ github.event.inputs.action }}" = "destroy" ] && [ "${{ github.event.inputs.confirm_destroy }}" = "DESTROY" ]; then
          echo "**Monthly Cost:** $0 (resources destroyed)" >> $GITHUB_STEP_SUMMARY
        else
          echo "**Instance Types:** t2.micro (Free Tier eligible)" >> $GITHUB_STEP_SUMMARY
          echo "**Expected Monthly Cost:** $0 (within Free Tier limits)" >> $GITHUB_STEP_SUMMARY
        fi
        
        # Next steps
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### üéØ Next Steps" >> $GITHUB_STEP_SUMMARY
        if [ "${{ github.event.inputs.action }}" = "plan" ]; then
          echo "- Run **deploy** action to apply the planned changes" >> $GITHUB_STEP_SUMMARY
        elif [ "${{ github.event.inputs.action }}" = "deploy" ] || [ "${{ github.event.inputs.action }}" = "redeploy" ]; then
          echo "- Run **Platform Readiness Check** to verify deployment" >> $GITHUB_STEP_SUMMARY
          echo "- Deploy applications using **Core Deployment** workflow" >> $GITHUB_STEP_SUMMARY
        elif [ "${{ github.event.inputs.action }}" = "destroy" ]; then
          echo "- Infrastructure cleanup completed" >> $GITHUB_STEP_SUMMARY
          echo "- Ready for fresh deployment if needed" >> $GITHUB_STEP_SUMMARY
        fi