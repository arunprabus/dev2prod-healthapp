name: Core Infrastructure

on:
  workflow_dispatch:
    inputs:
      action:
        description: 'Action'
        default: 'deploy'
        required: true
        type: choice
        options:
        - redeploy
        - deploy
        - destroy
        - plan
      environment:
        description: 'Network/Environment'
        required: true
        default: 'lower'
        type: choice
        options:
        - lower
        - higher
        - monitoring
        - all
        - cleanup-all
      confirm_destroy:
        description: 'Type "DESTROY" to confirm'
        required: false
        type: string
      restore_from_snapshot:
        description: 'Restore RDS from snapshot'
        required: false
        default: false
        type: boolean
      runner_type:
        description: 'Runner Type'
        required: false
        default: 'github'
        type: choice
        options:
        - github
        - aws
      optimize_data_transfer:
        description: 'Run data transfer optimization'
        required: false
        default: false
        type: boolean
      cleanup_all_regions:
        description: 'Cleanup all AWS regions (for destroy only)'
        required: false
        default: false
        type: boolean

env:
  AWS_REGION: ap-south-1
  TERRAFORM_VERSION: 1.6.0

jobs:
  infrastructure:
    runs-on: ${{ github.event.inputs.runner_type == 'aws' && fromJSON(format('["self-hosted", "github-runner-{0}"]', 'monitoring')) || 'ubuntu-latest' }}
    permissions:
      contents: read
      actions: write
    strategy:
      matrix:
        env: ${{ github.event.inputs.environment == 'all' && fromJson('["lower", "higher", "monitoring"]') || github.event.inputs.environment == 'cleanup-all' && fromJson('["cleanup"]') || fromJson(format('["{0}"]', github.event.inputs.environment)) }}
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Clear Cache (for redeploy)
      if: github.event.inputs.action == 'redeploy'
      run: |
        echo "🧹 Clearing ALL Terraform cache for fresh redeploy..."
        
        # Clear plugin cache
        rm -rf ~/.terraform.d/plugin-cache
        
        # Clear working directory cache
        rm -rf infra/live/.terraform
        rm -rf infra/live/.terraform.lock.hcl
        
        # Clear any module cache
        find infra -name ".terraform" -type d -exec rm -rf {} + 2>/dev/null || true
        find infra -name ".terraform.lock.hcl" -type f -delete 2>/dev/null || true
        
        # Clear any cached plans
        find infra -name "tfplan" -type f -delete 2>/dev/null || true
        find infra -name "*.tfplan" -type f -delete 2>/dev/null || true
        
        echo "✅ All Terraform cache cleared - modules will be freshly loaded"
        
    - name: Setup Terraform Cache Directory
      run: |
        mkdir -p ~/.terraform.d/plugin-cache
        mkdir -p infra/live/.terraform
        
    - name: Cache Terraform
      if: github.event.inputs.action != 'redeploy'
      uses: actions/cache@v4
      with:
        path: ~/.terraform.d/plugin-cache
        key: terraform-plugins-${{ runner.os }}-${{ env.TERRAFORM_VERSION }}
        restore-keys: terraform-plugins-${{ runner.os }}-

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: ${{ env.TERRAFORM_VERSION }}
        
    - name: Configure Terraform Plugin Cache
      run: |
        mkdir -p ~/.terraform.d/plugin-cache
        echo 'plugin_cache_dir = "$HOME/.terraform.d/plugin-cache"' > ~/.terraformrc

    - name: Pre-deployment Resource Cleanup
      if: github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy'
      continue-on-error: true
      run: |
        echo "🧹 Pre-deployment resource cleanup..."
        
        # Run cleanup script
        if [ -f "scripts/cleanup-resources.sh" ]; then
          chmod +x scripts/cleanup-resources.sh
          if ./scripts/cleanup-resources.sh; then
            echo "✅ Cleanup completed successfully"
          else
            echo "⚠️ Cleanup had issues but continuing with deployment"
            echo "💡 Terraform will handle any remaining resource conflicts"
          fi
        else
          echo "⚠️ Cleanup script not found - running manual cleanup"
          
          # Manual cleanup for VPC limit
          echo "🔍 Checking VPC usage..."
          VPC_COUNT=$(aws ec2 describe-vpcs --query 'length(Vpcs)')
          echo "Current VPC count: $VPC_COUNT/5"
          
          if [ $VPC_COUNT -ge 5 ]; then
            echo "⚠️ VPC limit reached - cleaning up old health-app VPCs"
            
            # Get health-app VPCs
            VPCS=$(aws ec2 describe-vpcs \
              --filters "Name=tag:Project,Values=health-app" \
              --query 'Vpcs[*].[VpcId,Tags[?Key==`Name`].Value|[0]]' \
              --output text)
            
            if [ -n "$VPCS" ]; then
              echo "Found health-app VPCs to clean:"
              echo "$VPCS"
              
              echo "$VPCS" | while read -r vpc_id vpc_name; do
                # Check for running instances
                RUNNING=$(aws ec2 describe-instances \
                  --filters "Name=vpc-id,Values=$vpc_id" "Name=instance-state-name,Values=running" \
                  --query 'Reservations[*].Instances[*].InstanceId' --output text)
                
                if [ -z "$RUNNING" ] || [ "$RUNNING" = "None" ]; then
                  echo "🗑️ Deleting empty VPC: $vpc_name ($vpc_id)"
                  
                  # Delete subnets
                  aws ec2 describe-subnets --filters "Name=vpc-id,Values=$vpc_id" --query 'Subnets[*].SubnetId' --output text | \
                  xargs -r -n1 -I {} aws ec2 delete-subnet --subnet-id {} || true
                  
                  # Delete internet gateways
                  aws ec2 describe-internet-gateways --filters "Name=attachment.vpc-id,Values=$vpc_id" --query 'InternetGateways[*].InternetGatewayId' --output text | \
                  xargs -r -n1 -I {} sh -c 'aws ec2 detach-internet-gateway --internet-gateway-id {} --vpc-id '$vpc_id' && aws ec2 delete-internet-gateway --internet-gateway-id {}' || true
                  
                  # Delete route tables
                  aws ec2 describe-route-tables --filters "Name=vpc-id,Values=$vpc_id" --query 'RouteTables[?Associations[0].Main!=`true`].RouteTableId' --output text | \
                  xargs -r -n1 -I {} aws ec2 delete-route-table --route-table-id {} || true
                  
                  # Delete security groups
                  aws ec2 describe-security-groups --filters "Name=vpc-id,Values=$vpc_id" --query 'SecurityGroups[?GroupName!=`default`].GroupId' --output text | \
                  xargs -r -n1 -I {} aws ec2 delete-security-group --group-id {} || true
                  
                  # Delete VPC
                  aws ec2 delete-vpc --vpc-id "$vpc_id" || true
                  echo "✅ VPC $vpc_name deleted"
                else
                  echo "⚠️ VPC $vpc_name has running instances, skipping"
                fi
              done
            fi
          fi
        fi
        
        # Final VPC check
        VPC_COUNT_AFTER=$(aws ec2 describe-vpcs --query 'length(Vpcs)')
        echo "VPC count after cleanup: $VPC_COUNT_AFTER/5"
        
        if [ $VPC_COUNT_AFTER -ge 5 ]; then
          echo "⚠️ VPC limit still at maximum after cleanup"
          echo "💡 Terraform will attempt deployment - may fail if new VPCs needed"
        fi
        
        echo "✅ Pre-deployment cleanup completed"

    - name: Terraform Init
      working-directory: infra/live
      run: |
        echo "🔍 Backend Configuration:"
        echo "- Bucket: ${{ secrets.TF_STATE_BUCKET }}"
        echo "- Key: health-app-${{ matrix.env }}.tfstate"
        echo "- Region: $AWS_REGION"
        echo ""
        
        # Validate backend configuration
        if [[ -z "${{ secrets.TF_STATE_BUCKET }}" ]]; then
          echo "❌ TF_STATE_BUCKET secret not configured"
          exit 1
        fi
        
        terraform init -reconfigure \
          -backend-config="bucket=${{ secrets.TF_STATE_BUCKET }}" \
          -backend-config="key=health-app-${{ matrix.env }}.tfstate" \
          -backend-config="region=$AWS_REGION"
        
        echo ""
        echo "📋 Terraform workspace: $(terraform workspace show)"
        echo "📋 Backend config verified"
        
        # Verify S3 backend is working
        echo ""
        echo "🔍 Verifying S3 backend..."
        if aws s3 ls s3://${{ secrets.TF_STATE_BUCKET }}/ > /dev/null 2>&1; then
          echo "✅ S3 bucket accessible"
          echo "📋 Existing state files:"
          aws s3 ls s3://${{ secrets.TF_STATE_BUCKET }}/ | grep ".tfstate" || echo "No state files found yet"
        else
          echo "❌ S3 bucket not accessible - check bucket name and permissions"
        fiho ""
        echo "📋 Terraform workspace: $(terraform workspace show)"
        echo "📋 Backend config verified"
        
        # Verify S3 backend is working
        echo ""
        echo "🔍 Verifying S3 backend..."
        if aws s3 ls s3://${{ secrets.TF_STATE_BUCKET }}/ > /dev/null 2>&1; then
          echo "✅ S3 bucket accessible"
          echo "📋 Existing state files:"
          aws s3 ls s3://${{ secrets.TF_STATE_BUCKET }}/ | grep ".tfstate" || echo "No state files found yet"
        else
          echo "❌ S3 bucket not accessible - check bucket name and permissions"
        fi

    - name: Terraform Plan
      working-directory: infra/live
      if: github.event.inputs.action == 'plan' || github.event.inputs.action == 'deploy'
      run: |
        echo "📋 Planning infrastructure changes..."
        
        # Check current state first
        echo "🔍 Checking current state..."
        if terraform state list > /tmp/current_state.txt 2>/dev/null; then
          echo "✅ Found existing state with $(wc -l < /tmp/current_state.txt) resources:"
          head -10 /tmp/current_state.txt
          if [ $(wc -l < /tmp/current_state.txt) -gt 10 ]; then
            echo "... and $(($(wc -l < /tmp/current_state.txt) - 10)) more resources"
          fi
        else
          echo "ℹ️ No existing state found - will create new resources"
        fi
        
        echo ""
        echo "📋 Planning changes..."
        terraform plan \
          -var="environment=${{ matrix.env == 'lower' && 'dev' || matrix.env == 'higher' && 'prod' || 'monitoring' }}" \
          -var="network_tier=${{ matrix.env }}" \
          -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
          -var="repo_pat=${{ secrets.REPO_PAT }}" \
          -var="repo_name=${{ secrets.REPO_NAME }}" \
          -var="restore_from_snapshot=${{ github.event.inputs.restore_from_snapshot }}" \
          -var="snapshot_identifier=${{ github.event.inputs.restore_from_snapshot == 'true' && 'healthapidb-snapshot' || 'null' }}" \
          -out=tfplan
        
        echo ""
        echo "🛡️ Running policy validation..."
        if [ -f "../../scripts/terraform-policy-check.sh" ]; then
          chmod +x ../../scripts/terraform-policy-check.sh
          if ! ../../scripts/terraform-policy-check.sh tfplan ../../policies cost-estimate; then
            echo "❌ Policy validation failed - deployment blocked"
            exit 1
          fi
        else
          echo "⚠️ Policy check script not found - skipping validation"
        fi
        
        echo ""
        echo "📊 Plan Summary:"
        terraform show -no-color tfplan | grep -E "Plan:|No changes|will be created|will be updated|will be destroyed" | head -20

    - name: Terraform Destroy (for redeploy)
      working-directory: infra/live
      if: github.event.inputs.action == 'redeploy'
      run: |
        echo "🧹 Destroying existing resources first..."
        
        # Check what will be destroyed
        if terraform state list > /dev/null 2>&1; then
          echo "📋 Resources to be destroyed:"
          terraform state list | head -10
          RESOURCE_COUNT=$(terraform state list | wc -l)
          echo "Total: $RESOURCE_COUNT resources"
          
          # Only destroy if resources exist
          terraform destroy \
            -var="environment=${{ matrix.env == 'lower' && 'dev' || matrix.env == 'higher' && 'prod' || 'monitoring' }}" \
            -var="network_tier=${{ matrix.env }}" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="repo_pat=${{ secrets.REPO_PAT }}" \
            -var="repo_name=${{ secrets.REPO_NAME }}" \
            -var="restore_from_snapshot=false" \
            -var="snapshot_identifier=null" \
            -auto-approve || echo "Destroy completed with warnings"
        else
          echo "ℹ️ No existing resources found to destroy"
        fi
        
        # Verify state is empty
        echo ""
        echo "🔍 Verifying cleanup..."
        REMAINING=$(terraform state list 2>/dev/null | wc -l)
        if [ "$REMAINING" -eq 0 ]; then
          echo "✅ All resources destroyed successfully"
        else
          echo "⚠️ Warning: $REMAINING resources remain in state"
        fi
        
        echo "⏳ Waiting for cleanup to complete..."
        sleep 30

    - name: Post-deployment Cleanup Check
      if: failure() && (github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy')
      run: |
        echo "🧹 Post-failure cleanup..."
        
        # Clean up any partially created resources
        if [ -f "scripts/cleanup-resources.sh" ]; then
          chmod +x scripts/cleanup-resources.sh
          ./scripts/cleanup-resources.sh
        fi
        
        echo "✅ Post-failure cleanup completed"

    - name: Terraform Apply
      id: terraform-apply
      working-directory: infra/live
      if: github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy'
      continue-on-error: false
      run: |
        echo "🚀 Applying infrastructure changes..."
        
        # For redeploy, create new plan after destroy
        if [ "${{ github.event.inputs.action }}" = "redeploy" ]; then
          echo "🔄 Creating fresh plan for redeploy..."
          terraform plan \
            -var="environment=${{ matrix.env == 'lower' && 'dev' || matrix.env == 'higher' && 'prod' || 'monitoring' }}" \
            -var="network_tier=${{ matrix.env }}" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="repo_pat=${{ secrets.REPO_PAT }}" \
            -var="repo_name=${{ secrets.REPO_NAME }}" \
            -var="restore_from_snapshot=${{ github.event.inputs.restore_from_snapshot }}" \
            -var="snapshot_identifier=${{ github.event.inputs.restore_from_snapshot == 'true' && 'healthapidb-snapshot' || 'null' }}" \
            -out=tfplan
        fi
        
        # Show what will be applied
        echo "📋 Resources to be modified:"
        terraform show -no-color tfplan | grep -E "# .* will be" | head -10
        
        echo ""
        echo "🔄 Applying changes..."
        if ! terraform apply -auto-approve tfplan; then
          echo "❌ Terraform apply failed"
          exit 1
        fi

    - name: Verify Infrastructure
      if: (github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy') && matrix.env != 'cleanup'
      run: |
        echo "🔍 Verifying infrastructure deployment..."
        
        # Get K3s instance details
        K3S_IP=$(aws ec2 describe-instances \
          --filters "Name=tag:Name,Values=health-app-${{ matrix.env }}-k3s-node" "Name=instance-state-name,Values=running" \
          --query "Reservations[0].Instances[0].PublicIpAddress" --output text)
        
        if [ "$K3S_IP" != "None" ] && [ -n "$K3S_IP" ]; then
          echo "✅ K3s cluster deployed at: $K3S_IP"
          echo "📊 Kubeconfig will be available at: s3://${{ secrets.TF_STATE_BUCKET }}/kubeconfig/${{ matrix.env }}-kubeconfig.yaml"
        else
          echo "❌ No K3s instance found"
        fi

    - name: Infrastructure Summary
      if: (github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy') && (steps.terraform-apply.outcome == 'success' || github.event.inputs.action == 'redeploy')
      working-directory: infra/live
      run: |
        echo "✅ Infrastructure deployed successfully for ${{ matrix.env }} environment"
        echo "📊 See detailed network design and access info in the job summary tab"

    - name: Multi-Region Cleanup (if enabled)
      if: github.event.inputs.action == 'destroy' && github.event.inputs.cleanup_all_regions == 'true'
      run: |
        echo "🌍 Starting multi-region cleanup..."
        
        REGIONS="us-east-1 us-west-1 us-west-2 eu-west-1 eu-west-2 eu-central-1 ap-south-1 ap-southeast-1 ap-southeast-2 ap-northeast-1 ca-central-1 sa-east-1"
        
        for REGION in $REGIONS; do
          echo "🧹 Cleaning region: $REGION"
          
          # Terminate EC2 instances
          INSTANCES=$(aws ec2 describe-instances --region $REGION \
            --filters "Name=tag:Project,Values=health-app" "Name=instance-state-name,Values=running,stopped,stopping" \
            --query "Reservations[].Instances[].InstanceId" --output text 2>/dev/null || echo "")
          
          if [ -n "$INSTANCES" ] && [ "$INSTANCES" != "None" ]; then
            echo "Terminating instances in $REGION: $INSTANCES"
            echo $INSTANCES | xargs -n1 aws ec2 terminate-instances --region $REGION --instance-ids || true
            sleep 10
          fi
          
          # Delete RDS instances
          RDS_INSTANCES=$(aws rds describe-db-instances --region $REGION \
            --query "DBInstances[?contains(DBInstanceIdentifier, 'health-app')].DBInstanceIdentifier" --output text 2>/dev/null || echo "")
          
          if [ -n "$RDS_INSTANCES" ] && [ "$RDS_INSTANCES" != "None" ]; then
            echo "Deleting RDS instances in $REGION: $RDS_INSTANCES"
            for db in $RDS_INSTANCES; do
              aws rds delete-db-instance --region $REGION --db-instance-identifier $db --skip-final-snapshot || true
            done
          fi
          
          # Delete VPCs (this will delete IGWs, subnets, etc.)
          VPCS=$(aws ec2 describe-vpcs --region $REGION \
            --filters "Name=tag:Project,Values=health-app" \
            --query "Vpcs[].VpcId" --output text 2>/dev/null || echo "")
          
          if [ -n "$VPCS" ] && [ "$VPCS" != "None" ]; then
            echo "Deleting VPCs in $REGION: $VPCS"
            for vpc in $VPCS; do
              # Delete subnets first
              SUBNETS=$(aws ec2 describe-subnets --region $REGION --filters "Name=vpc-id,Values=$vpc" --query "Subnets[].SubnetId" --output text 2>/dev/null || echo "")
              for subnet in $SUBNETS; do
                aws ec2 delete-subnet --region $REGION --subnet-id $subnet || true
              done
              
              # Delete IGW
              IGW=$(aws ec2 describe-internet-gateways --region $REGION --filters "Name=attachment.vpc-id,Values=$vpc" --query "InternetGateways[].InternetGatewayId" --output text 2>/dev/null || echo "")
              if [ -n "$IGW" ] && [ "$IGW" != "None" ]; then
                aws ec2 detach-internet-gateway --region $REGION --internet-gateway-id $IGW --vpc-id $vpc || true
                aws ec2 delete-internet-gateway --region $REGION --internet-gateway-id $IGW || true
              fi
              
              # Delete VPC
              aws ec2 delete-vpc --region $REGION --vpc-id $vpc || true
            done
          fi
          
          # Delete security groups
          SGS=$(aws ec2 describe-security-groups --region $REGION \
            --filters "Name=tag:Project,Values=health-app" \
            --query "SecurityGroups[?GroupName!='default'].GroupId" --output text 2>/dev/null || echo "")
          
          if [ -n "$SGS" ] && [ "$SGS" != "None" ]; then
            echo "Deleting security groups in $REGION: $SGS"
            for sg in $SGS; do
              aws ec2 delete-security-group --region $REGION --group-id $sg || true
            done
          fi
          
          # Delete key pairs
          KEYS=$(aws ec2 describe-key-pairs --region $REGION \
            --filters "Name=tag:Project,Values=health-app" \
            --query "KeyPairs[].KeyName" --output text 2>/dev/null || echo "")
          
          if [ -n "$KEYS" ] && [ "$KEYS" != "None" ]; then
            echo "Deleting key pairs in $REGION: $KEYS"
            for key in $KEYS; do
              aws ec2 delete-key-pair --region $REGION --key-name $key || true
            done
          fi
          
          echo "✅ Cleanup completed for $REGION"
        done
        
        echo "🌍 Multi-region cleanup completed"

    - name: Terraform Destroy
      working-directory: infra/live
      if: github.event.inputs.action == 'destroy' && github.event.inputs.confirm_destroy == 'DESTROY'
      run: |
        echo "🧹 Starting Terraform destroy for ${{ matrix.env }} environment"
        
        # Check if state exists
        if terraform state list > /dev/null 2>&1; then
          echo "📋 Resources to be destroyed:"
          terraform state list | head -10
          RESOURCE_COUNT=$(terraform state list | wc -l)
          echo "Total: $RESOURCE_COUNT resources"
          
          # Run Terraform destroy
          terraform destroy \
            -var="environment=${{ matrix.env == 'lower' && 'dev' || matrix.env == 'higher' && 'prod' || 'monitoring' }}" \
            -var="network_tier=${{ matrix.env }}" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="repo_pat=${{ secrets.REPO_PAT }}" \
            -var="repo_name=${{ secrets.REPO_NAME }}" \
            -var="restore_from_snapshot=false" \
            -var="snapshot_identifier=null" \
            -auto-approve || echo "⚠️ Terraform destroy completed with warnings"
          
          # Verify cleanup
          REMAINING=$(terraform state list 2>/dev/null | wc -l)
          if [ "$REMAINING" -eq 0 ]; then
            echo "✅ All resources destroyed successfully"
          else
            echo "⚠️ Warning: $REMAINING resources remain in state"
          fi
        else
          echo "ℹ️ No Terraform state found - nothing to destroy"
        fi
        
        echo "✅ Terraform destroy completed for ${{ matrix.env }} environment"

    - name: Generate Execution Report
      if: always()
      working-directory: infra/live
      run: |
        echo "## 🏗️ Infrastructure Execution Report" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Basic execution details
        echo "### 📋 Execution Details" >> $GITHUB_STEP_SUMMARY
        echo "**Action:** ${{ github.event.inputs.action }}" >> $GITHUB_STEP_SUMMARY
        echo "**Network Tier:** ${{ matrix.env }}" >> $GITHUB_STEP_SUMMARY
        echo "**Status:** ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
        echo "**Completed:** $(date '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
        echo "**Runner:** ${{ github.event.inputs.runner_type }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Current state and detailed infrastructure info
        echo "### 📊 Infrastructure State & Network Design" >> $GITHUB_STEP_SUMMARY
        if terraform state list > /tmp/resources.txt 2>/dev/null; then
          RESOURCE_COUNT=$(wc -l < /tmp/resources.txt)
          echo "**Total Resources:** $RESOURCE_COUNT" >> $GITHUB_STEP_SUMMARY
          
          # Generate network diagram with actual IPs
          K3S_IP=$(aws ec2 describe-instances --filters "Name=tag:Name,Values=health-app-${{ matrix.env }}-k3s-node" "Name=instance-state-name,Values=running" --query "Reservations[0].Instances[0].PublicIpAddress" --output text 2>/dev/null || echo "N/A")
          RUNNER_IP=$(aws ec2 describe-instances --filters "Name=tag:NetworkTier,Values=${{ matrix.env }}" "Name=tag:Name,Values=*runner*" "Name=instance-state-name,Values=running" --query "Reservations[0].Instances[0].PublicIpAddress" --output text 2>/dev/null || echo "N/A")
          RDS_ENDPOINT=$(aws rds describe-db-instances --query "DBInstances[?contains(DBInstanceIdentifier, 'health-app-${{ matrix.env }}')].Endpoint.Address" --output text 2>/dev/null | cut -d'.' -f1 || echo "N/A")
          
          echo "#### 🌐 Network Architecture" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "┌─────────────────────────────────────────────────────────────┐" >> $GITHUB_STEP_SUMMARY
          echo "│                    AWS Region: ap-south-1                   │" >> $GITHUB_STEP_SUMMARY
          echo "├─────────────────────────────────────────────────────────────┤" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ matrix.env }}" = "lower" ]; then
            echo "│ ┌─────────────────────────────────────────────────────────┐ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │                 LOWER NETWORK (ACTIVE)                 │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ ┌─────────────┐  ┌─────────────┐  ┌─────────────────┐   │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ │   K3S NODE  │  │ GITHUB RUN  │  │    DATABASE     │   │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ │ $K3S_IP │  │ $RUNNER_IP │  │ $RDS_ENDPOINT   │   │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ │   t2.micro  │  │   t2.micro  │  │   db.t3.micro   │   │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ │  Port: 6443 │  │  SSH: 22    │  │   Port: 5432    │   │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ └─────────────┘  └─────────────┘  └─────────────────┘   │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ └─────────────────────────────────────────────────────────┘ │" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ matrix.env }}" = "higher" ]; then
            echo "│ ┌─────────────────────────────────────────────────────────┐ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │                HIGHER NETWORK (ACTIVE)                 │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ ┌─────────────┐                  ┌─────────────────────┐ │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ │   K3S NODE  │                  │    DATABASE         │ │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ │ $K3S_IP │                  │ $RDS_ENDPOINT       │ │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ │   t2.micro  │                  │   db.t3.micro       │ │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ │  Port: 6443 │                  │   Port: 5432        │ │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ └─────────────┘                  └─────────────────────┘ │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │                                                         │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ ┌─────────────┐                                        │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ │ GITHUB RUN  │                                        │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ │ $RUNNER_IP │                                        │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ │   t2.micro  │                                        │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ └─────────────┘                                        │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ └─────────────────────────────────────────────────────────┘ │" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ matrix.env }}" = "monitoring" ]; then
            echo "│ ┌─────────────────────────────────────────────────────────┐ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │              MONITORING NETWORK (ACTIVE)               │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ ┌─────────────────────────────────────────────────────┐ │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ │              MONITORING CLUSTER                     │ │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ │         K3s Master + GitHub Runner                  │ │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ │         $K3S_IP + $RUNNER_IP                        │ │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ │         Prometheus + Grafana                        │ │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ │                t2.micro                             │ │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ └─────────────────────────────────────────────────────┘ │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ └─────────────────────────────────────────────────────────┘ │" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "└─────────────────────────────────────────────────────────────┘" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "#### 💻 Resource Details" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          aws ec2 describe-instances --filters "Name=tag:NetworkTier,Values=${{ matrix.env }}" "Name=instance-state-name,Values=running" --query "Reservations[*].Instances[*].[Tags[?Key=='Name'].Value|[0],InstanceType,PublicIpAddress,PrivateIpAddress,State.Name]" --output table >> $GITHUB_STEP_SUMMARY 2>/dev/null || echo "No EC2 instances found" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Connection details if resources exist
          if [ "$RESOURCE_COUNT" -gt 0 ]; then
            echo "#### 🚀 Access Information" >> $GITHUB_STEP_SUMMARY
            
            if [ "$K3S_IP" != "None" ] && [ -n "$K3S_IP" ]; then
              echo "**K3s Cluster:** \`ssh -i ~/.ssh/key ubuntu@$K3S_IP\`" >> $GITHUB_STEP_SUMMARY
              echo "**K3s API:** https://$K3S_IP:6443" >> $GITHUB_STEP_SUMMARY
            fi
            
            if [ "$RUNNER_IP" != "None" ] && [ -n "$RUNNER_IP" ]; then
              echo "**GitHub Runner:** \`ssh -i ~/.ssh/key ubuntu@$RUNNER_IP\`" >> $GITHUB_STEP_SUMMARY
              echo "**Runner Labels:** self-hosted, github-runner-${{ matrix.env }}" >> $GITHUB_STEP_SUMMARY
            fi
            
            if [ "$RDS_ENDPOINT" != "None" ] && [ -n "$RDS_ENDPOINT" ]; then
              echo "**Database:** $RDS_ENDPOINT:5432" >> $GITHUB_STEP_SUMMARY
            fi
            
            # DNS Setup Instructions
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "#### 🌐 DNS Setup Required (sharpzeal.com)" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            if [ "${{ matrix.env }}" = "lower" ] && [ "$K3S_IP" != "None" ]; then
              echo "**📋 Add these A records in Namecheap:**" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "| Host | Value | TTL |" >> $GITHUB_STEP_SUMMARY
              echo "|------|-------|-----|" >> $GITHUB_STEP_SUMMARY
              echo "| \`dev\` | \`$K3S_IP\` | 300 |" >> $GITHUB_STEP_SUMMARY
              echo "| \`test\` | \`$K3S_IP\` | 300 |" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "**🎯 Result URLs:**" >> $GITHUB_STEP_SUMMARY
              echo "- Development: https://dev.sharpzeal.com" >> $GITHUB_STEP_SUMMARY
              echo "- Test: https://test.sharpzeal.com" >> $GITHUB_STEP_SUMMARY
            elif [ "${{ matrix.env }}" = "higher" ] && [ "$K3S_IP" != "None" ]; then
              echo "**📋 Add this A record in Namecheap:**" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "| Host | Value | TTL |" >> $GITHUB_STEP_SUMMARY
              echo "|------|-------|-----|" >> $GITHUB_STEP_SUMMARY
              echo "| \`health-api\` | \`$K3S_IP\` | 300 |" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "**🎯 Result URL:**" >> $GITHUB_STEP_SUMMARY
              echo "- Production: https://health-api.sharpzeal.com" >> $GITHUB_STEP_SUMMARY
            elif [ "${{ matrix.env }}" = "monitoring" ] && [ "$K3S_IP" != "None" ]; then
              echo "**📋 Monitoring cluster deployed:**" >> $GITHUB_STEP_SUMMARY
              echo "- IP: $K3S_IP" >> $GITHUB_STEP_SUMMARY
              echo "- Used for centralized monitoring and GitHub runners" >> $GITHUB_STEP_SUMMARY
            fi
            
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "#### 🔧 Next Steps" >> $GITHUB_STEP_SUMMARY
            echo "1. **Setup DNS** - Add A records in Namecheap (see table above)" >> $GITHUB_STEP_SUMMARY
            echo "2. **Setup Ingress** - SSH to cluster and run: \`./scripts/setup-ingress.sh ${{ matrix.env == 'lower' && 'dev' || matrix.env == 'higher' && 'prod' || 'monitoring' }}\`" >> $GITHUB_STEP_SUMMARY
            echo "3. **Deploy Apps** - Run Core Deployment workflow" >> $GITHUB_STEP_SUMMARY
            echo "4. **Verify SSL** - Check https URLs after DNS propagation (5-10 min)" >> $GITHUB_STEP_SUMMARY
          fi
        else
          echo "**Status:** No infrastructure deployed" >> $GITHUB_STEP_SUMMARY
        fi

  kubeconfig:
    runs-on: ${{ fromJSON(format('["self-hosted", "github-runner-{0}"]', matrix.env)) }}
    needs: infrastructure
    if: (github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy') && needs.infrastructure.result == 'success'
    strategy:
      matrix:
        env: ${{ github.event.inputs.environment == 'all' && fromJson('["lower", "higher", "monitoring"]') || fromJson(format('["{0}"]', github.event.inputs.environment)) }}
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4
      
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ap-south-1

    - name: Verify tools
      run: |
        # Verify pre-installed tools
        aws --version
        kubectl version --client
        terraform version
        docker --version
        gh --version

    - name: Pre-Validation System Check
      run: |
        echo "🔍 Pre-validation system check for ${{ matrix.env }} environment..."
        
        # Get K3s instance ID
        INSTANCE_ID=$(aws ec2 describe-instances \
          --filters "Name=tag:Name,Values=health-app-${{ matrix.env }}-k3s-node" "Name=instance-state-name,Values=running" \
          --query "Reservations[0].Instances[0].InstanceId" --output text)
        
        if [ "$INSTANCE_ID" != "None" ] && [ -n "$INSTANCE_ID" ]; then
          echo "📋 Found K3s instance: $INSTANCE_ID"
          
          # Wait for SSM to be ready
          echo "⏳ Waiting for SSM agent to be online..."
          for i in {1..10}; do
            SSM_STATUS=$(aws ssm describe-instance-information \
              --filters "Key=InstanceIds,Values=$INSTANCE_ID" \
              --query "InstanceInformationList[0].PingStatus" --output text 2>/dev/null || echo "Offline")
            
            if [ "$SSM_STATUS" = "Online" ]; then
              echo "✅ SSM agent is online"
              break
            fi
            echo "⏳ SSM not ready, waiting... ($i/10)"
            if [ $i -eq 10 ]; then
              echo "⚠️ SSM agent not online after 5 minutes, continuing anyway..."
            else
              sleep 30
            fi
          done
          
          if [ "$SSM_STATUS" = "Online" ]; then
            # Get comprehensive K3s diagnostics
            echo "📊 Getting comprehensive K3s diagnostics via SSM..."
            DIAG_CMD=$(aws ssm send-command \
              --instance-ids "$INSTANCE_ID" \
              --document-name "AWS-RunShellScript" \
              --parameters 'commands=["echo === COMPREHENSIVE K3S DIAGNOSTICS === && echo Generated at: $(date) && echo && echo === NETWORK INFORMATION === && echo Hostname and IPs: && hostname -I && echo Network interfaces: && ip addr show | grep -E \"inet |UP|DOWN\" && echo && echo === K3S SERVICE STATUS === && systemctl status k3s --no-pager && echo Service is-active: && systemctl is-active k3s && echo && echo === NETWORK PORTS === && echo Ports listening on 6443: && sudo ss -tulnp | grep 6443 && echo && echo === KUBECONFIG ANALYSIS === && echo Server URL in kubeconfig: && cat /etc/rancher/k3s/k3s.yaml | grep server && echo && echo === K3S LOGS LAST 30 LINES === && sudo journalctl -u k3s -n 30 --no-pager && echo && echo === KUBERNETES API TESTS === && echo kubectl get nodes: && kubectl get nodes --insecure-skip-tls-verify 2>/dev/null || echo kubectl failed && echo && echo === FINAL STATUS === && if kubectl get nodes --insecure-skip-tls-verify >/dev/null 2>&1; then echo K3S STATUS: HEALTHY; else echo K3S STATUS: UNHEALTHY; fi"]' \
              --query "Command.CommandId" --output text)
            
            sleep 20
            
            DIAG_OUTPUT=$(aws ssm get-command-invocation \
              --command-id "$DIAG_CMD" \
              --instance-id "$INSTANCE_ID" \
              --query "StandardOutputContent" --output text 2>/dev/null || echo "Could not retrieve diagnostics")
            
            echo "📋 K3s Diagnostics Report:"
            echo "$DIAG_OUTPUT"
            
            # Add to job summary
            echo "## 🔍 K3s Diagnostics Report" >> $GITHUB_STEP_SUMMARY
            echo "**Environment:** ${{ matrix.env }}" >> $GITHUB_STEP_SUMMARY
            echo "**Instance ID:** $INSTANCE_ID" >> $GITHUB_STEP_SUMMARY
            echo "**Timestamp:** $(date)" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "$DIAG_OUTPUT" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          else
            echo "⚠️ SSM agent not available - skipping diagnostics"
          fi
        else
          echo "❌ No K3s instance found"
          exit 1
        fi



    - name: Setup kubeconfig via SSH
      run: |
        echo "🔧 Setting up kubeconfig for ${{ matrix.env }} environment..."
        
        # Get K3s instance details
        K3S_DETAILS=$(aws ec2 describe-instances \
          --filters "Name=tag:Name,Values=health-app-${{ matrix.env }}-k3s-node" "Name=instance-state-name,Values=running" \
          --query "Reservations[0].Instances[0].[InstanceId,PublicIpAddress]" --output text)
        
        if [ "$K3S_DETAILS" = "None" ]; then
          echo "❌ No K3s instance found for ${{ matrix.env }} environment"
          exit 1
        fi
        
        K3S_INSTANCE_ID=$(echo $K3S_DETAILS | cut -d' ' -f1)
        K3S_IP=$(echo $K3S_DETAILS | cut -d' ' -f2)
        
        echo "📥 Connecting to K3s via SSH: $K3S_IP"
        
        # Setup SSH key
        mkdir -p ~/.ssh
        echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/k3s-key
        chmod 600 ~/.ssh/k3s-key
        
        # Download kubeconfig via SSH with retry
        echo "📥 Downloading kubeconfig via SSH..."
        for i in {1..10}; do
          if scp -i ~/.ssh/k3s-key -o ConnectTimeout=30 -o StrictHostKeyChecking=no ubuntu@$K3S_IP:/etc/rancher/k3s/k3s.yaml /tmp/kubeconfig.yaml; then
            echo "✅ Kubeconfig downloaded successfully (attempt $i)"
            break
          else
            echo "⏳ Kubeconfig not ready yet (attempt $i/10), waiting..."
            if [ $i -eq 10 ]; then
              echo "❌ Failed to download kubeconfig after 10 attempts"
              echo "💡 Run the K3s Validation workflow to troubleshoot"
              exit 1
            fi
            sleep 60
          fi
        done
        
        # Update server IP to public IP
        sed -i "s/127.0.0.1/$K3S_IP/g" /tmp/kubeconfig.yaml
        
        # Upload kubeconfig to S3
        echo "📤 Uploading kubeconfig to S3..."
        aws s3 cp /tmp/kubeconfig.yaml s3://${{ secrets.TF_STATE_BUCKET }}/kubeconfig/${{ matrix.env }}-kubeconfig.yaml
        
        # Get final diagnostics for job summary
        echo "📊 Getting final cluster diagnostics..."
        FINAL_DIAG_CMD=$(aws ssm send-command \
          --instance-ids "$K3S_INSTANCE_ID" \
          --document-name "AWS-RunShellScript" \
          --parameters 'commands=["echo === FINAL CLUSTER STATUS === && echo Cluster IP: $(hostname -I | awk {print $1}) && echo Public IP: $(curl -s http://169.254.169.254/latest/meta-data/public-ipv4) && echo K3s Version: $(k3s --version | head -1) && echo API Status: && if kubectl get nodes --insecure-skip-tls-verify >/dev/null 2>&1; then echo HEALTHY; else echo UNHEALTHY; fi && echo Nodes: && kubectl get nodes --insecure-skip-tls-verify 2>/dev/null || echo No nodes && echo Namespaces: && kubectl get namespaces --insecure-skip-tls-verify 2>/dev/null | head -10 || echo No namespaces && echo Ingress Controller: && kubectl get pods -n ingress-nginx --insecure-skip-tls-verify 2>/dev/null || echo No ingress"]' \
          --query "Command.CommandId" --output text)
        
        sleep 10
        
        FINAL_DIAG=$(aws ssm get-command-invocation \
          --command-id "$FINAL_DIAG_CMD" \
          --instance-id "$K3S_INSTANCE_ID" \
          --query "StandardOutputContent" --output text 2>/dev/null || echo "Could not retrieve final diagnostics")
        
        # Generate comprehensive summary
        echo "## 🔐 Kubeconfig Ready" >> $GITHUB_STEP_SUMMARY
        echo "**Environment:** ${{ matrix.env }}" >> $GITHUB_STEP_SUMMARY
        echo "**Cluster:** $K3S_IP:6443" >> $GITHUB_STEP_SUMMARY
        echo "**S3 Location:** s3://${{ secrets.TF_STATE_BUCKET }}/kubeconfig/${{ matrix.env }}-kubeconfig.yaml" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### 📊 Final Cluster Status" >> $GITHUB_STEP_SUMMARY
        echo '```' >> $GITHUB_STEP_SUMMARY
        echo "$FINAL_DIAG" >> $GITHUB_STEP_SUMMARY
        echo '```' >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### 🔧 Next Steps" >> $GITHUB_STEP_SUMMARY
        echo "1. Run **K3s Validation** workflow to verify cluster health" >> $GITHUB_STEP_SUMMARY
        echo "2. Run **Core Deployment** workflow to deploy applications" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo '```bash' >> $GITHUB_STEP_SUMMARY
        echo "# Manual SSH access" >> $GITHUB_STEP_SUMMARY
        echo "ssh -i ~/.ssh/k3s-key ubuntu@$K3S_IP" >> $GITHUB_STEP_SUMMARY
        echo '```' >> $GITHUB_STEP_SUMMARY
        
        # Cleanup
        rm -f ~/.ssh/k3s-key /tmp/kubeconfig.yaml
        echo "🎉 Kubeconfig configured and uploaded to S3 for ${{ matrix.env }} environment"