name: Core Infrastructure

on:
  push:
    branches:
      - main
    paths:
      - 'infra/**'
      - '.github/workflows/core-infrastructure.yml'
  workflow_dispatch:
    inputs:
      action:
        description: 'Action'
        required: true
        type: choice
        options:
          - deploy
          - destroy
          - plan
          - redeploy
      network:
        description: 'Network Tier'
        required: true
        default: 'lower'
        type: choice
        options:
          - lower
          - higher
          - monitoring
      confirm_destroy:
        description: 'Type "DESTROY" to confirm'
        required: false
        type: string

env:
  AWS_REGION: ap-south-1
  TERRAFORM_VERSION: 1.6.0
  DEFAULT_ACTION: redeploy
  DEFAULT_NETWORK: lower

jobs:
  # Step 1: Terraform → Creates instances
  terraform-deploy:
    runs-on: ubuntu-latest
    outputs:
      cluster_ip: ${{ steps.redeploy.outputs.cluster_ip || steps.deploy.outputs.cluster_ip }}
      dev_cluster_ip: ${{ steps.redeploy.outputs.dev_cluster_ip || steps.deploy.outputs.dev_cluster_ip }}
      test_cluster_ip: ${{ steps.redeploy.outputs.test_cluster_ip || steps.deploy.outputs.test_cluster_ip }}

    steps:
      - uses: actions/checkout@v4

      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}
          terraform_wrapper: false

      - name: Cache Terraform
        uses: actions/cache@v3
        with:
          path: |
            infra/.terraform
            infra/.terraform.lock.hcl
            ~/.terraform.d/plugin-cache
          key: terraform-${{ env.TERRAFORM_VERSION }}-${{ github.event.inputs.network || env.DEFAULT_NETWORK }}-${{ hashFiles('infra/**/*.tf', 'infra/**/*.tfvars') }}
          restore-keys: |
            terraform-${{ env.TERRAFORM_VERSION }}-${{ github.event.inputs.network || env.DEFAULT_NETWORK }}-
            terraform-${{ env.TERRAFORM_VERSION }}-

      - name: Setup Terraform Plugin Cache
        run: |
          mkdir -p ~/.terraform.d/plugin-cache
          echo 'plugin_cache_dir = "$HOME/.terraform.d/plugin-cache"' > ~/.terraformrc
      
      - name: Terraform Init
        working-directory: infra
        run: |
          terraform init -reconfigure \
            -backend-config="bucket=${{ secrets.TF_STATE_BUCKET }}" \
            -backend-config="key=health-app-${{ github.event.inputs.network || env.DEFAULT_NETWORK }}.tfstate" \
            -backend-config="region=$AWS_REGION"

      - name: Terraform Plan
        if: github.event.inputs.action == 'plan'
        working-directory: infra
        run: |
          terraform plan \
            -var-file="environments/${{ github.event.inputs.network || env.DEFAULT_NETWORK }}.tfvars" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="github_pat=${{ secrets.REPO_PAT }}" \
            -var="aws_region=${{ env.AWS_REGION }}"

      - name: Complete Redeploy
        id: redeploy
        if: github.event.inputs.action == 'redeploy' || github.event_name == 'push'
        working-directory: infra
        run: |
          echo "🔥 Complete infrastructure redeploy - deleting everything..."
          
          # Manual cleanup first
          echo "🧹 Manual cleanup of persistent resources..."
          aws rds delete-db-instance --db-instance-identifier health-app-shared-db --skip-final-snapshot --delete-automated-backups || true
          
          # Wait for RDS deletion
          echo "⏳ Waiting for RDS deletion..."
          for i in {1..15}; do
            if ! aws rds describe-db-instances --db-instance-identifier health-app-shared-db >/dev/null 2>&1; then
              echo "✅ RDS instance deleted"
              break
            fi
            echo "Waiting for RDS deletion... ($i/15)"
            sleep 20
          done
          
          # Delete parameter group and subnet group
          aws rds delete-db-parameter-group --db-parameter-group-name health-app-shared-db-params || true
          aws rds delete-db-subnet-group --db-subnet-group-name health-app-shared-db-subnet-group || true
          aws kms delete-alias --alias-name alias/health-app-rds-export || true
          
          # Force remove Terraform state for problematic resources
          terraform state rm 'module.rds[0].aws_db_parameter_group.health_db' || true
          terraform state rm 'module.rds[0].aws_db_subnet_group.health_db' || true
          
          # Now destroy everything else
          terraform destroy \
            -var-file="environments/${{ github.event.inputs.network || env.DEFAULT_NETWORK }}.tfvars" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="github_pat=${{ secrets.REPO_PAT }}" \
            -var="aws_region=${{ env.AWS_REGION }}" \
            -auto-approve || echo "Some resources may have been manually deleted"
          
          echo "⏳ Waiting 60 seconds for complete cleanup..."
          sleep 60
          
          echo "✅ Complete cleanup done, deploying fresh infrastructure..."
          
          # Fresh deployment
          terraform apply \
            -var-file="environments/${{ github.event.inputs.network || env.DEFAULT_NETWORK }}.tfvars" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="github_pat=${{ secrets.REPO_PAT }}" \
            -var="aws_region=${{ env.AWS_REGION }}" \
            -auto-approve
          
          # Output cluster IPs
          if [ "${{ github.event.inputs.network || env.DEFAULT_NETWORK }}" == "lower" ]; then
            echo "dev_cluster_ip=$(terraform output -raw dev_cluster_ip)" >> $GITHUB_OUTPUT
            echo "test_cluster_ip=$(terraform output -raw test_cluster_ip)" >> $GITHUB_OUTPUT
          else
            echo "cluster_ip=$(terraform output -raw k3s_instance_ip)" >> $GITHUB_OUTPUT
          fi
      
      - name: Import Existing Resources
        if: github.event.inputs.action == 'deploy'
        working-directory: infra
        run: |
          echo "🔄 Importing existing RDS resources if they exist..."
          
          # Import existing RDS resources
          terraform import 'module.rds[0].aws_db_subnet_group.health_db' health-app-shared-db-subnet-group 2>/dev/null || echo "Subnet group doesn't exist, will create"
          terraform import 'module.rds[0].aws_db_parameter_group.health_db' health-app-shared-db-params 2>/dev/null || echo "Parameter group doesn't exist, will create"
          terraform import 'module.rds[0].aws_db_instance.health_db' health-app-shared-db 2>/dev/null || echo "RDS instance doesn't exist, will create"
          
          echo "✅ Import step completed"
      
      - name: Incremental Deploy
        id: deploy
        if: github.event.inputs.action == 'deploy'
        working-directory: infra
        run: |
          echo "🚀 Incremental deployment - applying changes only..."
          
          terraform apply \
            -var-file="environments/${{ github.event.inputs.network || env.DEFAULT_NETWORK }}.tfvars" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="github_pat=${{ secrets.REPO_PAT }}" \
            -var="aws_region=${{ env.AWS_REGION }}" \
            -auto-approve
          
          # Output cluster IPs
          if [ "${{ github.event.inputs.network || env.DEFAULT_NETWORK }}" == "lower" ]; then
            echo "dev_cluster_ip=$(terraform output -raw dev_cluster_ip)" >> $GITHUB_OUTPUT
            echo "test_cluster_ip=$(terraform output -raw test_cluster_ip)" >> $GITHUB_OUTPUT
          else
            echo "cluster_ip=$(terraform output -raw k3s_instance_ip)" >> $GITHUB_OUTPUT
          fi

      - name: Terraform Destroy
        if: github.event.inputs.action == 'destroy'
        working-directory: infra
        run: |
          if [ "${{ github.event.inputs.action || env.DEFAULT_ACTION }}" == "destroy" ] && [ "${{ github.event.inputs.confirm_destroy }}" != "DESTROY" ]; then
            echo "❌ Type 'DESTROY' to confirm explicit destroy action"
            exit 1
          fi
          
          echo "🔥 Destroying infrastructure..."
          
          terraform destroy \
            -var-file="environments/${{ github.event.inputs.network || env.DEFAULT_NETWORK }}.tfvars" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="github_pat=${{ secrets.REPO_PAT }}" \
            -var="aws_region=${{ env.AWS_REGION }}" \
            -auto-approve

  # Step 2: Setup Runner Service via AWS Run Command
  setup-runner-service:
    needs: terraform-deploy
    if: (github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy') || github.event_name == 'push'
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
      
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Get Instance IDs
        id: instances
        run: |
          # Wait for instances to be ready
          echo "⏳ Waiting 30 seconds for instances to initialize..."
          sleep 30
          
          if [ "${{ github.event.inputs.network || env.DEFAULT_NETWORK }}" == "lower" ]; then
            DEV_INSTANCE=$(aws ec2 describe-instances \
              --filters "Name=tag:Name,Values=health-app-dev-k3s" "Name=instance-state-name,Values=running" \
              --query 'Reservations[0].Instances[0].InstanceId' --output text 2>/dev/null || echo "None")
            TEST_INSTANCE=$(aws ec2 describe-instances \
              --filters "Name=tag:Name,Values=health-app-test-k3s" "Name=instance-state-name,Values=running" \
              --query 'Reservations[0].Instances[0].InstanceId' --output text 2>/dev/null || echo "None")
            
            echo "Found dev instance: $DEV_INSTANCE"
            echo "Found test instance: $TEST_INSTANCE"
            echo "dev_instance=$DEV_INSTANCE" >> $GITHUB_OUTPUT
            echo "test_instance=$TEST_INSTANCE" >> $GITHUB_OUTPUT
          else
            INSTANCE=$(aws ec2 describe-instances \
              --filters "Name=tag:Name,Values=health-app-k3s" "Name=instance-state-name,Values=running" \
              --query 'Reservations[0].Instances[0].InstanceId' --output text 2>/dev/null || echo "None")
            
            echo "Found instance: $INSTANCE"
            echo "instance=$INSTANCE" >> $GITHUB_OUTPUT
          fi
      
      - name: Fix Runner Service (Lower Tier)
        if: (github.event.inputs.network || env.DEFAULT_NETWORK) == 'lower'
        run: |
          RUNNER_FIX_SCRIPT='#!/bin/bash
          set -e
          cd /home/ubuntu/actions-runner || exit 1
          if [ ! -f "svc.sh" ]; then
            cat > svc.sh << "SVCEOF"
          #!/bin/bash
          SVC_NAME="actions.runner.$(cat .runner | jq -r ".gitHubUrl" | sed "s/https:\/\/github.com\///").$(cat .runner | jq -r ".runnerName").service"
          USER_ID=${2:-ubuntu}
          RUNNER_ROOT=$(pwd)
          case $1 in
              install)
                  sudo tee /etc/systemd/system/$SVC_NAME > /dev/null << UNIT
          [Unit]
          Description=GitHub Actions Runner
          After=network.target
          [Service]
          ExecStart=$RUNNER_ROOT/run.sh
          User=$USER_ID
          WorkingDirectory=$RUNNER_ROOT
          KillMode=process
          KillSignal=SIGTERM
          TimeoutStopSec=5min
          [Install]
          WantedBy=multi-user.target
          UNIT
                  sudo systemctl daemon-reload
                  sudo systemctl enable $SVC_NAME
                  ;;
              start) sudo systemctl start $SVC_NAME ;;
              stop) sudo systemctl stop $SVC_NAME ;;
              status) sudo systemctl status $SVC_NAME ;;
          esac
          SVCEOF
            chmod +x svc.sh
          fi
          ./svc.sh install ubuntu
          ./svc.sh start
          echo "Runner service status:"
          ./svc.sh status'
          
          # Fix dev instance
          if [ "${{ steps.instances.outputs.dev_instance }}" != "None" ] && [ "${{ steps.instances.outputs.dev_instance }}" != "" ]; then
            echo "🔧 Fixing dev runner service..."
            
            # Wait for SSM agent to be online
            for i in {1..10}; do
              if aws ssm describe-instance-information --filters "Key=InstanceIds,Values=${{ steps.instances.outputs.dev_instance }}" --query 'InstanceInformationList[0].PingStatus' --output text | grep -q "Online"; then
                echo "✅ SSM agent online for dev instance"
                break
              fi
              echo "⏳ Waiting for SSM agent... ($i/10)"
              sleep 30
            done
            
            COMMAND_ID=$(aws ssm send-command \
              --instance-ids "${{ steps.instances.outputs.dev_instance }}" \
              --document-name "AWS-RunShellScript" \
              --parameters "commands=[\"$RUNNER_FIX_SCRIPT\"]" \
              --query 'Command.CommandId' --output text)
            aws ssm wait command-executed --command-id "$COMMAND_ID" --instance-id "${{ steps.instances.outputs.dev_instance }}" --timeout-seconds 300
            aws ssm get-command-invocation --command-id "$COMMAND_ID" --instance-id "${{ steps.instances.outputs.dev_instance }}" --query 'StandardOutputContent' --output text
          fi
          
          # Fix test instance
          if [ "${{ steps.instances.outputs.test_instance }}" != "None" ] && [ "${{ steps.instances.outputs.test_instance }}" != "" ]; then
            echo "🔧 Fixing test runner service..."
            
            # Wait for SSM agent to be online
            for i in {1..10}; do
              if aws ssm describe-instance-information --filters "Key=InstanceIds,Values=${{ steps.instances.outputs.test_instance }}" --query 'InstanceInformationList[0].PingStatus' --output text | grep -q "Online"; then
                echo "✅ SSM agent online for test instance"
                break
              fi
              echo "⏳ Waiting for SSM agent... ($i/10)"
              sleep 30
            done
            
            COMMAND_ID=$(aws ssm send-command \
              --instance-ids "${{ steps.instances.outputs.test_instance }}" \
              --document-name "AWS-RunShellScript" \
              --parameters "commands=[\"$RUNNER_FIX_SCRIPT\"]" \
              --query 'Command.CommandId' --output text)
            aws ssm wait command-executed --command-id "$COMMAND_ID" --instance-id "${{ steps.instances.outputs.test_instance }}" --timeout-seconds 300
            aws ssm get-command-invocation --command-id "$COMMAND_ID" --instance-id "${{ steps.instances.outputs.test_instance }}" --query 'StandardOutputContent' --output text
          fi
      
      - name: Fix Runner Service (Higher Tier)
        if: (github.event.inputs.network || env.DEFAULT_NETWORK) != 'lower'
        run: |
          RUNNER_FIX_SCRIPT='#!/bin/bash
          set -e
          cd /home/ubuntu/actions-runner || exit 1
          if [ ! -f "svc.sh" ]; then
            cat > svc.sh << "SVCEOF"
          #!/bin/bash
          SVC_NAME="actions.runner.$(cat .runner | jq -r ".gitHubUrl" | sed "s/https:\/\/github.com\///").$(cat .runner | jq -r ".runnerName").service"
          USER_ID=${2:-ubuntu}
          RUNNER_ROOT=$(pwd)
          case $1 in
              install)
                  sudo tee /etc/systemd/system/$SVC_NAME > /dev/null << UNIT
          [Unit]
          Description=GitHub Actions Runner
          After=network.target
          [Service]
          ExecStart=$RUNNER_ROOT/run.sh
          User=$USER_ID
          WorkingDirectory=$RUNNER_ROOT
          KillMode=process
          KillSignal=SIGTERM
          TimeoutStopSec=5min
          [Install]
          WantedBy=multi-user.target
          UNIT
                  sudo systemctl daemon-reload
                  sudo systemctl enable $SVC_NAME
                  ;;
              start) sudo systemctl start $SVC_NAME ;;
              stop) sudo systemctl stop $SVC_NAME ;;
              status) sudo systemctl status $SVC_NAME ;;
          esac
          SVCEOF
            chmod +x svc.sh
          fi
          ./svc.sh install ubuntu
          ./svc.sh start
          echo "Runner service status:"
          ./svc.sh status'
          
          if [ "${{ steps.instances.outputs.instance }}" != "None" ] && [ "${{ steps.instances.outputs.instance }}" != "" ]; then
            echo "🔧 Fixing runner service..."
            COMMAND_ID=$(aws ssm send-command \
              --instance-ids "${{ steps.instances.outputs.instance }}" \
              --document-name "AWS-RunShellScript" \
              --parameters "commands=[\"$RUNNER_FIX_SCRIPT\"]" \
              --query 'Command.CommandId' --output text)
            aws ssm wait command-executed --command-id "$COMMAND_ID" --instance-id "${{ steps.instances.outputs.instance }}"
            aws ssm get-command-invocation --command-id "$COMMAND_ID" --instance-id "${{ steps.instances.outputs.instance }}" --query 'StandardOutputContent' --output text
          else
            echo "⚠️ No instance found for higher tier"
          fi

  # Step 3: Configure Security Groups → Sets up cross-SG references
  configure-security-groups:
    needs: [terraform-deploy, setup-runner-service]
    if: (github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy') || github.event_name == 'push'
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
      
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Configure Security Groups
        run: |
          echo "🔧 Step 2: Configuring security groups for cluster access..."
          
          # Wait for instances to be ready
          echo "⏳ Waiting 60 seconds for instances to initialize..."
          sleep 60
          
          # Configure cross-SG references for runner access
          echo "🔐 Setting up cross-SG references for secure access..."
          if [ "${{ github.event.inputs.network || env.DEFAULT_NETWORK }}" == "lower" ]; then
            # Get security group IDs
            DEV_SG=$(aws ec2 describe-instances \
              --filters "Name=tag:Name,Values=health-app-lower-dev-k3s-node-v2" \
              --query 'Reservations[0].Instances[0].SecurityGroups[0].GroupId' \
              --output text 2>/dev/null)
            
            TEST_SG=$(aws ec2 describe-instances \
              --filters "Name=tag:Name,Values=health-app-lower-test-k3s-node-v2" \
              --query 'Reservations[0].Instances[0].SecurityGroups[0].GroupId' \
              --output text 2>/dev/null)
            
            RUNNER_SG=$(aws ec2 describe-instances \
              --filters "Name=tag:Name,Values=health-app-runner-lower" \
              --query 'Reservations[0].Instances[0].SecurityGroups[0].GroupId' \
              --output text 2>/dev/null)
            
            # Add cross-SG references for secure access
            for SG in "$DEV_SG" "$TEST_SG"; do
              if [ "$SG" != "None" ] && [ -n "$SG" ] && [ "$RUNNER_SG" != "None" ] && [ -n "$RUNNER_SG" ]; then
                echo "Adding runner access to K3s API on $SG..."
                aws ec2 authorize-security-group-ingress \
                  --group-id "$SG" \
                  --ip-permissions '[{"IpProtocol":"tcp","FromPort":6443,"ToPort":6443,"UserIdGroupPairs":[{"GroupId":"'$RUNNER_SG'","Description":"K3s API access from runner"}]}]' 2>/dev/null || echo "Runner API rule may already exist"
                
                aws ec2 authorize-security-group-ingress \
                  --group-id "$SG" \
                  --ip-permissions '[{"IpProtocol":"tcp","FromPort":22,"ToPort":22,"UserIdGroupPairs":[{"GroupId":"'$RUNNER_SG'","Description":"SSH access from runner"}]}]' 2>/dev/null || echo "Runner SSH rule may already exist"
              fi
            done
            
            echo "✅ Cross-SG references configured for secure access"
          fi
          
          echo "✅ Step 2 Complete: Security group configuration done!"

  # Step 3: Create GitHub Runner → Launch EC2 and register it as a GitHub self-hosted runner
  create-github-runner:
    needs: [terraform-deploy, configure-security-groups]
    if: (github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy') || github.event_name == 'push'
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
      
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Create GitHub Runner with Enhanced Logging
        run: |
          echo "🏃 Step 3: Setting up GitHub self-hosted runner with detailed monitoring..."
          
          # Get runner instance details
          echo "🔍 Finding runner instance..."
          RUNNER_INSTANCE_ID=$(aws ec2 describe-instances \
            --filters "Name=tag:Name,Values=health-app-runner-${{ github.event.inputs.network || env.DEFAULT_NETWORK }}" "Name=instance-state-name,Values=running" \
            --query 'Reservations[0].Instances[0].InstanceId' \
            --output text 2>/dev/null)
          
          RUNNER_PUBLIC_IP=$(aws ec2 describe-instances \
            --filters "Name=tag:Name,Values=health-app-runner-${{ github.event.inputs.network || env.DEFAULT_NETWORK }}" "Name=instance-state-name,Values=running" \
            --query 'Reservations[0].Instances[0].PublicIpAddress' \
            --output text 2>/dev/null)
          
          if [ "$RUNNER_INSTANCE_ID" == "None" ] || [ -z "$RUNNER_INSTANCE_ID" ]; then
            echo "❌ Runner instance not found"
            echo "🔍 Checking all instances with runner tag..."
            aws ec2 describe-instances --filters "Name=tag:Name,Values=health-app-runner-*" --query 'Reservations[].Instances[].[InstanceId,State.Name,Tags[?Key==`Name`].Value|[0]]' --output table
            exit 1
          fi
          
          echo "✅ Runner instance found:"
          echo "   Instance ID: $RUNNER_INSTANCE_ID"
          echo "   Public IP: $RUNNER_PUBLIC_IP"
          
          # Monitor instance initialization
          echo "📊 Monitoring instance initialization..."
          for i in {1..20}; do
            INSTANCE_STATUS=$(aws ec2 describe-instance-status --instance-ids $RUNNER_INSTANCE_ID --query 'InstanceStatuses[0].InstanceStatus.Status' --output text 2>/dev/null || echo "initializing")
            SYSTEM_STATUS=$(aws ec2 describe-instance-status --instance-ids $RUNNER_INSTANCE_ID --query 'InstanceStatuses[0].SystemStatus.Status' --output text 2>/dev/null || echo "initializing")
            
            echo "   Check $i/20: Instance=$INSTANCE_STATUS, System=$SYSTEM_STATUS"
            
            if [ "$INSTANCE_STATUS" == "ok" ] && [ "$SYSTEM_STATUS" == "ok" ]; then
              echo "✅ Instance fully initialized"
              break
            fi
            
            if [ $i -eq 20 ]; then
              echo "⚠️ Instance still initializing after 10 minutes, but continuing..."
            fi
            
            sleep 30
          done
          
          # Monitor cloud-init and user data execution
          echo "📋 Monitoring cloud-init and runner setup..."
          for i in {1..30}; do
            echo "   Setup check $i/30 ($(date)):"
            
            # Check if we can connect via Session Manager
            if aws ssm describe-instance-information --filters "Key=InstanceIds,Values=$RUNNER_INSTANCE_ID" --query 'InstanceInformationList[0].PingStatus' --output text 2>/dev/null | grep -q "Online"; then
              echo "   ✅ SSM Agent online"
              
              # Check cloud-init status
              CLOUD_INIT_STATUS=$(aws ssm send-command \
                --instance-ids $RUNNER_INSTANCE_ID \
                --document-name "AWS-RunShellScript" \
                --parameters 'commands=["cloud-init status --wait --long 2>/dev/null || echo failed"]' \
                --query 'Command.CommandId' --output text 2>/dev/null)
              
              if [ -n "$CLOUD_INIT_STATUS" ] && [ "$CLOUD_INIT_STATUS" != "None" ]; then
                sleep 10
                CLOUD_INIT_RESULT=$(aws ssm get-command-invocation \
                  --command-id $CLOUD_INIT_STATUS \
                  --instance-id $RUNNER_INSTANCE_ID \
                  --query 'StandardOutputContent' --output text 2>/dev/null || echo "pending")
                echo "   📋 Cloud-init status: $CLOUD_INIT_RESULT"
              fi
              
              # Check if runner service is running
              RUNNER_CHECK=$(aws ssm send-command \
                --instance-ids $RUNNER_INSTANCE_ID \
                --document-name "AWS-RunShellScript" \
                --parameters 'commands=["systemctl is-active actions.runner.* 2>/dev/null || echo inactive"]' \
                --query 'Command.CommandId' --output text 2>/dev/null)
              
              if [ -n "$RUNNER_CHECK" ] && [ "$RUNNER_CHECK" != "None" ]; then
                sleep 5
                RUNNER_STATUS=$(aws ssm get-command-invocation \
                  --command-id $RUNNER_CHECK \
                  --instance-id $RUNNER_INSTANCE_ID \
                  --query 'StandardOutputContent' --output text 2>/dev/null || echo "checking")
                echo "   🏃 Runner service status: $RUNNER_STATUS"
                
                if echo "$RUNNER_STATUS" | grep -q "active"; then
                  echo "   ✅ Runner service is active!"
                  break
                fi
              fi
            else
              echo "   ⏳ SSM Agent not yet online"
              
              # Trigger SSM fix after 10 failed attempts
              if [ $i -eq 10 ]; then
                echo "   🔧 Triggering SSM Agent fix..."
                SSM_FIX_SCRIPT='#!/bin/bash
                echo "Fixing SSM Agent..."
                sudo snap install amazon-ssm-agent --classic || true
                sudo systemctl enable snap.amazon-ssm-agent.amazon-ssm-agent.service || true
                sudo systemctl start snap.amazon-ssm-agent.amazon-ssm-agent.service || true
                sudo systemctl status snap.amazon-ssm-agent.amazon-ssm-agent.service --no-pager || true
                echo "SSM Agent fix completed"'
                
                # Use user data to execute fix (fallback method)
                aws ec2 modify-instance-attribute \
                  --instance-id $RUNNER_INSTANCE_ID \
                  --user-data "$(echo "$SSM_FIX_SCRIPT" | base64 -w 0)" || echo "Could not modify user data"
                
                echo "   🔄 Restarting instance to apply SSM fix..."
                aws ec2 reboot-instances --instance-ids $RUNNER_INSTANCE_ID
                sleep 60  # Wait for reboot
              fi
            fi
            
            if [ $i -eq 30 ]; then
              echo "   ⚠️ Runner setup taking longer than expected, but continuing..."
            fi
            
            sleep 30
          done
          
          # Final status check
          echo "🔍 Final runner status check..."
          
          # Check GitHub API for registered runners
          echo "📡 Checking GitHub API for registered runners..."
          GITHUB_RUNNERS=$(curl -s -H "Authorization: token ${{ secrets.REPO_PAT }}" \
            -H "Accept: application/vnd.github.v3+json" \
            "https://api.github.com/repos/${{ github.repository }}/actions/runners" | \
            jq -r '.runners[] | select(.name | contains("github-runner-${{ github.event.inputs.network || env.DEFAULT_NETWORK }}")) | "\(.name) - \(.status)"')
          
          if [ -n "$GITHUB_RUNNERS" ]; then
            echo "✅ Found registered runners:"
            echo "$GITHUB_RUNNERS"
          else
            echo "⚠️ No runners found in GitHub API yet"
          fi
          
          # Get detailed logs via SSM
          echo "📋 Getting setup logs from instance..."
          LOG_COMMAND=$(aws ssm send-command \
            --instance-ids $RUNNER_INSTANCE_ID \
            --document-name "AWS-RunShellScript" \
            --parameters 'commands=["echo === Cloud-init output ===; tail -50 /var/log/cloud-init-output.log 2>/dev/null || echo No cloud-init log; echo === Runner config log ===; tail -20 /var/log/runner-config.log 2>/dev/null || echo No runner config log; echo === Service status ===; systemctl status actions.runner.* --no-pager 2>/dev/null || echo No service found"]' \
            --query 'Command.CommandId' --output text 2>/dev/null)
          
          if [ -n "$LOG_COMMAND" ] && [ "$LOG_COMMAND" != "None" ]; then
            sleep 10
            echo "📋 Instance logs:"
            aws ssm get-command-invocation \
              --command-id $LOG_COMMAND \
              --instance-id $RUNNER_INSTANCE_ID \
              --query 'StandardOutputContent' --output text 2>/dev/null || echo "Could not retrieve logs"
          fi
          
          # Create additional helper scripts on the instance
          echo "📝 Creating helper scripts on runner instance..."
          HELPER_COMMAND=$(aws ssm send-command \
            --instance-ids $RUNNER_INSTANCE_ID \
            --document-name "AWS-RunShellScript" \
            --parameters 'commands=["cat > /home/ubuntu/check-runner-github.sh << '"'"'EOF'"'"'\n#!/bin/bash\necho \"🔍 Checking runner registration with GitHub...\"\necho \"Runner name: github-runner-${{ github.event.inputs.network || env.DEFAULT_NETWORK }}-$(hostname | cut -d"-" -f3-)\"\necho \"Labels: github-runner-${{ github.event.inputs.network || env.DEFAULT_NETWORK }}\"\necho \"\"\necho \"📋 Service status:\"\nsystemctl status actions.runner.* --no-pager 2>/dev/null || echo \"No service found\"\necho \"\"\necho \"🔍 Process status:\"\nps aux | grep -E \"(Runner|run.sh)\" | grep -v grep || echo \"No runner process\"\necho \"\"\necho \"🌐 Network test:\"\ncurl -s --connect-timeout 10 https://api.github.com/rate_limit > /dev/null && echo \"✅ GitHub API accessible\" || echo \"❌ GitHub API not accessible\"\necho \"\"\necho \"📝 Recent logs:\"\ntail -10 /var/log/runner-config.log 2>/dev/null || echo \"No config log\"\nEOF\nchmod +x /home/ubuntu/check-runner-github.sh\nchown ubuntu:ubuntu /home/ubuntu/check-runner-github.sh"]' \
            --query 'Command.CommandId' --output text 2>/dev/null)
          
          if [ -n "$HELPER_COMMAND" ] && [ "$HELPER_COMMAND" != "None" ]; then
            sleep 5
            echo "✅ Helper scripts created on instance"
          fi
          
          echo "✅ Step 3 Complete: Runner setup monitoring finished!"
          echo "📋 Summary:"
          echo "   - Instance ID: $RUNNER_INSTANCE_ID"
          echo "   - Public IP: $RUNNER_PUBLIC_IP"
          echo "   - Check runner status in GitHub Settings > Actions > Runners"
          echo "   - Use Session Manager to connect: aws ssm start-session --target $RUNNER_INSTANCE_ID"
          echo "   - On instance, run: /home/ubuntu/check-runner-github.sh"
          echo "   - Debug with: /home/ubuntu/debug-runner.sh"
          echo "   - Restart with: /home/ubuntu/restart-runner.sh"

  # Step 4: Install and Configure K3s → Install K3s using the newly created GitHub runner (inside the VPC)
  install-k3s:
    needs: [terraform-deploy, create-github-runner]
    if: (github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy') || github.event_name == 'push'
    runs-on: self-hosted
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Install and Configure K3s
        env:
          DEV_CLUSTER_IP: ${{ needs.terraform-deploy.outputs.dev_cluster_ip }}
          TEST_CLUSTER_IP: ${{ needs.terraform-deploy.outputs.test_cluster_ip }}
          CLUSTER_IP: ${{ needs.terraform-deploy.outputs.cluster_ip }}
        run: |
          echo "☸️ Step 4: Installing and configuring K3s on cluster nodes using self-hosted runner..."
          
          # Function to install k3s on a node using script
          install_k3s() {
            local IP=$1
            local ENV=$2
            
            echo "Installing K3s on $ENV cluster ($IP)..."
            
            # Test SSH connectivity with retries
            echo "Testing SSH connectivity to $IP..."
            for retry in {1..5}; do
              if timeout 15 ssh -o StrictHostKeyChecking=no -o ConnectTimeout=10 ubuntu@$IP "echo 'SSH connection successful'" 2>/dev/null; then
                echo "✅ SSH connection established to $IP"
                break
              fi
              echo "⏳ SSH attempt $retry/5 failed, retrying in 10 seconds..."
              sleep 10
              if [ $retry -eq 5 ]; then
                echo "❌ Cannot connect to $IP via SSH after 5 attempts"
                return 1
              fi
            done
            
            # Copy and execute K3s installation script
            echo "Copying K3s installation script to $IP..."
            scp -o StrictHostKeyChecking=no scripts/install-k3s.sh ubuntu@$IP:/tmp/
            
            echo "Executing K3s installation on $IP..."
            ssh -o StrictHostKeyChecking=no ubuntu@$IP "chmod +x /tmp/install-k3s.sh && sudo /tmp/install-k3s.sh"
            
            if [ $? -eq 0 ]; then
              echo "✅ K3s installation completed on $ENV cluster"
            else
              echo "❌ K3s installation failed on $ENV cluster"
              return 1
            fi
          }
          
          # Install k3s based on network tier
          if [ "${{ github.event.inputs.network || env.DEFAULT_NETWORK }}" == "lower" ]; then
            success=true
            
            if [ -n "$DEV_CLUSTER_IP" ]; then
              install_k3s "$DEV_CLUSTER_IP" "dev" || success=false
            fi
            
            if [ -n "$TEST_CLUSTER_IP" ]; then
              install_k3s "$TEST_CLUSTER_IP" "test" || success=false
            fi
            
            if [ "$success" = false ]; then
              echo "❌ Some K3s installations failed"
              exit 1
            fi
          else
            if [ -n "$CLUSTER_IP" ]; then
              install_k3s "$CLUSTER_IP" "${{ github.event.inputs.network || env.DEFAULT_NETWORK }}"
            else
              echo "❌ Cluster IP not available"
              exit 1
            fi
          fi
          
          echo "🎉 Step 4 Complete: K3s installation and configuration done!"

  # Step 5: Verify K3s API Availability → Tests if K3s is responding
  verify-k3s-api:
    needs: [terraform-deploy, install-k3s]
    if: (github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy') || github.event_name == 'push'
    runs-on: self-hosted
    timeout-minutes: 20
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Verify K3s API Availability
        env:
          DEV_CLUSTER_IP: ${{ needs.terraform-deploy.outputs.dev_cluster_ip }}
          TEST_CLUSTER_IP: ${{ needs.terraform-deploy.outputs.test_cluster_ip }}
          CLUSTER_IP: ${{ needs.terraform-deploy.outputs.cluster_ip }}
        run: |
          echo "🔍 Step 5: Verifying K3s API availability..."
          
          # Function to test K3s API
          test_k3s_api() {
            local IP=$1
            local NAME=$2
            echo "Testing $NAME cluster at $IP..."
            
            for i in {1..20}; do
              if timeout 10 curl -k -s "https://$IP:6443/version" >/dev/null 2>&1; then
                echo "✅ $NAME cluster API is ready"
                return 0
              fi
              echo "⏳ $NAME cluster not ready, waiting... ($i/20)"
              sleep 30
            done
            echo "❌ $NAME cluster API not ready after 10 minutes"
            return 1
          }
          
          # Test clusters based on network tier
          if [ "${{ github.event.inputs.network || env.DEFAULT_NETWORK }}" == "lower" ]; then
            success=true
            
            if [ -n "$DEV_CLUSTER_IP" ]; then
              test_k3s_api "$DEV_CLUSTER_IP" "dev" || success=false
            fi
            
            if [ -n "$TEST_CLUSTER_IP" ]; then
              test_k3s_api "$TEST_CLUSTER_IP" "test" || success=false
            fi
            
            if [ "$success" = false ]; then
              echo "⚠️ Some clusters not ready, but continuing..."
            fi
          else
            if [ -n "$CLUSTER_IP" ]; then
              test_k3s_api "$CLUSTER_IP" "${{ github.event.inputs.network || env.DEFAULT_NETWORK }}"
            fi
          fi
          
          echo "✅ Step 5 Complete: K3s API verification done!"

  # Step 6: Setup Kubeconfig → Configures cluster access
  setup-kubeconfig:
    needs: [terraform-deploy, verify-k3s-api]
    if: (github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy') || github.event_name == 'push'
    runs-on: self-hosted
    
    steps:
      - uses: actions/checkout@v4
      
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Setup Kubeconfig
        env:
          SSH_PRIVATE_KEY: ${{ secrets.SSH_PRIVATE_KEY }}
          DEV_CLUSTER_IP: ${{ needs.terraform-deploy.outputs.dev_cluster_ip }}
          TEST_CLUSTER_IP: ${{ needs.terraform-deploy.outputs.test_cluster_ip }}
          CLUSTER_IP: ${{ needs.terraform-deploy.outputs.cluster_ip }}
        run: |
          echo "⚙️ Step 5: Setting up kubeconfig for cluster access..."
          
          # Setup SSH key
          mkdir -p ~/.ssh
          echo "$SSH_PRIVATE_KEY" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          
          # Function to setup kubeconfig
          setup_kubeconfig() {
            local IP=$1
            local ENV=$2
            
            echo "Setting up kubeconfig for $ENV cluster..."
            
            # Get kubeconfig from cluster
            ssh -o StrictHostKeyChecking=no ubuntu@$IP "sudo cat /etc/rancher/k3s/k3s.yaml" > /tmp/kubeconfig-$ENV.yaml
            
            # Replace localhost with actual IP
            sed -i "s/127.0.0.1/$IP/g" /tmp/kubeconfig-$ENV.yaml
            
            # Store in Parameter Store
            aws ssm put-parameter \
              --name "/health-app/kubeconfig/$ENV" \
              --value "$(cat /tmp/kubeconfig-$ENV.yaml)" \
              --type "SecureString" \
              --overwrite
            
            echo "✅ Kubeconfig stored in Parameter Store for $ENV"
          }
          
          # Setup kubeconfig based on network tier
          if [ "${{ github.event.inputs.network || env.DEFAULT_NETWORK }}" == "lower" ]; then
            if [ -n "$DEV_CLUSTER_IP" ]; then
              setup_kubeconfig "$DEV_CLUSTER_IP" "dev"
            fi
            
            if [ -n "$TEST_CLUSTER_IP" ]; then
              setup_kubeconfig "$TEST_CLUSTER_IP" "test"
            fi
          else
            if [ -n "$CLUSTER_IP" ]; then
              setup_kubeconfig "$CLUSTER_IP" "${{ github.event.inputs.network }}"
            fi
          fi
          
          # Cleanup SSH key
          rm -f ~/.ssh/id_rsa
          
          echo "✅ Step 6 Complete: Kubeconfig setup done!"

  # Step 7: Validate Cluster Deployment → Final validation
  validate-cluster:
    needs: [terraform-deploy, setup-kubeconfig]
    if: (github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy') || github.event_name == 'push'
    runs-on: self-hosted
    
    steps:
      - uses: actions/checkout@v4
      
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Validate Cluster Deployment
        env:
          DEV_CLUSTER_IP: ${{ needs.terraform-deploy.outputs.dev_cluster_ip }}
          TEST_CLUSTER_IP: ${{ needs.terraform-deploy.outputs.test_cluster_ip }}
          CLUSTER_IP: ${{ needs.terraform-deploy.outputs.cluster_ip }}
        run: |
          echo "✅ Step 6: Validating cluster deployment..."
          
          # Function to validate cluster
          validate_cluster() {
            local ENV=$1
            
            echo "Validating $ENV cluster..."
            
            # Get kubeconfig from Parameter Store
            aws ssm get-parameter \
              --name "/health-app/kubeconfig/$ENV" \
              --with-decryption \
              --query 'Parameter.Value' \
              --output text > /tmp/kubeconfig-$ENV.yaml
            
            # Test kubectl connectivity
            if kubectl --kubeconfig=/tmp/kubeconfig-$ENV.yaml get nodes >/dev/null 2>&1; then
              echo "✅ $ENV cluster validation successful"
              kubectl --kubeconfig=/tmp/kubeconfig-$ENV.yaml get nodes
            else
              echo "❌ $ENV cluster validation failed"
              return 1
            fi
          }
          
          # Install kubectl
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/
          
          # Validate clusters based on network tier
          if [ "${{ github.event.inputs.network || env.DEFAULT_NETWORK }}" == "lower" ]; then
            success=true
            
            if [ -n "$DEV_CLUSTER_IP" ]; then
              validate_cluster "dev" || success=false
            fi
            
            if [ -n "$TEST_CLUSTER_IP" ]; then
              validate_cluster "test" || success=false
            fi
            
            if [ "$success" = false ]; then
              echo "❌ Some cluster validations failed"
              exit 1
            fi
          else
            if [ -n "$CLUSTER_IP" ]; then
              validate_cluster "${{ github.event.inputs.network }}"
            fi
          fi
          
          echo "✅ Step 7 Complete: Cluster validation done!"

  # Step 8: Setup K8s Secrets → Creates Kubernetes secrets
  setup-k8s-secrets:
    needs: [terraform-deploy, validate-cluster]
    if: (github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy') || github.event_name == 'push'
    runs-on: self-hosted
    
    steps:
      - uses: actions/checkout@v4
      
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Setup K8s Secrets
        env:
          DEV_CLUSTER_IP: ${{ needs.terraform-deploy.outputs.dev_cluster_ip }}
          TEST_CLUSTER_IP: ${{ needs.terraform-deploy.outputs.test_cluster_ip }}
          CLUSTER_IP: ${{ needs.terraform-deploy.outputs.cluster_ip }}
        run: |
          echo "🔐 Step 7: Setting up Kubernetes secrets..."
          
          # Function to setup secrets
          setup_secrets() {
            local ENV=$1
            
            echo "Setting up secrets for $ENV cluster..."
            
            # Get kubeconfig from Parameter Store
            aws ssm get-parameter \
              --name "/health-app/kubeconfig/$ENV" \
              --with-decryption \
              --query 'Parameter.Value' \
              --output text > /tmp/kubeconfig-$ENV.yaml
            
            # Create namespace if it doesn't exist
            kubectl --kubeconfig=/tmp/kubeconfig-$ENV.yaml create namespace $ENV || echo "Namespace $ENV already exists"
            
            # Create database secret
            kubectl --kubeconfig=/tmp/kubeconfig-$ENV.yaml create secret generic db-credentials \
              --from-literal=username=healthapp \
              --from-literal=password=healthapp123 \
              --from-literal=host=health-app-shared-db.cluster-cqtoaos7cxkp.ap-south-1.rds.amazonaws.com \
              --from-literal=port=3306 \
              --from-literal=database=healthappdb \
              --namespace=$ENV \
              --dry-run=client -o yaml | kubectl --kubeconfig=/tmp/kubeconfig-$ENV.yaml apply -f -
            
            echo "✅ Secrets created for $ENV cluster"
          }
          
          # Install kubectl
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/
          
          # Setup secrets based on network tier
          if [ "${{ github.event.inputs.network || env.DEFAULT_NETWORK }}" == "lower" ]; then
            if [ -n "$DEV_CLUSTER_IP" ]; then
              setup_secrets "dev"
            fi
            
            if [ -n "$TEST_CLUSTER_IP" ]; then
              setup_secrets "test"
            fi
          else
            if [ -n "$CLUSTER_IP" ]; then
              setup_secrets "${{ github.event.inputs.network }}"
            fi
          fi
          
          echo "✅ Step 8 Complete: K8s secrets setup done!"

  # Step 9: Configure Network Security → Additional security rules
  configure-network-security:
    needs: [terraform-deploy, setup-k8s-secrets]
    if: (github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy') || github.event_name == 'push'
    runs-on: self-hosted
    
    steps:
      - uses: actions/checkout@v4
      
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Configure Network Security
        run: |
          echo "🛡️ Step 8: Configuring additional network security..."
          
          # Run network security configuration script
          if [ -f "scripts/setup-cross-sg-references.sh" ]; then
            chmod +x scripts/setup-cross-sg-references.sh
            ./scripts/setup-cross-sg-references.sh ${{ github.event.inputs.network || env.DEFAULT_NETWORK }}
          else
            echo "Network security script not found, skipping..."
          fi
          
          echo "✅ Step 9 Complete: Network security configuration done!"

  # Step 10: Test Connectivity → End-to-end testing
  test-connectivity:
    needs: [terraform-deploy, configure-network-security]
    if: (github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy') || github.event_name == 'push'
    runs-on: self-hosted
    
    steps:
      - uses: actions/checkout@v4
      
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Test Connectivity
        env:
          DEV_CLUSTER_IP: ${{ needs.terraform-deploy.outputs.dev_cluster_ip }}
          TEST_CLUSTER_IP: ${{ needs.terraform-deploy.outputs.test_cluster_ip }}
          CLUSTER_IP: ${{ needs.terraform-deploy.outputs.cluster_ip }}
        run: |
          echo "🔍 Step 9: Testing end-to-end connectivity..."
          
          # Run connectivity tests
          if [ -f "scripts/test-network-connectivity.sh" ]; then
            chmod +x scripts/test-network-connectivity.sh
            ./scripts/test-network-connectivity.sh ${{ github.event.inputs.network || env.DEFAULT_NETWORK }}
          else
            echo "Connectivity test script not found, performing basic tests..."
            
            # Basic connectivity tests
            if [ "${{ github.event.inputs.network || env.DEFAULT_NETWORK }}" == "lower" ]; then
              if [ -n "$DEV_CLUSTER_IP" ]; then
                echo "Testing dev cluster connectivity..."
                timeout 10 curl -k -s "https://$DEV_CLUSTER_IP:6443/version" && echo "✅ Dev cluster API accessible" || echo "❌ Dev cluster API not accessible"
              fi
              
              if [ -n "$TEST_CLUSTER_IP" ]; then
                echo "Testing test cluster connectivity..."
                timeout 10 curl -k -s "https://$TEST_CLUSTER_IP:6443/version" && echo "✅ Test cluster API accessible" || echo "❌ Test cluster API not accessible"
              fi
            else
              if [ -n "$CLUSTER_IP" ]; then
                echo "Testing cluster connectivity..."
                timeout 10 curl -k -s "https://$CLUSTER_IP:6443/version" && echo "✅ Cluster API accessible" || echo "❌ Cluster API not accessible"
              fi
            fi
          fi
          
          echo "🎉 Step 10 Complete: Infrastructure deployment pipeline finished!"
          echo "📋 Summary: All 10 steps completed successfully"
          echo "   1. ✅ Terraform → Created instances"
          echo "   2. ✅ Configure Security Groups → Set up cross-SG references"
          echo "   3. ✅ Create GitHub Runner → Launched EC2 and registered as self-hosted runner"
          echo "   4. ✅ Install and Configure K3s → Installed K3s software using self-hosted runner"
          echo "   5. ✅ Verify K3s API Availability → Tested K3s response"
          echo "   6. ✅ Setup Kubeconfig → Configured cluster access"
          echo "   7. ✅ Validate Cluster Deployment → Final validation"
          echo "   8. ✅ Setup K8s Secrets → Created Kubernetes secrets"
          echo "   9. ✅ Configure Network Security → Additional security rules"
          echo "   10. ✅ Test Connectivity → End-to-end testing"