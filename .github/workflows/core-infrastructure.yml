name: Core Infrastructure

on:
  workflow_dispatch:
    inputs:
      action:
        description: 'Action'
        required: true
        type: choice
        options:
        - deploy
        - destroy
        - plan
        - redeploy
      environment:
        description: 'Network/Environment'
        required: true
        default: 'lower'
        type: choice
        options:
        - lower
        - higher
        - monitoring
        - all
        - cleanup-all
      confirm_destroy:
        description: 'Type "DESTROY" to confirm'
        required: false
        type: string
      restore_from_snapshot:
        description: 'Restore RDS from snapshot'
        required: false
        default: false
        type: boolean
      runner_type:
        description: 'Runner Type'
        required: false
        default: 'aws'
        type: choice
        options:
        - aws
        - github

env:
  AWS_REGION: ap-south-1
  TERRAFORM_VERSION: 1.6.0

jobs:
  infrastructure:
    runs-on: ${{ github.event.inputs.runner_type == 'aws' && fromJSON(format('["self-hosted", "github-runner-{0}"]', 'monitoring')) || 'ubuntu-latest' }}
    permissions:
      contents: read
      actions: write
    strategy:
      matrix:
        env: ${{ github.event.inputs.environment == 'all' && fromJson('["lower", "higher", "monitoring"]') || github.event.inputs.environment == 'cleanup-all' && fromJson('["cleanup"]') || fromJson(format('["{0}"]', github.event.inputs.environment)) }}
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: ${{ env.TERRAFORM_VERSION }}

    - name: Terraform Init
      working-directory: infra/two-network-setup
      run: |
        echo "üîç Backend Configuration:"
        echo "- Bucket: ${{ secrets.TF_STATE_BUCKET }}"
        echo "- Key: health-app-${{ matrix.env }}.tfstate"
        echo "- Region: $AWS_REGION"
        
        terraform init \
          -backend-config="bucket=${{ secrets.TF_STATE_BUCKET }}" \
          -backend-config="key=health-app-${{ matrix.env }}.tfstate" \
          -backend-config="region=$AWS_REGION"

    - name: Terraform Plan
      working-directory: infra/two-network-setup
      if: github.event.inputs.action == 'plan' || github.event.inputs.action == 'deploy'
      run: |
        echo "üìã Planning infrastructure changes..."
        terraform plan \
          -var="environment=${{ matrix.env == 'lower' && 'dev' || matrix.env == 'higher' && 'prod' || 'monitoring' }}" \
          -var="network_tier=${{ matrix.env }}" \
          -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
          -var="repo_pat=${{ secrets.REPO_PAT }}" \
          -var="repo_name=${{ secrets.REPO_NAME }}" \
          -var="restore_from_snapshot=${{ github.event.inputs.restore_from_snapshot }}" \
          -var="snapshot_identifier=${{ github.event.inputs.restore_from_snapshot == 'true' && 'healthapidb-snapshot' || 'null' }}" \
          -out=tfplan

    - name: Terraform Destroy (for redeploy)
      working-directory: infra/two-network-setup
      if: github.event.inputs.action == 'redeploy'
      run: |
        echo "üßπ Destroying existing resources first..."
        terraform destroy \
          -var="environment=${{ matrix.env == 'lower' && 'dev' || matrix.env == 'higher' && 'prod' || 'monitoring' }}" \
          -var="network_tier=${{ matrix.env }}" \
          -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
          -var="repo_pat=${{ secrets.REPO_PAT }}" \
          -var="repo_name=${{ secrets.REPO_NAME }}" \
          -var="restore_from_snapshot=false" \
          -var="snapshot_identifier=null" \
          -auto-approve || echo "Destroy completed with warnings"

    - name: Terraform Apply
      working-directory: infra/two-network-setup
      if: github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy'
      run: |
        echo "üöÄ Applying infrastructure changes..."
        terraform apply \
          -var="environment=${{ matrix.env == 'lower' && 'dev' || matrix.env == 'higher' && 'prod' || 'monitoring' }}" \
          -var="network_tier=${{ matrix.env }}" \
          -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
          -var="repo_pat=${{ secrets.REPO_PAT }}" \
          -var="repo_name=${{ secrets.REPO_NAME }}" \
          -var="restore_from_snapshot=${{ github.event.inputs.restore_from_snapshot }}" \
          -var="snapshot_identifier=${{ github.event.inputs.restore_from_snapshot == 'true' && 'healthapidb-snapshot' || 'null' }}" \
          -auto-approve
        
        echo "‚úÖ Infrastructure deployed successfully"



    - name: Terraform Destroy
      working-directory: infra/two-network-setup
      if: github.event.inputs.action == 'destroy'
      run: |
        echo "üßπ Destroying infrastructure..."
        
        if [ "${{ github.event.inputs.confirm_destroy }}" != "DESTROY" ]; then
          echo "‚ùå Destroy confirmation required - type 'DESTROY' to confirm"
          exit 1
        fi
        
        terraform destroy \
          -var="environment=${{ matrix.env == 'lower' && 'dev' || matrix.env == 'higher' && 'prod' || 'monitoring' }}" \
          -var="network_tier=${{ matrix.env }}" \
          -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
          -var="repo_pat=${{ secrets.REPO_PAT }}" \
          -var="repo_name=${{ secrets.REPO_NAME }}" \
          -var="restore_from_snapshot=false" \
          -var="snapshot_identifier=null" \
          -auto-approve
        
        echo "‚úÖ Infrastructure destroyed successfully"

    - name: Post-deployment Summary
      if: success() && (github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy')
      working-directory: infra/two-network-setup
      run: |
        echo "‚úÖ Infrastructure deployment completed successfully!"
        echo "üìã Summary for ${{ matrix.env }} network:"
        
        echo "üîç Terraform outputs:"
        terraform output 2>/dev/null || echo "No outputs available"
        
        echo "üöÄ Proceeding to cleanup and kubeconfig setup..."

  cleanup-orphaned:
    needs: infrastructure
    if: github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy'
    runs-on: ubuntu-latest
    strategy:
      matrix:
        env: ${{ github.event.inputs.environment == 'all' && fromJson('["lower", "higher", "monitoring"]') || fromJson(format('["{0}"]', github.event.inputs.environment)) }}
    
    steps:
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Cleanup Orphaned Resources
      run: |
        echo "üßπ Cleaning up orphaned resources for ${{ matrix.env }}..."
        
        # Get current managed resources from terraform state
        MANAGED_INSTANCES=$(aws ec2 describe-instances \
          --filters "Name=tag:ManagedBy,Values=terraform" \
                   "Name=tag:Environment,Values=${{ matrix.env == 'lower' && 'dev' || matrix.env == 'higher' && 'prod' || 'monitoring' }}" \
                   "Name=instance-state-name,Values=running" \
          --query 'Reservations[].Instances[].InstanceId' \
          --output text)
        
        echo "‚úÖ Managed instances: $MANAGED_INSTANCES"
        
        # Find ALL instances with same name pattern and keep only the newest
        echo "üîç Finding duplicate instances..."
        
        # Get all K3s instances (sorted by launch time, newest first)
        ALL_K3S=$(aws ec2 describe-instances \
          --filters "Name=tag:Name,Values=*k3s*" \
                   "Name=instance-state-name,Values=running" \
          --query 'Reservations[].Instances[].[InstanceId,LaunchTime]' \
          --output text | sort -k2 -r)
        
        # Get all runner instances (sorted by launch time, newest first)
        ALL_RUNNERS=$(aws ec2 describe-instances \
          --filters "Name=tag:Name,Values=*runner*" \
                   "Name=instance-state-name,Values=running" \
          --query 'Reservations[].Instances[].[InstanceId,LaunchTime]' \
          --output text | sort -k2 -r)
        
        # Keep only the newest K3s instance
        ORPHANED_K3S=$(echo "$ALL_K3S" | tail -n +2 | cut -f1)
        
        # Keep only the newest runner instance
        ORPHANED_RUNNERS=$(echo "$ALL_RUNNERS" | tail -n +2 | cut -f1)
        
        echo "All K3s instances: $ALL_K3S"
        echo "All runner instances: $ALL_RUNNERS"
        echo "K3s to terminate: $ORPHANED_K3S"
        echo "Runners to terminate: $ORPHANED_RUNNERS"
        
        # Terminate orphaned instances
        if [[ -n "$ORPHANED_K3S" ]]; then
          echo "üóëÔ∏è Terminating orphaned K3s instances: $ORPHANED_K3S"
          aws ec2 terminate-instances --instance-ids $ORPHANED_K3S
        fi
        
        if [[ -n "$ORPHANED_RUNNERS" ]]; then
          echo "üóëÔ∏è Terminating orphaned runner instances: $ORPHANED_RUNNERS"
          aws ec2 terminate-instances --instance-ids $ORPHANED_RUNNERS
        fi
        
        echo "‚úÖ Cleanup completed for ${{ matrix.env }}"

  validate-runner:
    needs: [infrastructure, cleanup-orphaned]
    if: github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy'
    runs-on: ubuntu-latest
    outputs:
      runner-ready: ${{ steps.check-runner.outputs.ready }}
    strategy:
      matrix:
        env: ${{ github.event.inputs.environment == 'all' && fromJson('["lower", "higher", "monitoring"]') || fromJson(format('["{0}"]', github.event.inputs.environment)) }}
    
    steps:
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Check Runner Status
      id: check-runner
      run: |
        echo "üîç Checking if GitHub runner is ready..."
        
        # Get runner instance based on environment
        RUNNER_NAME="health-app-runner-${{ matrix.env }}"
        echo "Looking for runner: $RUNNER_NAME"
        
        # Check if runner instance is running
        RUNNER_STATUS=$(aws ec2 describe-instances \
          --filters "Name=tag:Name,Values=$RUNNER_NAME" \
                   "Name=instance-state-name,Values=running" \
          --query 'Reservations[0].Instances[0].State.Name' \
          --output text 2>/dev/null || echo "None")
        
        if [[ "$RUNNER_STATUS" == "running" ]]; then
          echo "‚úÖ Runner instance is running"
          
          # Wait for runner to be online (max 5 minutes)
          echo "‚è≥ Waiting for runner to be online..."
          for i in {1..30}; do
            # Check if runner is online via GitHub API
            ENV_NAME="${{ matrix.env }}"
            RUNNER_ONLINE=$(curl -s -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" \
              "https://api.github.com/repos/${{ github.repository }}/actions/runners?per_page=100" \
              | jq -r --arg ENV_NAME "$ENV_NAME" '.runners[] | select(.name | test("^github-runner-" + $ENV_NAME + "-.*")) | .status' 2>/dev/null || echo "")
            
            echo "üîç Found runner status: $RUNNER_ONLINE"
            
            if [[ "$RUNNER_ONLINE" == "online" ]]; then
              echo "‚úÖ Runner is online and ready"
              echo "ready=true" >> $GITHUB_OUTPUT
              exit 0
            fi
            
            echo "‚è≥ Attempt $i/30 - Runner not online yet..."
            sleep 10
          done
          
          echo "‚ùå Runner instance running but not online after 5 minutes"
          echo "ready=false" >> $GITHUB_OUTPUT
        else
          echo "‚ùå Runner instance not running: $RUNNER_STATUS"
          echo "ready=false" >> $GITHUB_OUTPUT
        fi

  setup-kubeconfig:
    needs: [infrastructure, cleanup-orphaned]
    if: github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy'
    runs-on: ${{ github.event.inputs.environment == 'lower' && fromJSON('["self-hosted", "github-runner-lower"]') || github.event.inputs.environment == 'higher' && fromJSON('["self-hosted", "github-runner-higher"]') || github.event.inputs.environment == 'monitoring' && fromJSON('["self-hosted", "github-runner-monitoring"]') || fromJSON('["self-hosted", "github-runner-lower"]') }}
    strategy:
      matrix:
        env: ${{ github.event.inputs.environment == 'all' && fromJson('["lower", "higher", "monitoring"]') || fromJson(format('["{0}"]', github.event.inputs.environment)) }}
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: ${{ env.TERRAFORM_VERSION }}

    - name: Terraform Init (for outputs)
      working-directory: infra/two-network-setup
      run: |
        terraform init \
          -backend-config="bucket=${{ secrets.TF_STATE_BUCKET }}" \
          -backend-config="key=health-app-${{ matrix.env }}.tfstate" \
          -backend-config="region=$AWS_REGION"

    - name: Install kubectl and GitHub CLI
      run: |
        # Install kubectl
        curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
        chmod +x kubectl
        sudo mv kubectl /usr/local/bin/
        
        # Install GitHub CLI
        curl -fsSL https://cli.github.com/packages/githubcli-archive-keyring.gpg | sudo dd of=/usr/share/keyrings/githubcli-archive-keyring.gpg
        echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main" | sudo tee /etc/apt/sources.list.d/github-cli.list > /dev/null
        sudo apt update
        sudo apt install gh -y

    - name: Setup Kubeconfig and Upload to S3
      env:
        SSH_PRIVATE_KEY: ${{ secrets.SSH_PRIVATE_KEY }}
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        TF_STATE_BUCKET: ${{ secrets.TF_STATE_BUCKET }}
      run: |
        echo "üîß Setting up kubeconfig for ${{ matrix.env }} network"
        
        # Get cluster IP from terraform output
        cd infra/two-network-setup
        CLUSTER_IP=$(terraform output -raw k3s_public_ip 2>/dev/null || echo "")
        
        if [[ -z "$CLUSTER_IP" || "$CLUSTER_IP" == "null" ]]; then
          echo "‚ùå Could not get cluster IP from terraform output"
          exit 1
        fi
        
        echo "üéØ Cluster IP: $CLUSTER_IP"
        cd ../..
        
        # Setup SSH key
        echo "$SSH_PRIVATE_KEY" > /tmp/ssh_key
        chmod 600 /tmp/ssh_key
        
        # Wait for K3s to be ready
        echo "‚è≥ Waiting for K3s cluster to be ready..."
        for i in {1..30}; do
          if timeout 10 ssh -i /tmp/ssh_key -o StrictHostKeyChecking=no ubuntu@$CLUSTER_IP "sudo test -f /etc/rancher/k3s/k3s.yaml" 2>/dev/null; then
            echo "‚úÖ K3s cluster is ready"
            break
          fi
          echo "‚è≥ Attempt $i/30 - waiting for K3s..."
          sleep 10
        done
        
        # Download and fix kubeconfig
        echo "üì• Downloading kubeconfig from cluster..."
        ssh -i /tmp/ssh_key -o StrictHostKeyChecking=no ubuntu@$CLUSTER_IP "sudo cat /etc/rancher/k3s/k3s.yaml" > /tmp/k3s-config
        sed "s/127.0.0.1/$CLUSTER_IP/g" /tmp/k3s-config > /tmp/fixed-config
        
        # Upload to S3
        echo "‚òÅÔ∏è Uploading kubeconfig to S3..."
        S3_KEY="kubeconfig-${{ matrix.env }}.yaml"
        aws s3 cp /tmp/fixed-config s3://$TF_STATE_BUCKET/$S3_KEY
        echo "‚úÖ Kubeconfig uploaded to s3://$TF_STATE_BUCKET/$S3_KEY"
        
        # Test kubeconfig
        echo "üß™ Testing kubeconfig..."
        export KUBECONFIG=/tmp/fixed-config
        if timeout 30 kubectl get nodes --insecure-skip-tls-verify; then
          echo "‚úÖ Kubeconfig test successful"
        else
          echo "‚ö†Ô∏è Kubeconfig test failed, but uploaded to S3"
        fi
        
        # Create GitHub secret
        echo "üîê Creating GitHub secret..."
        SECRET_NAME=""
        case "${{ matrix.env }}" in
          "lower") SECRET_NAME="KUBECONFIG_DEV" ;;
          "higher") SECRET_NAME="KUBECONFIG_PROD" ;;
          "monitoring") SECRET_NAME="KUBECONFIG_MONITORING" ;;
        esac
        
        if [[ -n "$SECRET_NAME" ]]; then
          KUBECONFIG_B64=$(base64 -w 0 /tmp/fixed-config)
          echo "$KUBECONFIG_B64" | gh secret set $SECRET_NAME --repo $GITHUB_REPOSITORY
          echo "‚úÖ GitHub secret $SECRET_NAME created"
        fi
        
        # Cleanup
        rm -f /tmp/ssh_key /tmp/k3s-config /tmp/fixed-config
        
        echo "üéâ Kubeconfig setup completed!"
        echo "üìç S3 location: s3://$TF_STATE_BUCKET/$S3_KEY"
        echo "üîê GitHub secret: $SECRET_NAME"

