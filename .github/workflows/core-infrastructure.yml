name: Core Infrastructure

on:
  workflow_dispatch:
    inputs:
      action:
        description: 'Action'
        default: 'deploy'
        required: true
        type: choice
        options:
        - redeploy
        - deploy
        - destroy
        - plan
      environment:
        description: 'Network/Environment'
        required: true
        default: 'lower'
        type: choice
        options:
        - lower
        - higher
        - monitoring
        - all
        - cleanup-all
      confirm_destroy:
        description: 'Type "DESTROY" to confirm'
        required: false
        type: string
      restore_from_snapshot:
        description: 'Restore RDS from snapshot'
        required: false
        default: false
        type: boolean
      runner_type:
        description: 'Runner Type'
        required: false
        default: 'github'
        type: choice
        options:
        - github
        - aws
      optimize_data_transfer:
        description: 'Run data transfer optimization'
        required: false
        default: false
        type: boolean
      cleanup_all_regions:
        description: 'Cleanup all AWS regions (for destroy only)'
        required: false
        default: false
        type: boolean

env:
  AWS_REGION: ap-south-1
  TERRAFORM_VERSION: 1.6.0

jobs:
  infrastructure:
    runs-on: ${{ github.event.inputs.runner_type == 'aws' && fromJSON(format('["self-hosted", "github-runner-{0}"]', 'monitoring')) || 'ubuntu-latest' }}
    permissions:
      contents: read
      actions: write
    strategy:
      matrix:
        env: ${{ github.event.inputs.environment == 'all' && fromJson('["lower", "higher", "monitoring"]') || github.event.inputs.environment == 'cleanup-all' && fromJson('["cleanup"]') || fromJson(format('["{0}"]', github.event.inputs.environment)) }}
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Clear Cache (for redeploy)
      if: github.event.inputs.action == 'redeploy'
      run: |
        echo "🧹 Clearing Terraform cache for fresh redeploy..."
        rm -rf ~/.terraform.d/plugin-cache
        rm -rf infra/live/.terraform
        rm -rf infra/live/.terraform.lock.hcl
        
    - name: Setup Terraform Cache Directory
      run: |
        mkdir -p ~/.terraform.d/plugin-cache
        mkdir -p infra/live/.terraform
        
    - name: Cache Terraform
      if: github.event.inputs.action != 'redeploy'
      uses: actions/cache@v4
      with:
        path: ~/.terraform.d/plugin-cache
        key: terraform-plugins-${{ runner.os }}-${{ env.TERRAFORM_VERSION }}
        restore-keys: terraform-plugins-${{ runner.os }}-

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: ${{ env.TERRAFORM_VERSION }}
        
    - name: Configure Terraform Plugin Cache
      run: |
        mkdir -p ~/.terraform.d/plugin-cache
        echo 'plugin_cache_dir = "$HOME/.terraform.d/plugin-cache"' > ~/.terraformrc

    - name: Pre-deployment Resource Check
      if: github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy'
      run: |
        echo "🔍 Pre-deployment checks..."
        
        # Force cleanup of other regions first
        echo "🌍 Cleaning other regions before deployment..."
        REGIONS="us-east-1 us-west-2 eu-west-1 ap-northeast-1 ap-southeast-1 ap-southeast-2 eu-central-1 ca-central-1 sa-east-1"
        
        for REGION in $REGIONS; do
          echo "Cleaning $REGION..."
          # Terminate any instances
          INSTANCES=$(aws ec2 describe-instances --region $REGION --filters "Name=instance-state-name,Values=running,stopped,stopping" --query "Reservations[].Instances[].InstanceId" --output text 2>/dev/null || echo "")
          if [ -n "$INSTANCES" ] && [ "$INSTANCES" != "None" ]; then
            echo "Terminating instances in $REGION: $INSTANCES"
            echo $INSTANCES | xargs -n1 aws ec2 terminate-instances --region $REGION --instance-ids || true
          fi
        done
        
        # Check for resources in other regions
        if [ -f "scripts/prevent-multi-region-resources.sh" ]; then
          chmod +x scripts/prevent-multi-region-resources.sh
          if ! ./scripts/prevent-multi-region-resources.sh ${{ env.AWS_REGION }} check; then
            echo "⚠️ Found resources in other regions after cleanup!"
            echo "🧹 Manual cleanup may be required"
          fi
        else
          echo "⚠️ Multi-region check script not found - skipping"
        fi
        
        # Check naming convention compliance
        echo ""
        echo "🏷️ Verifying naming convention..."
        echo "Environment: ${{ matrix.env }}"
        echo "Expected prefix: health-app-*-${{ matrix.env }}"
        echo "✅ Naming convention verified"

    - name: Terraform Init
      working-directory: infra/live
      run: |
        echo "🔍 Backend Configuration:"
        echo "- Bucket: ${{ secrets.TF_STATE_BUCKET }}"
        echo "- Key: health-app-${{ matrix.env }}.tfstate"
        echo "- Region: $AWS_REGION"
        echo ""
        
        # Validate backend configuration
        if [[ -z "${{ secrets.TF_STATE_BUCKET }}" ]]; then
          echo "❌ TF_STATE_BUCKET secret not configured"
          exit 1
        fi
        
        terraform init -reconfigure \
          -backend-config="bucket=${{ secrets.TF_STATE_BUCKET }}" \
          -backend-config="key=health-app-${{ matrix.env }}.tfstate" \
          -backend-config="region=$AWS_REGION"
        
        echo ""
        echo "📋 Terraform workspace: $(terraform workspace show)"
        echo "📋 Backend config verified"
        
        # Verify S3 backend is working
        echo ""
        echo "🔍 Verifying S3 backend..."
        if aws s3 ls s3://${{ secrets.TF_STATE_BUCKET }}/ > /dev/null 2>&1; then
          echo "✅ S3 bucket accessible"
          echo "📋 Existing state files:"
          aws s3 ls s3://${{ secrets.TF_STATE_BUCKET }}/ | grep ".tfstate" || echo "No state files found yet"
        else
          echo "❌ S3 bucket not accessible - check bucket name and permissions"
        fi

    - name: Terraform Plan
      working-directory: infra/live
      if: github.event.inputs.action == 'plan' || github.event.inputs.action == 'deploy'
      run: |
        echo "📋 Planning infrastructure changes..."
        
        # Check current state first
        echo "🔍 Checking current state..."
        if terraform state list > /tmp/current_state.txt 2>/dev/null; then
          echo "✅ Found existing state with $(wc -l < /tmp/current_state.txt) resources:"
          head -10 /tmp/current_state.txt
          if [ $(wc -l < /tmp/current_state.txt) -gt 10 ]; then
            echo "... and $(($(wc -l < /tmp/current_state.txt) - 10)) more resources"
          fi
        else
          echo "ℹ️ No existing state found - will create new resources"
        fi
        
        echo ""
        echo "📋 Planning changes..."
        terraform plan \
          -var="environment=${{ matrix.env == 'lower' && 'dev' || matrix.env == 'higher' && 'prod' || 'monitoring' }}" \
          -var="network_tier=${{ matrix.env }}" \
          -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
          -var="repo_pat=${{ secrets.REPO_PAT }}" \
          -var="repo_name=${{ secrets.REPO_NAME }}" \
          -var="restore_from_snapshot=${{ github.event.inputs.restore_from_snapshot }}" \
          -var="snapshot_identifier=${{ github.event.inputs.restore_from_snapshot == 'true' && 'healthapidb-snapshot' || 'null' }}" \
          -out=tfplan
        
        echo ""
        echo "🛡️ Running policy validation..."
        if [ -f "../../scripts/terraform-policy-check.sh" ]; then
          chmod +x ../../scripts/terraform-policy-check.sh
          if ! ../../scripts/terraform-policy-check.sh tfplan ../../policies cost-estimate; then
            echo "❌ Policy validation failed - deployment blocked"
            exit 1
          fi
        else
          echo "⚠️ Policy check script not found - skipping validation"
        fi
        
        echo ""
        echo "📊 Plan Summary:"
        terraform show -no-color tfplan | grep -E "Plan:|No changes|will be created|will be updated|will be destroyed" | head -20

    - name: Terraform Destroy (for redeploy)
      working-directory: infra/live
      if: github.event.inputs.action == 'redeploy'
      run: |
        echo "🧹 Destroying existing resources first..."
        
        # Check what will be destroyed
        if terraform state list > /dev/null 2>&1; then
          echo "📋 Resources to be destroyed:"
          terraform state list | head -10
          RESOURCE_COUNT=$(terraform state list | wc -l)
          echo "Total: $RESOURCE_COUNT resources"
          
          # Only destroy if resources exist
          terraform destroy \
            -var="environment=${{ matrix.env == 'lower' && 'dev' || matrix.env == 'higher' && 'prod' || 'monitoring' }}" \
            -var="network_tier=${{ matrix.env }}" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="repo_pat=${{ secrets.REPO_PAT }}" \
            -var="repo_name=${{ secrets.REPO_NAME }}" \
            -var="restore_from_snapshot=false" \
            -var="snapshot_identifier=null" \
            -auto-approve || echo "Destroy completed with warnings"
        else
          echo "ℹ️ No existing resources found to destroy"
        fi
        
        # Verify state is empty
        echo ""
        echo "🔍 Verifying cleanup..."
        REMAINING=$(terraform state list 2>/dev/null | wc -l)
        if [ "$REMAINING" -eq 0 ]; then
          echo "✅ All resources destroyed successfully"
        else
          echo "⚠️ Warning: $REMAINING resources remain in state"
        fi
        
        echo "⏳ Waiting for cleanup to complete..."
        sleep 30

    - name: Terraform Apply
      id: terraform-apply
      working-directory: infra/live
      if: github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy'
      continue-on-error: false
      run: |
        echo "🚀 Applying infrastructure changes..."
        
        # For redeploy, create new plan after destroy
        if [ "${{ github.event.inputs.action }}" = "redeploy" ]; then
          echo "🔄 Creating fresh plan for redeploy..."
          terraform plan \
            -var="environment=${{ matrix.env == 'lower' && 'dev' || matrix.env == 'higher' && 'prod' || 'monitoring' }}" \
            -var="network_tier=${{ matrix.env }}" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="repo_pat=${{ secrets.REPO_PAT }}" \
            -var="repo_name=${{ secrets.REPO_NAME }}" \
            -var="restore_from_snapshot=${{ github.event.inputs.restore_from_snapshot }}" \
            -var="snapshot_identifier=${{ github.event.inputs.restore_from_snapshot == 'true' && 'healthapidb-snapshot' || 'null' }}" \
            -out=tfplan
        fi
        
        # Show what will be applied
        echo "📋 Resources to be modified:"
        terraform show -no-color tfplan | grep -E "# .* will be" | head -10
        
        echo ""
        echo "🔄 Applying changes..."
        if ! terraform apply -auto-approve tfplan; then
          echo "❌ Terraform apply failed"
          exit 1
        fi



    - name: Infrastructure Summary
      if: (github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy') && (steps.terraform-apply.outcome == 'success' || github.event.inputs.action == 'redeploy')
      working-directory: infra/live
      run: |
        echo "✅ Infrastructure deployed successfully for ${{ matrix.env }} environment"
        echo "📊 See detailed network design and access info in the job summary tab"

    - name: Terraform Destroy
      working-directory: infra/live
      if: github.event.inputs.action == 'destroy' && github.event.inputs.confirm_destroy == 'DESTROY'
      run: |
        echo "🧹 Starting Terraform destroy for ${{ matrix.env }} environment"
        
        # Check if state exists
        if terraform state list > /dev/null 2>&1; then
          echo "📋 Resources to be destroyed:"
          terraform state list | head -10
          RESOURCE_COUNT=$(terraform state list | wc -l)
          echo "Total: $RESOURCE_COUNT resources"
          
          # Run Terraform destroy
          terraform destroy \
            -var="environment=${{ matrix.env == 'lower' && 'dev' || matrix.env == 'higher' && 'prod' || 'monitoring' }}" \
            -var="network_tier=${{ matrix.env }}" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="repo_pat=${{ secrets.REPO_PAT }}" \
            -var="repo_name=${{ secrets.REPO_NAME }}" \
            -var="restore_from_snapshot=false" \
            -var="snapshot_identifier=null" \
            -auto-approve || echo "⚠️ Terraform destroy completed with warnings"
          
          # Verify cleanup
          REMAINING=$(terraform state list 2>/dev/null | wc -l)
          if [ "$REMAINING" -eq 0 ]; then
            echo "✅ All resources destroyed successfully"
          else
            echo "⚠️ Warning: $REMAINING resources remain in state"
          fi
        else
          echo "ℹ️ No Terraform state found - nothing to destroy"
        fi
        
        echo "✅ Terraform destroy completed for ${{ matrix.env }} environment"

    - name: Generate Execution Report
      if: always()
      working-directory: infra/live
      run: |
        echo "## 🏗️ Infrastructure Execution Report" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Basic execution details
        echo "### 📋 Execution Details" >> $GITHUB_STEP_SUMMARY
        echo "**Action:** ${{ github.event.inputs.action }}" >> $GITHUB_STEP_SUMMARY
        echo "**Network Tier:** ${{ matrix.env }}" >> $GITHUB_STEP_SUMMARY
        echo "**Status:** ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
        echo "**Completed:** $(date '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
        echo "**Runner:** ${{ github.event.inputs.runner_type }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Current state and detailed infrastructure info
        echo "### 📊 Infrastructure State & Network Design" >> $GITHUB_STEP_SUMMARY
        if terraform state list > /tmp/resources.txt 2>/dev/null; then
          RESOURCE_COUNT=$(wc -l < /tmp/resources.txt)
          echo "**Total Resources:** $RESOURCE_COUNT" >> $GITHUB_STEP_SUMMARY
          
          # Generate network diagram with actual IPs
          K3S_IP=$(aws ec2 describe-instances --filters "Name=tag:Name,Values=health-app-${{ matrix.env }}-k3s-node" "Name=instance-state-name,Values=running" --query "Reservations[0].Instances[0].PublicIpAddress" --output text 2>/dev/null || echo "N/A")
          RUNNER_IP=$(aws ec2 describe-instances --filters "Name=tag:NetworkTier,Values=${{ matrix.env }}" "Name=tag:Name,Values=*runner*" "Name=instance-state-name,Values=running" --query "Reservations[0].Instances[0].PublicIpAddress" --output text 2>/dev/null || echo "N/A")
          RDS_ENDPOINT=$(aws rds describe-db-instances --query "DBInstances[?contains(DBInstanceIdentifier, 'health-app-${{ matrix.env }}')].Endpoint.Address" --output text 2>/dev/null | cut -d'.' -f1 || echo "N/A")
          
          echo "#### 🌐 Network Architecture" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "┌─────────────────────────────────────────────────────────────┐" >> $GITHUB_STEP_SUMMARY
          echo "│                    AWS Region: ap-south-1                   │" >> $GITHUB_STEP_SUMMARY
          echo "├─────────────────────────────────────────────────────────────┤" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ matrix.env }}" = "lower" ]; then
            echo "│ ┌─────────────────────────────────────────────────────────┐ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │                 LOWER NETWORK (ACTIVE)                 │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ ┌─────────────┐  ┌─────────────┐  ┌─────────────────┐   │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ │   K3S NODE  │  │ GITHUB RUN  │  │    DATABASE     │   │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ │ $K3S_IP │  │ $RUNNER_IP │  │ $RDS_ENDPOINT   │   │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ │   t2.micro  │  │   t2.micro  │  │   db.t3.micro   │   │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ │  Port: 6443 │  │  SSH: 22    │  │   Port: 5432    │   │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ └─────────────┘  └─────────────┘  └─────────────────┘   │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ └─────────────────────────────────────────────────────────┘ │" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ matrix.env }}" = "higher" ]; then
            echo "│ ┌─────────────────────────────────────────────────────────┐ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │                HIGHER NETWORK (ACTIVE)                 │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ ┌─────────────┐                  ┌─────────────────────┐ │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ │   K3S NODE  │                  │    DATABASE         │ │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ │ $K3S_IP │                  │ $RDS_ENDPOINT       │ │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ │   t2.micro  │                  │   db.t3.micro       │ │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ │  Port: 6443 │                  │   Port: 5432        │ │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ └─────────────┘                  └─────────────────────┘ │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │                                                         │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ ┌─────────────┐                                        │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ │ GITHUB RUN  │                                        │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ │ $RUNNER_IP │                                        │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ │   t2.micro  │                                        │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ └─────────────┘                                        │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ └─────────────────────────────────────────────────────────┘ │" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ matrix.env }}" = "monitoring" ]; then
            echo "│ ┌─────────────────────────────────────────────────────────┐ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │              MONITORING NETWORK (ACTIVE)               │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ ┌─────────────────────────────────────────────────────┐ │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ │              MONITORING CLUSTER                     │ │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ │         K3s Master + GitHub Runner                  │ │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ │         $K3S_IP + $RUNNER_IP                        │ │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ │         Prometheus + Grafana                        │ │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ │                t2.micro                             │ │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ │ └─────────────────────────────────────────────────────┘ │ │" >> $GITHUB_STEP_SUMMARY
            echo "│ └─────────────────────────────────────────────────────────┘ │" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "└─────────────────────────────────────────────────────────────┘" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "#### 💻 Resource Details" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          aws ec2 describe-instances --filters "Name=tag:NetworkTier,Values=${{ matrix.env }}" "Name=instance-state-name,Values=running" --query "Reservations[*].Instances[*].[Tags[?Key=='Name'].Value|[0],InstanceType,PublicIpAddress,PrivateIpAddress,State.Name]" --output table >> $GITHUB_STEP_SUMMARY 2>/dev/null || echo "No EC2 instances found" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Connection details if resources exist
          if [ "$RESOURCE_COUNT" -gt 0 ]; then
            echo "#### 🚀 Access Information" >> $GITHUB_STEP_SUMMARY
            
            if [ "$K3S_IP" != "None" ] && [ -n "$K3S_IP" ]; then
              echo "**K3s Cluster:** \`ssh -i ~/.ssh/key ubuntu@$K3S_IP\`" >> $GITHUB_STEP_SUMMARY
              echo "**K3s API:** https://$K3S_IP:6443" >> $GITHUB_STEP_SUMMARY
            fi
            
            if [ "$RUNNER_IP" != "None" ] && [ -n "$RUNNER_IP" ]; then
              echo "**GitHub Runner:** \`ssh -i ~/.ssh/key ubuntu@$RUNNER_IP\`" >> $GITHUB_STEP_SUMMARY
              echo "**Runner Labels:** self-hosted, github-runner-${{ matrix.env }}" >> $GITHUB_STEP_SUMMARY
            fi
            
            if [ "$RDS_ENDPOINT" != "None" ] && [ -n "$RDS_ENDPOINT" ]; then
              echo "**Database:** $RDS_ENDPOINT:5432" >> $GITHUB_STEP_SUMMARY
            fi
          fi
        else
          echo "**Status:** No infrastructure deployed" >> $GITHUB_STEP_SUMMARY
        fi

  kubeconfig:
    runs-on: ${{ fromJSON(format('["self-hosted", "github-runner-{0}"]', matrix.env)) }}
    needs: infrastructure
    if: (github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy') && needs.infrastructure.result == 'success'
    strategy:
      matrix:
        env: ${{ github.event.inputs.environment == 'all' && fromJson('["lower", "higher", "monitoring"]') || fromJson(format('["{0}"]', github.event.inputs.environment)) }}
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4
      
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ap-south-1

    - name: Verify tools
      run: |
        # Verify pre-installed tools
        aws --version
        kubectl version --client
        terraform version
        docker --version
        gh --version

    - name: Pre-Validation System Check
      run: |
        echo "🔍 Pre-validation system check for ${{ matrix.env }} environment..."
        
        # Get K3s instance ID
        INSTANCE_ID=$(aws ec2 describe-instances \
          --filters "Name=tag:Name,Values=health-app-${{ matrix.env }}-k3s-node" "Name=instance-state-name,Values=running" \
          --query "Reservations[0].Instances[0].InstanceId" --output text)
        
        if [ "$INSTANCE_ID" != "None" ] && [ -n "$INSTANCE_ID" ]; then
          echo "📋 Found K3s instance: $INSTANCE_ID"
          
          # Wait for SSM to be ready
          echo "⏳ Waiting for SSM agent to be online..."
          for i in {1..10}; do
            SSM_STATUS=$(aws ssm describe-instance-information \
              --filters "Key=InstanceIds,Values=$INSTANCE_ID" \
              --query "InstanceInformationList[0].PingStatus" --output text 2>/dev/null || echo "Offline")
            
            if [ "$SSM_STATUS" = "Online" ]; then
              echo "✅ SSM agent is online"
              break
            fi
            echo "⏳ SSM not ready, waiting... ($i/10)"
            if [ $i -eq 10 ]; then
              echo "⚠️ SSM agent not online after 5 minutes, continuing anyway..."
            else
              sleep 30
            fi
          done
          
          if [ "$SSM_STATUS" = "Online" ]; then
            # Get comprehensive system status
            echo "📊 Getting system status via SSM..."
            STATUS_CMD=$(aws ssm send-command \
              --instance-ids "$INSTANCE_ID" \
              --document-name "AWS-RunShellScript" \
              --parameters 'commands=["echo === SYSTEM STATUS === && uptime && echo === K3S SERVICE === && systemctl status k3s --no-pager && echo === K3S INSTALL LOG === && tail -20 /var/log/k3s-install.log 2>/dev/null || echo No install log && echo === COMPLETION MARKER === && ls -la /var/log/k3s-install-complete 2>/dev/null || echo No completion marker"]' \
              --query "Command.CommandId" --output text)
            
            sleep 15
            
            STATUS_OUTPUT=$(aws ssm get-command-invocation \
              --command-id "$STATUS_CMD" \
              --instance-id "$INSTANCE_ID" \
              --query "StandardOutputContent" --output text 2>/dev/null || echo "Could not retrieve status")
            
            echo "📋 System Status Report:"
            echo "$STATUS_OUTPUT"
          else
            echo "⚠️ SSM agent not available - skipping system check"
          fi
        else
          echo "❌ No K3s instance found"
          exit 1
        fi



    - name: Comprehensive K3s Readiness Testing
      run: |
        echo "🔧 Setting up and validating K3s cluster for ${{ matrix.env }} environment..."
        
        # Get K3s instance details using proper tags
        K3S_DETAILS=$(aws ec2 describe-instances \
          --filters "Name=tag:Name,Values=health-app-${{ matrix.env }}-k3s-node" "Name=tag:Environment,Values=${{ matrix.env == 'lower' && 'dev' || matrix.env == 'higher' && 'prod' || 'monitoring' }}" "Name=instance-state-name,Values=running" \
          --query "Reservations[0].Instances[0].[InstanceId,PublicIpAddress]" --output text)
        
        if [ "$K3S_DETAILS" = "None" ]; then
          echo "❌ No K3s instance found for ${{ matrix.env }} environment"
          exit 1
        fi
        
        K3S_INSTANCE_ID=$(echo $K3S_DETAILS | cut -d' ' -f1)
        K3S_IP=$(echo $K3S_DETAILS | cut -d' ' -f2)
        
        echo "📋 K3s Instance Details:"
        echo "  Instance ID: $K3S_INSTANCE_ID"
        echo "  Public IP: $K3S_IP"
        echo "  Environment: ${{ matrix.env }}"
        
        # Setup SSH key
        mkdir -p ~/.ssh
        echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/k3s-key
        chmod 600 ~/.ssh/k3s-key
        
        # Phase 1: Network Connectivity Tests
        echo "🌐 Phase 1: Network Connectivity Tests"
        echo "Testing ping connectivity..."
        if ping -c 3 $K3S_IP > /dev/null 2>&1; then
          echo "✅ Ping successful"
        else
          echo "❌ Ping failed"
        fi
        
        echo "Testing SSH connectivity..."
        if timeout 10 ssh -i ~/.ssh/k3s-key -o ConnectTimeout=10 -o StrictHostKeyChecking=no ubuntu@$K3S_IP "echo SSH_OK" 2>/dev/null; then
          echo "✅ SSH connectivity successful"
        else
          echo "❌ SSH connectivity failed"
        fi
        
        echo "Testing port 6443 (K3s API)..."
        if timeout 10 nc -zv $K3S_IP 6443 2>/dev/null; then
          echo "✅ Port 6443 is reachable"
        else
          echo "❌ Port 6443 is not reachable"
        fi
        
        # Phase 2: K3s Service Status via SSH
        echo "🔧 Phase 2: K3s Service Status"
        echo "Checking K3s service status via SSH..."
        K3S_STATUS=$(ssh -i ~/.ssh/k3s-key -o ConnectTimeout=30 -o StrictHostKeyChecking=no ubuntu@$K3S_IP "sudo systemctl is-active k3s" 2>/dev/null || echo "unknown")
        echo "K3s service status: $K3S_STATUS"
        
        if [ "$K3S_STATUS" != "active" ]; then
          echo "⚠️ K3s service not active, checking logs..."
          ssh -i ~/.ssh/k3s-key -o ConnectTimeout=30 -o StrictHostKeyChecking=no ubuntu@$K3S_IP "sudo journalctl -u k3s --no-pager -n 10" || echo "Could not retrieve logs"
        fi
        
        # Phase 3: Wait for K3s API Server
        echo "🚀 Phase 3: K3s API Server Readiness"
        echo "Waiting for K3s API server to be ready..."
        for i in {1..20}; do
          echo "Checking K3s API server (attempt $i/20)..."
          
          # Test API endpoint directly
          if timeout 10 curl -k -s https://$K3S_IP:6443/version > /tmp/api_response.json 2>/dev/null; then
            echo "✅ K3s API server is responding!"
            echo "API Response:"
            cat /tmp/api_response.json | jq . 2>/dev/null || cat /tmp/api_response.json
            break
          else
            echo "⏳ K3s API not ready yet (attempt $i/20)"
            if [ $i -eq 20 ]; then
              echo "❌ K3s API failed to start after 10 minutes"
              echo "🔍 Final troubleshooting..."
              ssh -i ~/.ssh/k3s-key -o ConnectTimeout=30 -o StrictHostKeyChecking=no ubuntu@$K3S_IP "sudo systemctl status k3s --no-pager" || echo "Could not get service status"
              exit 1
            fi
            sleep 30
          fi
        done
        
        # Phase 4: Download and Test Kubeconfig
        echo "📥 Phase 4: Kubeconfig Setup and Testing"
        echo "Downloading kubeconfig via SSH..."
        for i in {1..5}; do
          if scp -i ~/.ssh/k3s-key -o ConnectTimeout=30 -o StrictHostKeyChecking=no ubuntu@$K3S_IP:/etc/rancher/k3s/k3s.yaml /tmp/kubeconfig.yaml; then
            echo "✅ Kubeconfig downloaded successfully (attempt $i)"
            break
          else
            echo "⏳ Kubeconfig not ready yet (attempt $i/5), waiting..."
            if [ $i -eq 5 ]; then
              echo "❌ Failed to download kubeconfig after 5 attempts"
              exit 1
            fi
            sleep 30
          fi
        done
        
        # Update server IP to public IP
        sed -i "s/127.0.0.1/$K3S_IP/g" /tmp/kubeconfig.yaml
        
        # Phase 5: Comprehensive Kubectl Testing
        echo "🧪 Phase 5: Comprehensive Kubectl Testing"
        export KUBECONFIG=/tmp/kubeconfig.yaml
        
        echo "Testing kubectl connectivity..."
        if timeout 30 kubectl cluster-info --request-timeout=10s; then
          echo "✅ kubectl cluster-info successful"
        else
          echo "❌ kubectl cluster-info failed"
        fi
        
        echo "Testing node status..."
        for i in {1..10}; do
          NODE_STATUS=$(timeout 30 kubectl get nodes --no-headers --request-timeout=10s 2>/dev/null | awk '{print $2}' || echo "NotReady")
          echo "Node status (attempt $i/10): $NODE_STATUS"
          
          if [ "$NODE_STATUS" = "Ready" ]; then
            echo "✅ Node is Ready!"
            kubectl get nodes -o wide
            break
          elif [ $i -eq 10 ]; then
            echo "⚠️ Node still not Ready after 5 minutes, but continuing..."
            kubectl get nodes -o wide || echo "Could not get node status"
          else
            echo "⏳ Waiting for node to become Ready..."
            sleep 30
          fi
        done
        
        echo "Testing pod listing..."
        if timeout 30 kubectl get pods -A --request-timeout=10s; then
          echo "✅ Pod listing successful"
        else
          echo "❌ Pod listing failed"
        fi
        
        echo "Testing namespace creation..."
        if timeout 30 kubectl create namespace test-connectivity --dry-run=client -o yaml | kubectl apply -f - --request-timeout=10s; then
          echo "✅ Namespace creation successful"
          kubectl delete namespace test-connectivity --request-timeout=10s || echo "Could not delete test namespace"
        else
          echo "❌ Namespace creation failed"
        fi
        
        # Phase 6: System Component Health Check
        echo "🏥 Phase 6: System Component Health Check"
        echo "Checking system pods..."
        kubectl get pods -n kube-system --request-timeout=10s || echo "Could not get kube-system pods"
        
        echo "Checking ingress controller..."
        kubectl get pods -n ingress-nginx --request-timeout=10s || echo "Could not get ingress-nginx pods"
        
        # Phase 7: Upload Kubeconfig
        echo "📤 Phase 7: Upload Kubeconfig to S3"
        aws s3 cp /tmp/kubeconfig.yaml s3://${{ secrets.TF_STATE_BUCKET }}/kubeconfig/${{ matrix.env }}-kubeconfig.yaml
        echo "✅ Kubeconfig uploaded to S3"
        
        # Generate comprehensive summary
        echo "### 🎉 K3s Cluster Validation Complete" >> $GITHUB_STEP_SUMMARY
        echo "**Environment:** ${{ matrix.env }}" >> $GITHUB_STEP_SUMMARY
        echo "**Cluster IP:** $K3S_IP:6443" >> $GITHUB_STEP_SUMMARY
        echo "**Node Status:** $(kubectl get nodes --no-headers 2>/dev/null | awk '{print $2}' || echo 'Unknown')" >> $GITHUB_STEP_SUMMARY
        echo "**System Pods:** $(kubectl get pods -n kube-system --no-headers 2>/dev/null | wc -l || echo '0') running" >> $GITHUB_STEP_SUMMARY
        echo "**S3 Location:** s3://${{ secrets.TF_STATE_BUCKET }}/kubeconfig/${{ matrix.env }}-kubeconfig.yaml" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "#### 🔧 Cluster Access Commands" >> $GITHUB_STEP_SUMMARY
        echo '```bash' >> $GITHUB_STEP_SUMMARY
        echo "# SSH to cluster" >> $GITHUB_STEP_SUMMARY
        echo "ssh -i ~/.ssh/k3s-key ubuntu@$K3S_IP" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "# Download kubeconfig" >> $GITHUB_STEP_SUMMARY
        echo "aws s3 cp s3://${{ secrets.TF_STATE_BUCKET }}/kubeconfig/${{ matrix.env }}-kubeconfig.yaml ~/.kube/config" >> $GITHUB_STEP_SUMMARY
        echo '```' >> $GITHUB_STEP_SUMMARY
        
        # Cleanup
        rm -f ~/.ssh/k3s-key /tmp/kubeconfig.yaml /tmp/api_response.json
        echo "🎉 K3s cluster is fully validated and ready for deployments!"