name: Core Infrastructure

on:
  workflow_dispatch:
    inputs:
      action:
        description: 'Action'
        required: true
        type: choice
        options:
        - deploy
        - destroy
        - plan
        - redeploy
      environment:
        description: 'Network/Environment'
        required: true
        default: 'lower'
        type: choice
        options:
        - lower
        - higher
        - monitoring
        - all
        - cleanup-all
      confirm_destroy:
        description: 'Type "DESTROY" to confirm'
        required: false
        type: string
      restore_from_snapshot:
        description: 'Restore RDS from snapshot'
        required: false
        default: false
        type: boolean
      runner_type:
        description: 'Runner Type'
        required: false
        default: 'aws'
        type: choice
        options:
        - aws
        - github
      optimize_data_transfer:
        description: 'Run data transfer optimization'
        required: false
        default: false
        type: boolean
      cleanup_all_regions:
        description: 'Cleanup all AWS regions (for destroy only)'
        required: false
        default: false
        type: boolean

env:
  AWS_REGION: ap-south-1
  TERRAFORM_VERSION: 1.6.0

jobs:
  infrastructure:
    runs-on: ${{ github.event.inputs.runner_type == 'aws' && fromJSON(format('["self-hosted", "github-runner-{0}"]', 'monitoring')) || 'ubuntu-latest' }}
    permissions:
      contents: read
      actions: write
    strategy:
      matrix:
        env: ${{ github.event.inputs.environment == 'all' && fromJson('["lower", "higher", "monitoring"]') || github.event.inputs.environment == 'cleanup-all' && fromJson('["cleanup"]') || fromJson(format('["{0}"]', github.event.inputs.environment)) }}
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: ${{ env.TERRAFORM_VERSION }}

    - name: Pre-deployment Resource Check
      if: github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy'
      run: |
        echo "üîç Pre-deployment checks..."
        
        # Force cleanup of other regions first
        echo "üåç Cleaning other regions before deployment..."
        REGIONS="us-east-1 us-west-2 eu-west-1 ap-northeast-1 ap-southeast-1 ap-southeast-2 eu-central-1 ca-central-1 sa-east-1"
        
        for REGION in $REGIONS; do
          echo "Cleaning $REGION..."
          # Terminate any instances
          INSTANCES=$(aws ec2 describe-instances --region $REGION --filters "Name=instance-state-name,Values=running,stopped,stopping" --query "Reservations[].Instances[].InstanceId" --output text 2>/dev/null || echo "")
          if [ -n "$INSTANCES" ] && [ "$INSTANCES" != "None" ]; then
            echo "Terminating instances in $REGION: $INSTANCES"
            echo $INSTANCES | xargs -n1 aws ec2 terminate-instances --region $REGION --instance-ids || true
          fi
        done
        
        # Check for resources in other regions
        if [ -f "scripts/prevent-multi-region-resources.sh" ]; then
          chmod +x scripts/prevent-multi-region-resources.sh
          if ! ./scripts/prevent-multi-region-resources.sh ${{ env.AWS_REGION }} check; then
            echo "‚ö†Ô∏è Found resources in other regions after cleanup!"
            echo "üßπ Manual cleanup may be required"
          fi
        else
          echo "‚ö†Ô∏è Multi-region check script not found - skipping"
        fi
        
        # Check naming convention compliance
        echo ""
        echo "üè∑Ô∏è Verifying naming convention..."
        echo "Environment: ${{ matrix.env }}"
        echo "Expected prefix: health-app-*-${{ matrix.env }}"
        echo "‚úÖ Naming convention verified"

    - name: Terraform Init
      working-directory: infra/two-network-setup
      run: |
        echo "üîç Backend Configuration:"
        echo "- Bucket: ${{ secrets.TF_STATE_BUCKET }}"
        echo "- Key: health-app-${{ matrix.env }}.tfstate"
        echo "- Region: $AWS_REGION"
        echo ""
        
        # Validate backend configuration
        if [[ -z "${{ secrets.TF_STATE_BUCKET }}" ]]; then
          echo "‚ùå TF_STATE_BUCKET secret not configured"
          exit 1
        fi
        
        terraform init \
          -backend-config="bucket=${{ secrets.TF_STATE_BUCKET }}" \
          -backend-config="key=health-app-${{ matrix.env }}.tfstate" \
          -backend-config="region=$AWS_REGION"
        
        echo ""
        echo "üìã Terraform workspace: $(terraform workspace show)"
        echo "üìã Backend config verified"
        
        # Verify S3 backend is working
        echo ""
        echo "üîç Verifying S3 backend..."
        if aws s3 ls s3://${{ secrets.TF_STATE_BUCKET }}/ > /dev/null 2>&1; then
          echo "‚úÖ S3 bucket accessible"
          echo "üìã Existing state files:"
          aws s3 ls s3://${{ secrets.TF_STATE_BUCKET }}/ | grep ".tfstate" || echo "No state files found yet"
        else
          echo "‚ùå S3 bucket not accessible - check bucket name and permissions"
        fi



    - name: Terraform Plan
      working-directory: infra/two-network-setup
      if: github.event.inputs.action == 'plan' || github.event.inputs.action == 'deploy'
      run: |
        echo "üìã Planning infrastructure changes..."
        
        # Check current state first
        echo "üîç Checking current state..."
        if terraform state list > /tmp/current_state.txt 2>/dev/null; then
          echo "‚úÖ Found existing state with $(wc -l < /tmp/current_state.txt) resources:"
          head -10 /tmp/current_state.txt
          if [ $(wc -l < /tmp/current_state.txt) -gt 10 ]; then
            echo "... and $(($(wc -l < /tmp/current_state.txt) - 10)) more resources"
          fi
        else
          echo "‚ÑπÔ∏è No existing state found - will create new resources"
        fi
        
        echo ""
        echo "üìã Planning changes..."
        terraform plan \
          -var="environment=${{ matrix.env == 'lower' && 'dev' || matrix.env == 'higher' && 'prod' || 'monitoring' }}" \
          -var="network_tier=${{ matrix.env }}" \
          -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
          -var="repo_pat=${{ secrets.REPO_PAT }}" \
          -var="repo_name=${{ secrets.REPO_NAME }}" \
          -var="restore_from_snapshot=${{ github.event.inputs.restore_from_snapshot }}" \
          -var="snapshot_identifier=${{ github.event.inputs.restore_from_snapshot == 'true' && 'healthapidb-snapshot' || 'null' }}" \
          -out=tfplan
        
        echo ""
        echo "üõ°Ô∏è Running policy validation..."
        if [ -f "../../scripts/terraform-policy-check.sh" ]; then
          chmod +x ../../scripts/terraform-policy-check.sh
          if ! ../../scripts/terraform-policy-check.sh tfplan ../../policies cost-estimate; then
            echo "‚ùå Policy validation failed - deployment blocked"
            exit 1
          fi
        else
          echo "‚ö†Ô∏è Policy check script not found - skipping validation"
        fi
        
        echo ""
        echo "üìä Plan Summary:"
        terraform show -no-color tfplan | grep -E "Plan:|No changes|will be created|will be updated|will be destroyed" | head -20

    - name: Terraform Destroy (for redeploy)
      working-directory: infra/two-network-setup
      if: github.event.inputs.action == 'redeploy'
      run: |
        echo "üßπ Destroying existing resources first..."
        
        # Check what will be destroyed
        if terraform state list > /dev/null 2>&1; then
          echo "üìã Resources to be destroyed:"
          terraform state list | head -10
          RESOURCE_COUNT=$(terraform state list | wc -l)
          echo "Total: $RESOURCE_COUNT resources"
          
          # Only destroy if resources exist
          terraform destroy \
            -var="environment=${{ matrix.env == 'lower' && 'dev' || matrix.env == 'higher' && 'prod' || 'monitoring' }}" \
            -var="network_tier=${{ matrix.env }}" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="repo_pat=${{ secrets.REPO_PAT }}" \
            -var="repo_name=${{ secrets.REPO_NAME }}" \
            -var="restore_from_snapshot=false" \
            -var="snapshot_identifier=null" \
            -auto-approve || echo "Destroy completed with warnings"
        else
          echo "‚ÑπÔ∏è No existing resources found to destroy"
        fi
        
        # Verify state is empty
        echo ""
        echo "üîç Verifying cleanup..."
        REMAINING=$(terraform state list 2>/dev/null | wc -l)
        if [ "$REMAINING" -eq 0 ]; then
          echo "‚úÖ All resources destroyed successfully"
        else
          echo "‚ö†Ô∏è $REMAINING resources still exist - may need manual cleanup"
        fi

    - name: Terraform Apply
      working-directory: infra/two-network-setup
      if: github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy'
      run: |
        echo "üöÄ Applying infrastructure changes..."
        
        terraform apply \
          -var="environment=${{ matrix.env == 'lower' && 'dev' || matrix.env == 'higher' && 'prod' || 'monitoring' }}" \
          -var="network_tier=${{ matrix.env }}" \
          -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
          -var="repo_pat=${{ secrets.REPO_PAT }}" \
          -var="repo_name=${{ secrets.REPO_NAME }}" \
          -var="restore_from_snapshot=${{ github.event.inputs.restore_from_snapshot }}" \
          -var="snapshot_identifier=${{ github.event.inputs.restore_from_snapshot == 'true' && 'healthapidb-snapshot' || 'null' }}" \
          -auto-approve
        
        echo ""
        echo "üìã Deployment Summary:"
        terraform output -json > /tmp/tf_output.json
        
        # Show key outputs
        if [ -f "/tmp/tf_output.json" ]; then
          echo "‚úÖ Infrastructure deployed successfully"
          
          # Extract cluster IP if available
          if jq -e '.k8s_master_public_ip.value' /tmp/tf_output.json > /dev/null 2>&1; then
            CLUSTER_IP=$(jq -r '.k8s_master_public_ip.value' /tmp/tf_output.json)
            echo "üéØ K3s Cluster IP: $CLUSTER_IP"
            echo "CLUSTER_IP=$CLUSTER_IP" >> $GITHUB_ENV
          fi
          
          # Extract runner IP if available
          if jq -e '.github_runner_public_ip.value' /tmp/tf_output.json > /dev/null 2>&1; then
            RUNNER_IP=$(jq -r '.github_runner_public_ip.value' /tmp/tf_output.json)
            echo "ü§ñ GitHub Runner IP: $RUNNER_IP"
          fi
        else
          echo "‚ö†Ô∏è No terraform output found"
        fi

    - name: Wait for K3s Cluster Ready
      if: (github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy') && env.CLUSTER_IP != ''
      run: |
        echo "‚è≥ Waiting for K3s cluster to be ready..."
        CLUSTER_IP="${{ env.CLUSTER_IP }}"
        
        if [ -z "$CLUSTER_IP" ]; then
          echo "‚ùå No cluster IP found - skipping cluster setup"
          exit 0
        fi
        
        echo "üéØ Cluster IP: $CLUSTER_IP"
        
        # Wait for SSH to be ready
        echo "üîç Waiting for SSH access..."
        for i in {1..30}; do
          if ssh -i <(echo "${{ secrets.SSH_PRIVATE_KEY }}") -o StrictHostKeyChecking=no -o ConnectTimeout=10 ubuntu@$CLUSTER_IP "echo 'SSH ready'" 2>/dev/null; then
            echo "‚úÖ SSH connection established"
            break
          fi
          echo "‚è≥ Attempt $i/30 - waiting for SSH..."
          sleep 10
        done
        
        # Force K3s certificate regeneration to fix TLS issues
        echo "üîß Regenerating K3s certificates..."
        ssh -i <(echo "${{ secrets.SSH_PRIVATE_KEY }}") -o StrictHostKeyChecking=no ubuntu@$CLUSTER_IP << 'EOF'
          # Stop K3s service
          sudo systemctl stop k3s || true
          
          # Remove old certificates
          sudo rm -rf /var/lib/rancher/k3s/server/tls/ || true
          
          # Restart K3s with fresh certificates
          sudo systemctl start k3s
          
          # Wait for K3s to be ready
          echo "‚è≥ Waiting for K3s to restart..."
          for i in {1..60}; do
            if sudo k3s kubectl get nodes --no-headers 2>/dev/null | grep -q Ready; then
              echo "‚úÖ K3s cluster is ready"
              break
            fi
            echo "‚è≥ Attempt $i/60 - waiting for K3s..."
            sleep 5
          done
          
          # Verify cluster status
          echo "üìã Cluster status:"
          sudo k3s kubectl get nodes
          sudo k3s kubectl cluster-info
          EOF
        
        echo "‚úÖ K3s cluster setup completed"

    - name: Setup Kubeconfig
      if: (github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy') && env.CLUSTER_IP != ''
      run: |
        echo "üîß Setting up kubeconfig..."
        CLUSTER_IP="${{ env.CLUSTER_IP }}"
        ENV_NAME="${{ matrix.env }}"
        
        if [ -z "$CLUSTER_IP" ]; then
          echo "‚ùå No cluster IP found - skipping kubeconfig setup"
          exit 0
        fi
        
        # Download kubeconfig from cluster
        echo "üì• Downloading kubeconfig from $CLUSTER_IP..."
        ssh -i <(echo "${{ secrets.SSH_PRIVATE_KEY }}") -o StrictHostKeyChecking=no ubuntu@$CLUSTER_IP \
          "sudo cat /etc/rancher/k3s/k3s.yaml" > /tmp/kubeconfig_raw.yaml
        
        # Update server IP in kubeconfig
        echo "üîß Updating kubeconfig server IP..."
        sed "s/127.0.0.1/$CLUSTER_IP/g" /tmp/kubeconfig_raw.yaml > /tmp/kubeconfig.yaml
        
        # Test connection with new kubeconfig
        echo "üß™ Testing connection to $ENV_NAME..."
        export KUBECONFIG=/tmp/kubeconfig.yaml
        
        # Wait for API server to be ready
        for i in {1..30}; do
          if kubectl get nodes --request-timeout=10s 2>/dev/null; then
            echo "‚úÖ Connection successful!"
            kubectl get nodes
            kubectl cluster-info
            break
          fi
          echo "‚è≥ Attempt $i/30 - waiting for API server..."
          sleep 10
        done
        
        # Verify connection worked
        if ! kubectl get nodes --request-timeout=10s 2>/dev/null; then
          echo "‚ùå Connection failed after 5 minutes"
          echo "üîç Debugging information:"
          echo "Cluster IP: $CLUSTER_IP"
          echo "Kubeconfig server:"
          grep "server:" /tmp/kubeconfig.yaml
          exit 1
        fi
        
        # Create GitHub secret with kubeconfig
        echo "üîê Creating GitHub secret..."
        KUBECONFIG_B64=$(base64 -w 0 /tmp/kubeconfig.yaml)
        
        # Map environment to secret name
        case "$ENV_NAME" in
          "lower")
            SECRET_NAME="KUBECONFIG_DEV"
            ;;
          "higher")
            SECRET_NAME="KUBECONFIG_PROD"
            ;;
          "monitoring")
            SECRET_NAME="KUBECONFIG_MONITORING"
            ;;
          *)
            SECRET_NAME="KUBECONFIG_${ENV_NAME^^}"
            ;;
        esac
        
        echo "üìù Creating secret: $SECRET_NAME"
        
        # Create the secret using GitHub CLI or API
        if command -v gh >/dev/null 2>&1; then
          echo "$KUBECONFIG_B64" | gh secret set "$SECRET_NAME" --body -
          echo "‚úÖ Secret $SECRET_NAME created successfully"
        else
          echo "‚ö†Ô∏è GitHub CLI not available - secret creation skipped"
          echo "üìã Manual setup required:"
          echo "Secret name: $SECRET_NAME"
          echo "Secret value: $KUBECONFIG_B64"
        fi
        
        # Cleanup
        rm -f /tmp/kubeconfig*.yaml
        
        echo "‚úÖ Kubeconfig setup completed for $ENV_NAME"

    - name: Terraform Destroy
      working-directory: infra/two-network-setup
      if: github.event.inputs.action == 'destroy'
      run: |
        echo "üßπ Destroying infrastructure..."
        
        # Validate destroy confirmation
        if [ "${{ github.event.inputs.confirm_destroy }}" != "DESTROY" ]; then
          echo "‚ùå Destroy confirmation required - type 'DESTROY' to confirm"
          exit 1
        fi
        
        # Check what will be destroyed
        if terraform state list > /dev/null 2>&1; then
          echo "üìã Resources to be destroyed:"
          terraform state list | head -10
          RESOURCE_COUNT=$(terraform state list | wc -l)
          echo "Total: $RESOURCE_COUNT resources"
          
          terraform destroy \
            -var="environment=${{ matrix.env == 'lower' && 'dev' || matrix.env == 'higher' && 'prod' || 'monitoring' }}" \
            -var="network_tier=${{ matrix.env }}" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="repo_pat=${{ secrets.REPO_PAT }}" \
            -var="repo_name=${{ secrets.REPO_NAME }}" \
            -var="restore_from_snapshot=false" \
            -var="snapshot_identifier=null" \
            -auto-approve
          
          echo "‚úÖ Infrastructure destroyed successfully"
        else
          echo "‚ÑπÔ∏è No resources found to destroy"
        fi

    - name: Cleanup on Failure
      if: failure() && (github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy')
      working-directory: infra/two-network-setup
      run: |
        echo "üßπ Cleaning up failed deployment..."
        
        terraform destroy \
          -var="environment=${{ matrix.env == 'lower' && 'dev' || matrix.env == 'higher' && 'prod' || 'monitoring' }}" \
          -var="network_tier=${{ matrix.env }}" \
          -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
          -var="repo_pat=${{ secrets.REPO_PAT }}" \
          -var="repo_name=${{ secrets.REPO_NAME }}" \
          -var="restore_from_snapshot=false" \
          -var="snapshot_identifier=null" \
          -auto-approve || echo "Cleanup completed with warnings"
        
        echo "üßπ Cleanup completed"

    - name: Post-deployment Validation
      if: success() && (github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy')
      run: |
        echo "‚úÖ Infrastructure deployment completed successfully!"
        echo ""
        echo "üìã Summary for ${{ matrix.env }} network:"
        
        if [ -n "${{ env.CLUSTER_IP }}" ]; then
          echo "üéØ K3s Cluster: ${{ env.CLUSTER_IP }}:6443"
          echo "üîê Kubeconfig: Available in GitHub Secrets"
          echo "üåê SSH Access: ssh -i ~/.ssh/k3s-key ubuntu@${{ env.CLUSTER_IP }}"
        fi
        
        echo ""
        echo "üöÄ Next steps:"
        echo "1. Deploy applications using Core Deployment workflow"
        echo "2. Access cluster using kubeconfig from GitHub Secrets"
        echo "3. Monitor resources using AWS Console"
        
        # Run governance validation
        if [ -f "scripts/validate-deployment.sh" ]; then
          echo ""
          echo "üõ°Ô∏è Running post-deployment validation..."
          chmod +x scripts/validate-deployment.sh
          ./scripts/validate-deployment.sh ${{ matrix.env }} || echo "‚ö†Ô∏è Validation completed with warnings"
        fies destroyed successfully"
        else
          echo "‚ö†Ô∏è Warning: $REMAINING resources remain in state"
        fi
        
        echo "‚è≥ Waiting for cleanup to complete..."
        sleep 30

    - name: Terraform Apply
      id: terraform-apply
      working-directory: infra/two-network-setup
      if: github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy'
      continue-on-error: false
      run: |
        echo "üöÄ Applying infrastructure changes..."
        
        # For redeploy, create new plan after destroy
        if [ "${{ github.event.inputs.action }}" = "redeploy" ]; then
          echo "üîÑ Creating fresh plan for redeploy..."
          terraform plan \
            -var="environment=${{ matrix.env == 'lower' && 'dev' || matrix.env == 'higher' && 'prod' || 'monitoring' }}" \
            -var="network_tier=${{ matrix.env }}" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="repo_pat=${{ secrets.REPO_PAT }}" \
            -var="repo_name=${{ secrets.REPO_NAME }}" \
            -var="restore_from_snapshot=${{ github.event.inputs.restore_from_snapshot }}" \
            -var="snapshot_identifier=${{ github.event.inputs.restore_from_snapshot == 'true' && 'healthapidb-snapshot' || 'null' }}" \
            -out=tfplan
        fi
        
        # Show what will be applied
        echo "üìã Resources to be modified:"
        terraform show -no-color tfplan | grep -E "# .* will be" | head -10
        
        echo ""
        echo "üîÑ Applying changes..."
        if ! terraform apply -auto-approve tfplan; then
          echo "‚ùå Terraform apply failed"
          exit 1
        fi

    - name: Automated Kubeconfig Setup
      if: github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy'
      working-directory: infra/two-network-setup
      run: |
        echo "üîë Automated kubeconfig setup..."
        
        # Get K3s cluster IP
        K3S_IP=$(terraform output -raw k3s_public_ip 2>/dev/null || echo "")
        
        if [ -n "$K3S_IP" ] && [ "$K3S_IP" != "Not available" ]; then
          echo "K3s cluster IP: $K3S_IP"
          
          # Setup SSH key
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > /tmp/ssh_key
          chmod 600 /tmp/ssh_key
          
          # Wait for K3s to be ready
          echo "‚è≥ Waiting for K3s to be ready..."
          for i in {1..10}; do
            if ssh -i /tmp/ssh_key -o StrictHostKeyChecking=no -o ConnectTimeout=10 ubuntu@$K3S_IP "sudo systemctl is-active k3s" 2>/dev/null | grep -q "active"; then
              echo "‚úÖ K3s service is active (attempt $i)"
              sleep 30
              break
            else
              echo "K3s not ready, waiting... (attempt $i/10)"
              sleep 30
            fi
          done
          
          # Download and setup kubeconfig
          if scp -i /tmp/ssh_key -o StrictHostKeyChecking=no ubuntu@$K3S_IP:/etc/rancher/k3s/k3s.yaml /tmp/kubeconfig; then
            # Update server IP
            sed -i "s/127.0.0.1/$K3S_IP/g" /tmp/kubeconfig
            
            # Test connection
            export KUBECONFIG=/tmp/kubeconfig
            if timeout 30 kubectl get nodes --request-timeout=20s; then
              echo "‚úÖ Kubeconfig connection successful!"
            else
              echo "‚ö†Ô∏è Connection test failed but proceeding with secret creation"
            fi
            
            # Create GitHub secrets using API
            KUBECONFIG_B64=$(base64 -w 0 /tmp/kubeconfig)
            
            # Get repository public key for encryption
            PUBLIC_KEY_RESPONSE=$(curl -s -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" \
              "https://api.github.com/repos/${{ github.repository }}/actions/secrets/public-key")
            
            PUBLIC_KEY=$(echo "$PUBLIC_KEY_RESPONSE" | jq -r '.key')
            KEY_ID=$(echo "$PUBLIC_KEY_RESPONSE" | jq -r '.key_id')
            
            # Function to create/update secret
            create_secret() {
              local secret_name=$1
              local secret_value=$2
              
              # For simplicity, we'll use base64 directly (GitHub will handle encryption)
              curl -X PUT \
                -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" \
                -H "Accept: application/vnd.github.v3+json" \
                "https://api.github.com/repos/${{ github.repository }}/actions/secrets/$secret_name" \
                -d "{\"encrypted_value\":\"$secret_value\",\"key_id\":\"$KEY_ID\"}"
            }
            
            # Create appropriate secrets based on environment
            if [ "${{ matrix.env }}" = "lower" ]; then
              echo "üîê Creating KUBECONFIG_DEV and KUBECONFIG_TEST secrets..."
              create_secret "KUBECONFIG_DEV" "$KUBECONFIG_B64"
              create_secret "KUBECONFIG_TEST" "$KUBECONFIG_B64"
              echo "‚úÖ Created secrets for dev and test environments"
            elif [ "${{ matrix.env }}" = "higher" ]; then
              echo "üîê Creating KUBECONFIG_PROD secret..."
              create_secret "KUBECONFIG_PROD" "$KUBECONFIG_B64"
              echo "‚úÖ Created secret for prod environment"
            elif [ "${{ matrix.env }}" = "monitoring" ]; then
              echo "üîê Creating KUBECONFIG_MONITORING secret..."
              create_secret "KUBECONFIG_MONITORING" "$KUBECONFIG_B64"
              echo "‚úÖ Created secret for monitoring environment"
            fi
            
            echo "üéâ Kubeconfig automation completed!"
          else
            echo "‚ùå Failed to download kubeconfig"
          fi
          
          rm -f /tmp/ssh_key /tmp/kubeconfig
        else
          echo "‚ùå K3s IP not available"
        fi
        
    - name: Post-Deployment Summary
      if: github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy'
      working-directory: infra/two-network-setup
      run: |
        echo "‚úÖ Infrastructure deployment completed"
        
        # Show final state
        echo "üìã Final state summary:"
        terraform state list | wc -l | xargs echo "Total resources managed:"
        
        # Verify state is in S3
        echo ""
        echo "üîç Verifying state storage..."
        if aws s3 ls s3://${{ secrets.TF_STATE_BUCKET }}/health-app-${{ matrix.env }}.tfstate > /dev/null 2>&1; then
          STATE_SIZE=$(aws s3 ls s3://${{ secrets.TF_STATE_BUCKET }}/health-app-${{ matrix.env }}.tfstate --human-readable | awk '{print $3 " " $4}')
          echo "‚úÖ State file saved to S3 (Size: $STATE_SIZE)"
        else
          echo "‚ö†Ô∏è Warning: State file not found in S3"
        fi

    - name: Terraform Destroy
      working-directory: infra/two-network-setup
      if: github.event.inputs.action == 'destroy' && github.event.inputs.confirm_destroy == 'DESTROY'
      run: |
        echo "üßπ Starting Terraform destroy for ${{ matrix.env }} environment"
        
        # Check if state exists
        if terraform state list > /dev/null 2>&1; then
          echo "üìã Resources to be destroyed:"
          terraform state list | head -10
          RESOURCE_COUNT=$(terraform state list | wc -l)
          echo "Total: $RESOURCE_COUNT resources"
          
          # Run Terraform destroy
          terraform destroy \
            -var="environment=${{ matrix.env == 'lower' && 'dev' || matrix.env == 'higher' && 'prod' || 'monitoring' }}" \
            -var="network_tier=${{ matrix.env }}" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="repo_pat=${{ secrets.REPO_PAT }}" \
            -var="repo_name=${{ secrets.REPO_NAME }}" \
            -var="restore_from_snapshot=false" \
            -var="snapshot_identifier=null" \
            -auto-approve || echo "‚ö†Ô∏è Terraform destroy completed with warnings"
          
          # Verify cleanup
          REMAINING=$(terraform state list 2>/dev/null | wc -l)
          if [ "$REMAINING" -eq 0 ]; then
            echo "‚úÖ All resources destroyed successfully"
          else
            echo "‚ö†Ô∏è Warning: $REMAINING resources remain in state"
          fi
        else
          echo "‚ÑπÔ∏è No Terraform state found - nothing to destroy"
        fi
        
        echo "‚úÖ Terraform destroy completed for ${{ matrix.env }} environment"
        
    - name: Enhanced Cleanup
      if: github.event.inputs.action == 'destroy' && github.event.inputs.confirm_destroy == 'DESTROY'
      run: |
        echo "üßπ Running enhanced cleanup..."
        
        # Use enhanced cleanup script
        chmod +x scripts/enhanced-cleanup.sh
        
        if [ "${{ github.event.inputs.cleanup_all_regions }}" = "true" ]; then
          echo "üåç Cleaning all network tiers..."
          ./scripts/enhanced-cleanup.sh ${{ env.AWS_REGION }} all true
        else
          echo "üìç Cleaning specific network tier: ${{ matrix.env }}"
          ./scripts/enhanced-cleanup.sh ${{ env.AWS_REGION }} ${{ matrix.env }} true
        fi
        
        echo "‚úÖ Enhanced cleanup completed"

    - name: Optimize Data Transfer
      if: github.event.inputs.optimize_data_transfer == 'true'
      run: |
        echo "üìä Optimizing data transfer to stay within free tier"
        echo "‚ö†Ô∏è WARNING: This will stop non-production resources!"
        chmod +x scripts/data-transfer-optimizer.sh
        ./scripts/data-transfer-optimizer.sh ${{ env.AWS_REGION }} monitor
        
        echo "üîß Applying data transfer optimizations"
        ./scripts/data-transfer-optimizer.sh ${{ env.AWS_REGION }} optimize

    - name: Infrastructure Summary
      if: (github.event.inputs.action == 'deploy' || github.event.inputs.action == 'redeploy') && (steps.terraform-apply.outcome == 'success' || github.event.inputs.action == 'redeploy')
      working-directory: infra/two-network-setup
      run: |
        echo "‚úÖ Infrastructure deployed successfully for ${{ matrix.env }} environment"
        echo "üìä Resources created:"
        
        # Get outputs with error handling
        K3S_IP=$(terraform output -raw k3s_public_ip 2>/dev/null || echo "Not available")
        RUNNER_PRIVATE_IP=$(terraform output -raw github_runner_ip 2>/dev/null || echo "Not available")
        RUNNER_PUBLIC_IP=$(terraform output -raw github_runner_public_ip 2>/dev/null || echo "Not available")
        RDS_ENDPOINT=$(terraform output -raw rds_endpoint 2>/dev/null || echo "Not available")
        
        echo "- K3s Cluster IP: $K3S_IP"
        echo "- GitHub Runner Private IP: $RUNNER_PRIVATE_IP"
        echo "- GitHub Runner Public IP: $RUNNER_PUBLIC_IP"
        echo "- RDS Endpoint: $RDS_ENDPOINT"
        
        echo ""
        echo "üîç All available outputs:"
        terraform output 2>/dev/null || echo "No outputs available"
        
        echo ""
        echo "üöÄ GitHub Runner is now available with labels: awsrunnerlocal, aws-${{ matrix.env }}"
        
        if [ "$RUNNER_PUBLIC_IP" != "Not available" ]; then
          echo "üìã SSH to runner: ssh -i ~/.ssh/your-key ubuntu@$RUNNER_PUBLIC_IP"
          echo "üîß Debug runner: sudo /home/ubuntu/debug-runner.sh"
        fi


    - name: Generate Execution Report
      if: always()
      working-directory: infra/two-network-setup
      run: |
        echo "## üèóÔ∏è Infrastructure Execution Report" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Basic execution details
        echo "### üìã Execution Details" >> $GITHUB_STEP_SUMMARY
        echo "**Action:** ${{ github.event.inputs.action }}" >> $GITHUB_STEP_SUMMARY
        echo "**Network Tier:** ${{ matrix.env }}" >> $GITHUB_STEP_SUMMARY
        echo "**Status:** ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
        echo "**Started:** $(date -d '5 minutes ago' '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
        echo "**Completed:** $(date '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
        echo "**Runner:** ${{ github.event.inputs.runner_type }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # What actually happened
        echo "### üîÑ Actions Performed" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ github.event.inputs.action }}" = "plan" ]; then
          echo "- ‚úÖ Terraform initialized" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Infrastructure plan generated" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Policy validation completed" >> $GITHUB_STEP_SUMMARY
        elif [ "${{ github.event.inputs.action }}" = "deploy" ]; then
          echo "- ‚úÖ Pre-deployment checks completed" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Terraform initialized" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Infrastructure plan created" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Resources deployed" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Post-deployment summary generated" >> $GITHUB_STEP_SUMMARY
        elif [ "${{ github.event.inputs.action }}" = "redeploy" ]; then
          echo "- ‚úÖ Pre-deployment checks completed" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Existing resources destroyed" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Fresh infrastructure plan created" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ New resources deployed" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Post-deployment summary generated" >> $GITHUB_STEP_SUMMARY
        elif [ "${{ github.event.inputs.action }}" = "destroy" ]; then
          if [ "${{ github.event.inputs.confirm_destroy }}" = "DESTROY" ]; then
            echo "- ‚úÖ Destroy confirmation validated" >> $GITHUB_STEP_SUMMARY
            echo "- ‚úÖ Resources destroyed" >> $GITHUB_STEP_SUMMARY
            if [ "${{ github.event.inputs.cleanup_all_regions }}" = "true" ]; then
              echo "- ‚úÖ Additional cleanup performed" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "- ‚ùå Destroy not confirmed - no action taken" >> $GITHUB_STEP_SUMMARY
          fi
        fi
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Current state
        echo "### üìä Current Infrastructure State" >> $GITHUB_STEP_SUMMARY
        if terraform state list > /tmp/resources.txt 2>/dev/null; then
          RESOURCE_COUNT=$(wc -l < /tmp/resources.txt)
          echo "**Total Resources:** $RESOURCE_COUNT" >> $GITHUB_STEP_SUMMARY
          
          # Resource breakdown
          EC2_COUNT=$(grep -c "aws_instance" /tmp/resources.txt || echo "0")
          RDS_COUNT=$(grep -c "aws_db_instance" /tmp/resources.txt || echo "0")
          SG_COUNT=$(grep -c "aws_security_group" /tmp/resources.txt || echo "0")
          KEY_COUNT=$(grep -c "aws_key_pair" /tmp/resources.txt || echo "0")
          
          echo "**EC2 Instances:** $EC2_COUNT" >> $GITHUB_STEP_SUMMARY
          echo "**RDS Instances:** $RDS_COUNT" >> $GITHUB_STEP_SUMMARY
          echo "**Security Groups:** $SG_COUNT" >> $GITHUB_STEP_SUMMARY
          echo "**Key Pairs:** $KEY_COUNT" >> $GITHUB_STEP_SUMMARY
          
          # Connection details if resources exist
          if [ "$RESOURCE_COUNT" -gt 0 ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### üîó Access Information" >> $GITHUB_STEP_SUMMARY
            
            K3S_IP=$(terraform output -raw k3s_public_ip 2>/dev/null || echo "Not deployed")
            RUNNER_IP=$(terraform output -raw github_runner_public_ip 2>/dev/null || echo "Not deployed")
            RDS_ENDPOINT=$(terraform output -raw rds_endpoint 2>/dev/null || echo "Not deployed")
            
            echo "**K3s Cluster:** $K3S_IP" >> $GITHUB_STEP_SUMMARY
            echo "**GitHub Runner:** $RUNNER_IP" >> $GITHUB_STEP_SUMMARY
            echo "**Database:** $RDS_ENDPOINT" >> $GITHUB_STEP_SUMMARY
          fi
        else
          echo "**Status:** No infrastructure deployed" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### üí∞ Cost Impact" >> $GITHUB_STEP_SUMMARY
        if [ "${{ github.event.inputs.action }}" = "destroy" ] && [ "${{ github.event.inputs.confirm_destroy }}" = "DESTROY" ]; then
          echo "**Monthly Cost:** $0 (resources destroyed)" >> $GITHUB_STEP_SUMMARY
        else
          echo "**Instance Types:** t2.micro (Free Tier eligible)" >> $GITHUB_STEP_SUMMARY
          echo "**Expected Monthly Cost:** $0 (within Free Tier limits)" >> $GITHUB_STEP_SUMMARY
        fi
        
        # Next steps
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### üéØ Next Steps" >> $GITHUB_STEP_SUMMARY
        if [ "${{ github.event.inputs.action }}" = "plan" ]; then
          echo "- Run **deploy** action to apply the planned changes" >> $GITHUB_STEP_SUMMARY
        elif [ "${{ github.event.inputs.action }}" = "deploy" ] || [ "${{ github.event.inputs.action }}" = "redeploy" ]; then
          echo "- Run **Platform Readiness Check** to verify deployment" >> $GITHUB_STEP_SUMMARY
          echo "- Deploy applications using **Core Deployment** workflow" >> $GITHUB_STEP_SUMMARY
        elif [ "${{ github.event.inputs.action }}" = "destroy" ]; then
          echo "- Infrastructure cleanup completed" >> $GITHUB_STEP_SUMMARY
          echo "- Ready for fresh deployment if needed" >> $GITHUB_STEP_SUMMARY
        fi
