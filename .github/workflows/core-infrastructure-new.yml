name: Core Infrastructure

on:
  push:
    branches:
      - main
    paths:
      - 'infra/**'
      - '.github/workflows/core-infrastructure-new.yml'
  workflow_dispatch:
    inputs:
      action:
        description: 'Action'
        required: true
        default: 'deploy'
        type: choice
        options:
          - deploy
          - destroy
          - plan
      network:
        description: 'Network Tier'
        required: true
        default: 'lower'
        type: choice
        options:
          - lower
          - higher
          - monitoring
      confirm_destroy:
        description: 'Type "DESTROY" to confirm'
        required: false
        type: string

env:
  AWS_REGION: ap-south-1
  TERRAFORM_VERSION: 1.6.0
  # Default values for push events
  DEFAULT_ACTION: deploy
  DEFAULT_NETWORK: lower

jobs:
  # Step 1: Deploy GitHub Runner (only for deploy action)
  deploy-runner:
    if: (github.event.inputs.action == 'deploy') || (github.event_name == 'push')
    runs-on: ubuntu-latest
    outputs:
      runner_ready: ${{ steps.verify-runner.outputs.ready }}
    steps:
      - uses: actions/checkout@v4
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      - uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}
          terraform_wrapper: false
      
      - name: Deploy Runner Infrastructure
        working-directory: infra
        env:
          NETWORK_TIER: ${{ github.event.inputs.network || env.DEFAULT_NETWORK }}
        run: |
          terraform init -reconfigure \
            -backend-config="bucket=${{ secrets.TF_STATE_BUCKET }}" \
            -backend-config="key=health-app-$NETWORK_TIER-runner.tfstate" \
            -backend-config="region=$AWS_REGION"
          
          # Clean up existing resources
          echo "ðŸ§¹ Cleaning up existing resources..."
          
          # Get account ID
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          
          # Clean up IAM resources with proper sequence
          echo "Cleaning up IAM roles and policies..."
          
          # Runner IAM cleanup
          aws iam remove-role-from-instance-profile --instance-profile-name health-app-runner-profile-$NETWORK_TIER --role-name health-app-runner-role-$NETWORK_TIER 2>/dev/null || true
          aws iam delete-instance-profile --instance-profile-name health-app-runner-profile-$NETWORK_TIER 2>/dev/null || true
          aws iam detach-role-policy --role-name health-app-runner-role-$NETWORK_TIER --policy-arn arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore 2>/dev/null || true
          aws iam delete-role --role-name health-app-runner-role-$NETWORK_TIER 2>/dev/null || true
          
          # Parameter Store IAM cleanup
          aws iam detach-role-policy --role-name health-app-dev-parameter-access --policy-arn arn:aws:iam::$ACCOUNT_ID:policy/health-app-dev-parameter-read 2>/dev/null || true
          aws iam delete-role --role-name health-app-dev-parameter-access 2>/dev/null || true
          aws iam delete-policy --policy-arn arn:aws:iam::$ACCOUNT_ID:policy/health-app-dev-parameter-read 2>/dev/null || true
          
          # Clean up key pairs
          aws ec2 delete-key-pair --key-name health-app-$NETWORK_TIER-key 2>/dev/null || true
          
          # Clean up VPCs to free up space
          echo "Cleaning up VPCs..."
          for vpc in $(aws ec2 describe-vpcs --query "Vpcs[?Tags[?Key=='ManagedBy' && Value=='terraform']].VpcId" --output text); do
            echo "Deleting VPC: $vpc"
            # Delete subnets first
            for subnet in $(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$vpc" --query "Subnets[].SubnetId" --output text); do
              aws ec2 delete-subnet --subnet-id $subnet 2>/dev/null || true
            done
            # Delete internet gateway
            for igw in $(aws ec2 describe-internet-gateways --filters "Name=attachment.vpc-id,Values=$vpc" --query "InternetGateways[].InternetGatewayId" --output text); do
              aws ec2 detach-internet-gateway --internet-gateway-id $igw --vpc-id $vpc 2>/dev/null || true
              aws ec2 delete-internet-gateway --internet-gateway-id $igw 2>/dev/null || true
            done
            # Delete VPC
            aws ec2 delete-vpc --vpc-id $vpc 2>/dev/null || true
          done
          
          sleep 20
          
          # Destroy existing terraform resources first
          echo "ðŸ”¥ Destroying existing terraform resources..."
          terraform destroy \
            -var-file="environments/runner-$NETWORK_TIER.tfvars" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="github_pat=${{ secrets.REPO_PAT }}" \
            -var="aws_region=${{ env.AWS_REGION }}" \
            -auto-approve 2>/dev/null || echo "No existing resources to destroy"
          
          sleep 10
          
          # Use network-specific runner config
          cp environments/runner.tfvars environments/runner-$NETWORK_TIER.tfvars
          sed -i "s/network_tier = \"monitoring\"/network_tier = \"$NETWORK_TIER\"/g" environments/runner-$NETWORK_TIER.tfvars
          
          terraform apply \
            -var-file="environments/runner-$NETWORK_TIER.tfvars" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="github_pat=${{ secrets.REPO_PAT }}" \
            -var="aws_region=${{ env.AWS_REGION }}" \
            -auto-approve
      
      - name: Verify Runner Ready
        id: verify-runner
        env:
          SSH_PRIVATE_KEY: ${{ secrets.SSH_PRIVATE_KEY }}
        run: |
          echo "â³ Waiting for runner to be ready..."
          sleep 60
          
          # Setup SSH key for testing
          mkdir -p ~/.ssh
          echo "$SSH_PRIVATE_KEY" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          
          # Get runner IP
          RUNNER_IP=$(terraform output -raw github_runner_public_ip)
          echo "Runner IP: $RUNNER_IP"
          
          # Wait for SSH to be ready
          for i in {1..20}; do
            if timeout 10 ssh -o StrictHostKeyChecking=no -o ConnectTimeout=5 ubuntu@$RUNNER_IP "echo 'Runner SSH ready'" 2>/dev/null; then
              echo "âœ… Runner SSH is ready"
              rm -f ~/.ssh/id_rsa
              break
            fi
            echo "â³ Waiting for runner SSH... ($i/20)"
            sleep 15
          done
          
          rm -f ~/.ssh/id_rsa
          echo "ready=true" >> $GITHUB_OUTPUT

  # Step 2: Deploy Infrastructure Resources
  deploy-infrastructure:
    if: (github.event.inputs.action == 'deploy') || (github.event_name == 'push')
    needs: deploy-runner
    runs-on: ubuntu-latest
    outputs:
      cluster_ip: ${{ steps.deploy.outputs.cluster_ip }}
      dev_cluster_ip: ${{ steps.deploy.outputs.dev_cluster_ip }}
      test_cluster_ip: ${{ steps.deploy.outputs.test_cluster_ip }}
    steps:
      - uses: actions/checkout@v4
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      - uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}
          terraform_wrapper: false
      
      - name: Deploy Infrastructure
        id: deploy
        working-directory: infra
        env:
          NETWORK_TIER: ${{ github.event.inputs.network || env.DEFAULT_NETWORK }}
        run: |
          terraform init -reconfigure \
            -backend-config="bucket=${{ secrets.TF_STATE_BUCKET }}" \
            -backend-config="key=health-app-${{ github.event.inputs.network }}.tfstate" \
            -backend-config="region=$AWS_REGION"
          
          # Clean up existing key pairs and terraform state
          echo "ðŸ§¹ Cleaning up existing resources..."
          if [ "$NETWORK_TIER" == "lower" ]; then
            aws ec2 delete-key-pair --key-name health-app-lower-dev-k3s-key 2>/dev/null || echo "Dev key doesn't exist"
            aws ec2 delete-key-pair --key-name health-app-lower-test-k3s-key 2>/dev/null || echo "Test key doesn't exist"
            terraform state rm 'module.k3s_clusters["dev"].aws_key_pair.main' 2>/dev/null || echo "Dev key not in state"
            terraform state rm 'module.k3s_clusters["test"].aws_key_pair.main' 2>/dev/null || echo "Test key not in state"
          else
            aws ec2 delete-key-pair --key-name health-app-$NETWORK_TIER-k3s-key 2>/dev/null || echo "Key doesn't exist"
            terraform state rm 'module.k3s[0].aws_key_pair.main' 2>/dev/null || echo "Key not in state"
          fi
          
          # Clean up terraform state for existing resources
          terraform state rm 'module.github_runner.aws_iam_role.runner_role' 2>/dev/null || true
          terraform state rm 'module.parameter_store.aws_iam_role.parameter_access' 2>/dev/null || true
          terraform state rm 'module.parameter_store.aws_iam_policy.parameter_read' 2>/dev/null || true
          terraform state rm 'module.vpc.aws_vpc.main' 2>/dev/null || true
          
          sleep 10
          
          # Destroy existing infrastructure first
          echo "ðŸ”¥ Destroying existing infrastructure..."
          terraform destroy \
            -var-file="environments/$NETWORK_TIER.tfvars" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="github_pat=${{ secrets.REPO_PAT }}" \
            -var="aws_region=${{ env.AWS_REGION }}" \
            -auto-approve 2>/dev/null || echo "No existing infrastructure to destroy"
          
          sleep 10
          
          terraform apply \
            -var-file="environments/$NETWORK_TIER.tfvars" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="github_pat=${{ secrets.REPO_PAT }}" \
            -var="aws_region=${{ env.AWS_REGION }}" \
            -auto-approve
          
          # Output cluster IPs
          if [ "$NETWORK_TIER" == "lower" ]; then
            echo "dev_cluster_ip=$(terraform output -raw dev_cluster_ip)" >> $GITHUB_OUTPUT
            echo "test_cluster_ip=$(terraform output -raw test_cluster_ip)" >> $GITHUB_OUTPUT
          else
            echo "cluster_ip=$(terraform output -raw k3s_instance_ip)" >> $GITHUB_OUTPUT
          fi
      
      - name: Verify Instances Ready
        env:
          SSH_PRIVATE_KEY: ${{ secrets.SSH_PRIVATE_KEY }}
          DEV_CLUSTER_IP: ${{ steps.deploy.outputs.dev_cluster_ip }}
          TEST_CLUSTER_IP: ${{ steps.deploy.outputs.test_cluster_ip }}
          CLUSTER_IP: ${{ steps.deploy.outputs.cluster_ip }}
        run: |
          # Setup SSH key
          mkdir -p ~/.ssh
          echo "$SSH_PRIVATE_KEY" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          echo "â³ Waiting for instances to be ready..."
          sleep 120
          
          # Function to test SSH connectivity
          test_ssh() {
            local IP=$1
            local NAME=$2
            echo "Testing SSH connectivity to $NAME ($IP)..."
            
            for i in {1..15}; do
              if timeout 10 ssh -o StrictHostKeyChecking=no -o ConnectTimeout=5 ubuntu@$IP "echo 'SSH ready'" 2>/dev/null; then
                echo "âœ… $NAME instance SSH is ready"
                return 0
              fi
              echo "â³ Waiting for $NAME SSH... ($i/15)"
              sleep 20
            done
            echo "âŒ $NAME instance SSH not ready after 5 minutes"
            return 1
          }
          
          # Test instances based on network tier
          if [ "${{ github.event.inputs.network }}" == "lower" ]; then
            success=true
            [ -n "$DEV_CLUSTER_IP" ] && { test_ssh "$DEV_CLUSTER_IP" "dev" || success=false; }
            [ -n "$TEST_CLUSTER_IP" ] && { test_ssh "$TEST_CLUSTER_IP" "test" || success=false; }
            
            if [ "$success" = false ]; then
              echo "âŒ Some instances not ready"
              exit 1
            fi
          else
            [ -n "$CLUSTER_IP" ] && test_ssh "$CLUSTER_IP" "${{ github.event.inputs.network }}"
          fi
          
          echo "âœ… All instances are ready for K3s installation"
          
          # Cleanup SSH key
          rm -f ~/.ssh/id_rsa

  # Step 3: Install K3s using Ansible
  install-k3s:
    if: (github.event.inputs.action == 'deploy') || (github.event_name == 'push')
    needs: [deploy-runner, deploy-infrastructure]
    runs-on: self-hosted
    container:
      image: ghcr.io/arunprabus/dev2prod-healthapp/runner-tools:latest
      credentials:
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup SSH Key
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
      
      - name: Create Ansible Inventory
        env:
          DEV_CLUSTER_IP: ${{ needs.deploy-infrastructure.outputs.dev_cluster_ip }}
          TEST_CLUSTER_IP: ${{ needs.deploy-infrastructure.outputs.test_cluster_ip }}
          CLUSTER_IP: ${{ needs.deploy-infrastructure.outputs.cluster_ip }}
        run: |
          cat > inventory.ini << EOF
          [k3s_nodes]
          EOF
          
          if [ "${{ github.event.inputs.network }}" == "lower" ]; then
            [ -n "$DEV_CLUSTER_IP" ] && echo "dev-cluster ansible_host=$DEV_CLUSTER_IP" >> inventory.ini
            [ -n "$TEST_CLUSTER_IP" ] && echo "test-cluster ansible_host=$TEST_CLUSTER_IP" >> inventory.ini
          else
            [ -n "$CLUSTER_IP" ] && echo "${{ github.event.inputs.network }}-cluster ansible_host=$CLUSTER_IP" >> inventory.ini
          fi
          
          cat >> inventory.ini << EOF
          
          [k3s_nodes:vars]
          ansible_user=ubuntu
          ansible_ssh_private_key_file=~/.ssh/id_rsa
          ansible_ssh_common_args='-o StrictHostKeyChecking=no'
          EOF
      
      - name: Install K3s with Ansible
        run: |
          ansible-playbook -i inventory.ini ansible/k3s-install.yml
      
      - name: Cleanup SSH Key
        if: always()
        run: rm -f ~/.ssh/id_rsa
      
      - name: Job Summary
        if: always()
        run: |
          echo "## ðŸš€ Infrastructure Deployment Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Component | Status | Details |" >> $GITHUB_STEP_SUMMARY
          echo "|-----------|--------|---------|" >> $GITHUB_STEP_SUMMARY
          echo "| GitHub Runner | âœ… Ready | Self-hosted runner deployed |" >> $GITHUB_STEP_SUMMARY
          echo "| Infrastructure | âœ… Deployed | ${{ github.event.inputs.network }} tier resources |" >> $GITHUB_STEP_SUMMARY
          echo "| K3s Installation | âœ… Complete | Ansible deployment successful |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ github.event.inputs.network }}" == "lower" ]; then
            echo "### ðŸŽ¯ Cluster Endpoints" >> $GITHUB_STEP_SUMMARY
            [ -n "${{ needs.deploy-infrastructure.outputs.dev_cluster_ip }}" ] && echo "- **Dev Cluster**: https://${{ needs.deploy-infrastructure.outputs.dev_cluster_ip }}:6443" >> $GITHUB_STEP_SUMMARY
            [ -n "${{ needs.deploy-infrastructure.outputs.test_cluster_ip }}" ] && echo "- **Test Cluster**: https://${{ needs.deploy-infrastructure.outputs.test_cluster_ip }}:6443" >> $GITHUB_STEP_SUMMARY
          else
            echo "### ðŸŽ¯ Cluster Endpoint" >> $GITHUB_STEP_SUMMARY
            [ -n "${{ needs.deploy-infrastructure.outputs.cluster_ip }}" ] && echo "- **${{ github.event.inputs.network }} Cluster**: https://${{ needs.deploy-infrastructure.outputs.cluster_ip }}:6443" >> $GITHUB_STEP_SUMMARY
          fi

  # Destroy job
  destroy:
    if: github.event.inputs.action == 'destroy'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      - uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}
          terraform_wrapper: false

      - name: Terraform Destroy
        working-directory: infra
        run: |
          if [ "${{ github.event.inputs.confirm_destroy }}" != "DESTROY" ]; then
            echo "âŒ Type 'DESTROY' to confirm"
            exit 1
          fi
          
          terraform init -reconfigure \
            -backend-config="bucket=${{ secrets.TF_STATE_BUCKET }}" \
            -backend-config="key=health-app-${{ github.event.inputs.network }}.tfstate" \
            -backend-config="region=$AWS_REGION"
          
          terraform destroy \
            -var-file="environments/${{ github.event.inputs.network }}.tfvars" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="github_pat=${{ secrets.REPO_PAT }}" \
            -var="aws_region=${{ env.AWS_REGION }}" \
            -auto-approve
      
      - name: Destroy Summary
        if: always()
        run: |
          echo "## ðŸ’¥ Infrastructure Destroy Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Component | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-----------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| ${{ github.event.inputs.network }} Infrastructure | âœ… Destroyed |" >> $GITHUB_STEP_SUMMARY
          echo "| Resources Cleaned | âœ… Complete |" >> $GITHUB_STEP_SUMMARY